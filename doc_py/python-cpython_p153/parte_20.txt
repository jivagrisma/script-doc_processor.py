   +-------+--------------------------+

   .. note::

      The ZIP file format does not support timestamps before 1980.


.. attribute:: ZipInfo.compress_type

   Type of compression for the archive member.


.. attribute:: ZipInfo.comment

   Comment for the individual archive member as a :class:`bytes` object.


.. attribute:: ZipInfo.extra

   Expansion field data.  The `PKZIP Application Note`_ contains
   some comments on the internal structure of the data contained in this
   :class:`bytes` object.


.. attribute:: ZipInfo.create_system

   System which created ZIP archive.


.. attribute:: ZipInfo.create_version

   PKZIP version which created ZIP archive.


.. attribute:: ZipInfo.extract_version

   PKZIP version needed to extract archive.


.. attribute:: ZipInfo.reserved

   Must be zero.


.. attribute:: ZipInfo.flag_bits

   ZIP flag bits.


.. attribute:: ZipInfo.volume

   Volume number of file header.


.. attribute:: ZipInfo.internal_attr

   Internal attributes.


.. attribute:: ZipInfo.external_attr

   External file attributes.


.. attribute:: ZipInfo.header_offset

   Byte offset to the file header.


.. attribute:: ZipInfo.CRC

   CRC-32 of the uncompressed file.


.. attribute:: ZipInfo.compress_size

   Size of the compressed data.


.. attribute:: ZipInfo.file_size

   Size of the uncompressed file.


.. _zipfile-commandline:
.. program:: zipfile

Command-Line Interface
----------------------

The :mod:`zipfile` module provides a simple command-line interface to interact
with ZIP archives.

If you want to create a new ZIP archive, specify its name after the :option:`-c`
option and then list the filename(s) that should be included:

.. code-block:: shell-session

    $ python -m zipfile -c monty.zip spam.txt eggs.txt

Passing a directory is also acceptable:

.. code-block:: shell-session

    $ python -m zipfile -c monty.zip life-of-brian_1979/

If you want to extract a ZIP archive into the specified directory, use
the :option:`-e` option:

.. code-block:: shell-session

    $ python -m zipfile -e monty.zip target-dir/

For a list of the files in a ZIP archive, use the :option:`-l` option:

.. code-block:: shell-session

    $ python -m zipfile -l monty.zip


Command-line options
~~~~~~~~~~~~~~~~~~~~

.. option:: -l <zipfile>
            --list <zipfile>

   List files in a zipfile.

.. option:: -c <zipfile> <source1> ... <sourceN>
            --create <zipfile> <source1> ... <sourceN>

   Create zipfile from source files.

.. option:: -e <zipfile> <output_dir>
            --extract <zipfile> <output_dir>

   Extract zipfile into target directory.

.. option:: -t <zipfile>
            --test <zipfile>

   Test whether the zipfile is valid or not.

.. option:: --metadata-encoding <encoding>

   Specify encoding of member names for :option:`-l`, :option:`-e` and
   :option:`-t`.

   .. versionadded:: 3.11


Decompression pitfalls
----------------------

The extraction in zipfile module might fail due to some pitfalls listed below.

From file itself
~~~~~~~~~~~~~~~~

Decompression may fail due to incorrect password / CRC checksum / ZIP format or
unsupported compression method / decryption.

File System limitations
~~~~~~~~~~~~~~~~~~~~~~~

Exceeding limitations on different file systems can cause decompression failed.
Such as allowable characters in the directory entries, length of the file name,
length of the pathname, size of a single file, and number of files, etc.

.. _zipfile-resources-limitations:

Resources limitations
~~~~~~~~~~~~~~~~~~~~~

The lack of memory or disk volume would lead to decompression
failed. For example, decompression bombs (aka `ZIP bomb`_)
apply to zipfile library that can cause disk volume exhaustion.

Interruption
~~~~~~~~~~~~

Interruption during the decompression, such as pressing control-C or killing the
decompression process may result in incomplete decompression of the archive.

Default behaviors of extraction
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Not knowing the default extraction behaviors
can cause unexpected decompression results.
For example, when extracting the same archive twice,
it overwrites files without asking.


.. _ZIP bomb: https://en.wikipedia.org/wiki/Zip_bomb
.. _PKZIP Application Note: https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT


================================================
File: /Doc/library/zipimport.rst
================================================
:mod:`!zipimport` --- Import modules from Zip archives
======================================================

.. module:: zipimport
   :synopsis: Support for importing Python modules from ZIP archives.

.. moduleauthor:: Just van Rossum <just@letterror.com>

**Source code:** :source:`Lib/zipimport.py`

--------------

This module adds the ability to import Python modules (:file:`\*.py`,
:file:`\*.pyc`) and packages from ZIP-format archives. It is usually not
needed to use the :mod:`zipimport` module explicitly; it is automatically used
by the built-in :keyword:`import` mechanism for :data:`sys.path` items that are paths
to ZIP archives.

Typically, :data:`sys.path` is a list of directory names as strings.  This module
also allows an item of :data:`sys.path` to be a string naming a ZIP file archive.
The ZIP archive can contain a subdirectory structure to support package imports,
and a path within the archive can be specified to only import from a
subdirectory.  For example, the path :file:`example.zip/lib/` would only
import from the :file:`lib/` subdirectory within the archive.

Any files may be present in the ZIP archive, but importers are only invoked for
:file:`.py` and :file:`.pyc` files.  ZIP import of dynamic modules
(:file:`.pyd`, :file:`.so`) is disallowed. Note that if an archive only contains
:file:`.py` files, Python will not attempt to modify the archive by adding the
corresponding :file:`.pyc` file, meaning that if a ZIP archive
doesn't contain :file:`.pyc` files, importing may be rather slow.

.. versionchanged:: 3.13
   ZIP64 is supported

.. versionchanged:: 3.8
   Previously, ZIP archives with an archive comment were not supported.

.. seealso::

   `PKZIP Application Note <https://pkware.cachefly.net/webdocs/casestudies/APPNOTE.TXT>`_
      Documentation on the ZIP file format by Phil Katz, the creator of the format and
      algorithms used.

   :pep:`273` - Import Modules from Zip Archives
      Written by James C. Ahlstrom, who also provided an implementation. Python 2.3
      follows the specification in :pep:`273`, but uses an implementation written by Just
      van Rossum that uses the import hooks described in :pep:`302`.

   :mod:`importlib` - The implementation of the import machinery
      Package providing the relevant protocols for all importers to
      implement.


This module defines an exception:

.. exception:: ZipImportError

   Exception raised by zipimporter objects. It's a subclass of :exc:`ImportError`,
   so it can be caught as :exc:`ImportError`, too.


.. _zipimporter-objects:

zipimporter Objects
-------------------

:class:`zipimporter` is the class for importing ZIP files.

.. class:: zipimporter(archivepath)

   Create a new zipimporter instance. *archivepath* must be a path to a ZIP
   file, or to a specific path within a ZIP file.  For example, an *archivepath*
   of :file:`foo/bar.zip/lib` will look for modules in the :file:`lib` directory
   inside the ZIP file :file:`foo/bar.zip` (provided that it exists).

   :exc:`ZipImportError` is raised if *archivepath* doesn't point to a valid ZIP
   archive.

   .. versionchanged:: 3.12

      Methods ``find_loader()`` and ``find_module()``, deprecated in 3.10 are
      now removed.  Use :meth:`find_spec` instead.

   .. method:: create_module(spec)

      Implementation of :meth:`importlib.abc.Loader.create_module` that returns
      :const:`None` to explicitly request the default semantics.

      .. versionadded:: 3.10


   .. method:: exec_module(module)

      Implementation of :meth:`importlib.abc.Loader.exec_module`.

      .. versionadded:: 3.10


   .. method:: find_spec(fullname, target=None)

      An implementation of :meth:`importlib.abc.PathEntryFinder.find_spec`.

      .. versionadded:: 3.10


   .. method:: get_code(fullname)

      Return the code object for the specified module. Raise
      :exc:`ZipImportError` if the module couldn't be imported.


   .. method:: get_data(pathname)

      Return the data associated with *pathname*. Raise :exc:`OSError` if the
      file wasn't found.

      .. versionchanged:: 3.3
         :exc:`IOError` used to be raised, it is now an alias of :exc:`OSError`.


   .. method:: get_filename(fullname)

      Return the value ``__file__`` would be set to if the specified module
      was imported. Raise :exc:`ZipImportError` if the module couldn't be
      imported.

      .. versionadded:: 3.1


   .. method:: get_source(fullname)

      Return the source code for the specified module. Raise
      :exc:`ZipImportError` if the module couldn't be found, return
      :const:`None` if the archive does contain the module, but has no source
      for it.


   .. method:: is_package(fullname)

      Return ``True`` if the module specified by *fullname* is a package. Raise
      :exc:`ZipImportError` if the module couldn't be found.


   .. method:: load_module(fullname)

      Load the module specified by *fullname*. *fullname* must be the fully
      qualified (dotted) module name. Returns the imported module on success,
      raises :exc:`ZipImportError` on failure.

      .. deprecated:: 3.10

         Use :meth:`exec_module` instead.


   .. method:: invalidate_caches()

      Clear out the internal cache of information about files found within
      the ZIP archive.

      .. versionadded:: 3.10


   .. attribute:: archive

      The file name of the importer's associated ZIP file, without a possible
      subpath.


   .. attribute:: prefix

      The subpath within the ZIP file where modules are searched.  This is the
      empty string for zipimporter objects which point to the root of the ZIP
      file.

   The :attr:`archive` and :attr:`prefix` attributes, when combined with a
   slash, equal the original *archivepath* argument given to the
   :class:`zipimporter` constructor.


.. _zipimport-examples:

Examples
--------

Here is an example that imports a module from a ZIP archive - note that the
:mod:`zipimport` module is not explicitly used.

.. code-block:: shell-session

   $ unzip -l example.zip
   Archive:  example.zip
     Length     Date   Time    Name
    --------    ----   ----    ----
        8467  11-26-02 22:30   jwzthreading.py
    --------                   -------
        8467                   1 file
   $ ./python
   Python 2.3 (#1, Aug 1 2003, 19:54:32)
   >>> import sys
   >>> sys.path.insert(0, 'example.zip')  # Add .zip file to front of path
   >>> import jwzthreading
   >>> jwzthreading.__file__
   'example.zip/jwzthreading.py'


================================================
File: /Doc/library/zlib.rst
================================================
:mod:`!zlib` --- Compression compatible with :program:`gzip`
============================================================

.. module:: zlib
   :synopsis: Low-level interface to compression and decompression routines
              compatible with gzip.

--------------

For applications that require data compression, the functions in this module
allow compression and decompression, using the zlib library. The zlib library
has its own home page at https://www.zlib.net.   There are known
incompatibilities between the Python module and versions of the zlib library
earlier than 1.1.3; 1.1.3 has a `security vulnerability <https://zlib.net/zlib_faq.html#faq33>`_, so we recommend using
1.1.4 or later.

zlib's functions have many options and often need to be used in a particular
order.  This documentation doesn't attempt to cover all of the permutations;
consult the zlib manual at http://www.zlib.net/manual.html for authoritative
information.

For reading and writing ``.gz`` files see the :mod:`gzip` module.

The available exception and functions in this module are:


.. exception:: error

   Exception raised on compression and decompression errors.


.. function:: adler32(data[, value])

   Computes an Adler-32 checksum of *data*.  (An Adler-32 checksum is almost as
   reliable as a CRC32 but can be computed much more quickly.)  The result
   is an unsigned 32-bit integer.  If *value* is present, it is used as
   the starting value of the checksum; otherwise, a default value of 1
   is used.  Passing in *value* allows computing a running checksum over the
   concatenation of several inputs.  The algorithm is not cryptographically
   strong, and should not be used for authentication or digital signatures.  Since
   the algorithm is designed for use as a checksum algorithm, it is not suitable
   for use as a general hash algorithm.

   .. versionchanged:: 3.0
      The result is always unsigned.

.. function:: compress(data, /, level=-1, wbits=MAX_WBITS)

   Compresses the bytes in *data*, returning a bytes object containing compressed data.
   *level* is an integer from ``0`` to ``9`` or ``-1`` controlling the level of compression;
   ``1`` (Z_BEST_SPEED) is fastest and produces the least compression, ``9`` (Z_BEST_COMPRESSION)
   is slowest and produces the most.  ``0`` (Z_NO_COMPRESSION) is no compression.
   The default value is ``-1`` (Z_DEFAULT_COMPRESSION).  Z_DEFAULT_COMPRESSION represents a default
   compromise between speed and compression (currently equivalent to level 6).

   .. _compress-wbits:

   The *wbits* argument controls the size of the history buffer (or the
   "window size") used when compressing data, and whether a header and
   trailer is included in the output.  It can take several ranges of values,
   defaulting to ``15`` (MAX_WBITS):

   * +9 to +15: The base-two logarithm of the window size, which
     therefore ranges between 512 and 32768.  Larger values produce
     better compression at the expense of greater memory usage.  The
     resulting output will include a zlib-specific header and trailer.

   * −9 to −15: Uses the absolute value of *wbits* as the
     window size logarithm, while producing a raw output stream with no
     header or trailing checksum.

   * +25 to +31 = 16 + (9 to 15): Uses the low 4 bits of the value as the
     window size logarithm, while including a basic :program:`gzip` header
     and trailing checksum in the output.

   Raises the :exc:`error` exception if any error occurs.

   .. versionchanged:: 3.6
      *level* can now be used as a keyword parameter.

   .. versionchanged:: 3.11
      The *wbits* parameter is now available to set window bits and
      compression type.

.. function:: compressobj(level=-1, method=DEFLATED, wbits=MAX_WBITS, memLevel=DEF_MEM_LEVEL, strategy=Z_DEFAULT_STRATEGY[, zdict])

   Returns a compression object, to be used for compressing data streams that won't
   fit into memory at once.

   *level* is the compression level -- an integer from ``0`` to ``9`` or ``-1``.
   A value of ``1`` (Z_BEST_SPEED) is fastest and produces the least compression,
   while a value of ``9`` (Z_BEST_COMPRESSION) is slowest and produces the most.
   ``0`` (Z_NO_COMPRESSION) is no compression.  The default value is ``-1`` (Z_DEFAULT_COMPRESSION).
   Z_DEFAULT_COMPRESSION represents a default compromise between speed and compression
   (currently equivalent to level 6).

   *method* is the compression algorithm. Currently, the only supported value is
   :const:`DEFLATED`.

   The *wbits* parameter controls the size of the history buffer (or the
   "window size"), and what header and trailer format will be used. It has
   the same meaning as `described for compress() <#compress-wbits>`__.

   The *memLevel* argument controls the amount of memory used for the
   internal compression state. Valid values range from ``1`` to ``9``.
   Higher values use more memory, but are faster and produce smaller output.

   *strategy* is used to tune the compression algorithm. Possible values are
   :const:`Z_DEFAULT_STRATEGY`, :const:`Z_FILTERED`, :const:`Z_HUFFMAN_ONLY`,
   :const:`Z_RLE` (zlib 1.2.0.1) and :const:`Z_FIXED` (zlib 1.2.2.2).

   *zdict* is a predefined compression dictionary. This is a sequence of bytes
   (such as a :class:`bytes` object) containing subsequences that are expected
   to occur frequently in the data that is to be compressed. Those subsequences
   that are expected to be most common should come at the end of the dictionary.

   .. versionchanged:: 3.3
      Added the *zdict* parameter and keyword argument support.


.. function:: crc32(data[, value])

   .. index::
      single: Cyclic Redundancy Check
      single: checksum; Cyclic Redundancy Check

   Computes a CRC (Cyclic Redundancy Check) checksum of *data*. The
   result is an unsigned 32-bit integer. If *value* is present, it is used
   as the starting value of the checksum; otherwise, a default value of 0
   is used.  Passing in *value* allows computing a running checksum over the
   concatenation of several inputs.  The algorithm is not cryptographically
   strong, and should not be used for authentication or digital signatures.  Since
   the algorithm is designed for use as a checksum algorithm, it is not suitable
   for use as a general hash algorithm.

   .. versionchanged:: 3.0
      The result is always unsigned.

.. function:: decompress(data, /, wbits=MAX_WBITS, bufsize=DEF_BUF_SIZE)

   Decompresses the bytes in *data*, returning a bytes object containing the
   uncompressed data.  The *wbits* parameter depends on
   the format of *data*, and is discussed further below.
   If *bufsize* is given, it is used as the initial size of the output
   buffer.  Raises the :exc:`error` exception if any error occurs.

   .. _decompress-wbits:

   The *wbits* parameter controls the size of the history buffer
   (or "window size"), and what header and trailer format is expected.
   It is similar to the parameter for :func:`compressobj`, but accepts
   more ranges of values:

   * +8 to +15: The base-two logarithm of the window size.  The input
     must include a zlib header and trailer.

   * 0: Automatically determine the window size from the zlib header.
     Only supported since zlib 1.2.3.5.

   * −8 to −15: Uses the absolute value of *wbits* as the window size
     logarithm.  The input must be a raw stream with no header or trailer.

   * +24 to +31 = 16 + (8 to 15): Uses the low 4 bits of the value as
     the window size logarithm.  The input must include a gzip header and
     trailer.

   * +40 to +47 = 32 + (8 to 15): Uses the low 4 bits of the value as
     the window size logarithm, and automatically accepts either
     the zlib or gzip format.

   When decompressing a stream, the window size must not be smaller
   than the size originally used to compress the stream; using a too-small
   value may result in an :exc:`error` exception. The default *wbits* value
   corresponds to the largest window size and requires a zlib header and
   trailer to be included.

   *bufsize* is the initial size of the buffer used to hold decompressed data.  If
   more space is required, the buffer size will be increased as needed, so you
   don't have to get this value exactly right; tuning it will only save a few calls
   to :c:func:`malloc`.

   .. versionchanged:: 3.6
      *wbits* and *bufsize* can be used as keyword arguments.

.. function:: decompressobj(wbits=MAX_WBITS[, zdict])

   Returns a decompression object, to be used for decompressing data streams that
   won't fit into memory at once.

   The *wbits* parameter controls the size of the history buffer (or the
   "window size"), and what header and trailer format is expected.  It has
   the same meaning as `described for decompress() <#decompress-wbits>`__.

   The *zdict* parameter specifies a predefined compression dictionary. If
   provided, this must be the same dictionary as was used by the compressor that
   produced the data that is to be decompressed.

   .. note::

      If *zdict* is a mutable object (such as a :class:`bytearray`), you must not
      modify its contents between the call to :func:`decompressobj` and the first
      call to the decompressor's ``decompress()`` method.

   .. versionchanged:: 3.3
      Added the *zdict* parameter.


Compression objects support the following methods:


.. method:: Compress.compress(data)

   Compress *data*, returning a bytes object containing compressed data for at least
   part of the data in *data*.  This data should be concatenated to the output
   produced by any preceding calls to the :meth:`compress` method.  Some input may
   be kept in internal buffers for later processing.


.. method:: Compress.flush([mode])

   All pending input is processed, and a bytes object containing the remaining compressed
   output is returned.  *mode* can be selected from the constants
   :const:`Z_NO_FLUSH`, :const:`Z_PARTIAL_FLUSH`, :const:`Z_SYNC_FLUSH`,
   :const:`Z_FULL_FLUSH`, :const:`Z_BLOCK` (zlib 1.2.3.4), or :const:`Z_FINISH`,
   defaulting to :const:`Z_FINISH`.  Except :const:`Z_FINISH`, all constants
   allow compressing further bytestrings of data, while :const:`Z_FINISH` finishes the
   compressed stream and prevents compressing any more data.  After calling :meth:`flush`
   with *mode* set to :const:`Z_FINISH`, the :meth:`compress` method cannot be called again;
   the only realistic action is to delete the object.


.. method:: Compress.copy()

   Returns a copy of the compression object.  This can be used to efficiently
   compress a set of data that share a common initial prefix.


.. versionchanged:: 3.8
   Added :func:`copy.copy` and :func:`copy.deepcopy` support to compression
   objects.


Decompression objects support the following methods and attributes:


.. attribute:: Decompress.unused_data

   A bytes object which contains any bytes past the end of the compressed data. That is,
   this remains ``b""`` until the last byte that contains compression data is
   available.  If the whole bytestring turned out to contain compressed data, this is
   ``b""``, an empty bytes object.


.. attribute:: Decompress.unconsumed_tail

   A bytes object that contains any data that was not consumed by the last
   :meth:`decompress` call because it exceeded the limit for the uncompressed data
   buffer.  This data has not yet been seen by the zlib machinery, so you must feed
   it (possibly with further data concatenated to it) back to a subsequent
   :meth:`decompress` method call in order to get correct output.


.. attribute:: Decompress.eof

   A boolean indicating whether the end of the compressed data stream has been
   reached.

   This makes it possible to distinguish between a properly formed compressed
   stream, and an incomplete or truncated one.

   .. versionadded:: 3.3


.. method:: Decompress.decompress(data, max_length=0)

   Decompress *data*, returning a bytes object containing the uncompressed data
   corresponding to at least part of the data in *string*.  This data should be
   concatenated to the output produced by any preceding calls to the
   :meth:`decompress` method.  Some of the input data may be preserved in internal
   buffers for later processing.

   If the optional parameter *max_length* is non-zero then the return value will be
   no longer than *max_length*. This may mean that not all of the compressed input
   can be processed; and unconsumed data will be stored in the attribute
   :attr:`unconsumed_tail`. This bytestring must be passed to a subsequent call to
   :meth:`decompress` if decompression is to continue.  If *max_length* is zero
   then the whole input is decompressed, and :attr:`unconsumed_tail` is empty.

   .. versionchanged:: 3.6
      *max_length* can be used as a keyword argument.


.. method:: Decompress.flush([length])

   All pending input is processed, and a bytes object containing the remaining
   uncompressed output is returned.  After calling :meth:`flush`, the
   :meth:`decompress` method cannot be called again; the only realistic action is
   to delete the object.

   The optional parameter *length* sets the initial size of the output buffer.


.. method:: Decompress.copy()

   Returns a copy of the decompression object.  This can be used to save the state
   of the decompressor midway through the data stream in order to speed up random
   seeks into the stream at a future point.


.. versionchanged:: 3.8
   Added :func:`copy.copy` and :func:`copy.deepcopy` support to decompression
   objects.


Information about the version of the zlib library in use is available through
the following constants:


.. data:: ZLIB_VERSION

   The version string of the zlib library that was used for building the module.
   This may be different from the zlib library actually used at runtime, which
   is available as :const:`ZLIB_RUNTIME_VERSION`.


.. data:: ZLIB_RUNTIME_VERSION

   The version string of the zlib library actually loaded by the interpreter.

   .. versionadded:: 3.3


.. seealso::

   Module :mod:`gzip`
      Reading and writing :program:`gzip`\ -format files.

   http://www.zlib.net
      The zlib library home page.

   http://www.zlib.net/manual.html
      The zlib manual explains  the semantics and usage of the library's many
      functions.


================================================
File: /Doc/library/zoneinfo.rst
================================================
:mod:`!zoneinfo` --- IANA time zone support
===========================================

.. module:: zoneinfo
    :synopsis: IANA time zone support

.. versionadded:: 3.9

.. moduleauthor:: Paul Ganssle <paul@ganssle.io>
.. sectionauthor:: Paul Ganssle <paul@ganssle.io>

**Source code:** :source:`Lib/zoneinfo`

--------------

The :mod:`zoneinfo` module provides a concrete time zone implementation to
support the IANA time zone database as originally specified in :pep:`615`. By
default, :mod:`zoneinfo` uses the system's time zone data if available; if no
system time zone data is available, the library will fall back to using the
first-party :pypi:`tzdata` package available on PyPI.

.. seealso::

    Module: :mod:`datetime`
        Provides the :class:`~datetime.time` and :class:`~datetime.datetime`
        types with which the :class:`ZoneInfo` class is designed to be used.

    Package :pypi:`tzdata`
        First-party package maintained by the CPython core developers to supply
        time zone data via PyPI.

.. include:: ../includes/wasm-notavail.rst

Using ``ZoneInfo``
------------------

:class:`ZoneInfo` is a concrete implementation of the :class:`datetime.tzinfo`
abstract base class, and is intended to be attached to ``tzinfo``, either via
the constructor, the :meth:`datetime.replace <datetime.datetime.replace>`
method or :meth:`datetime.astimezone <datetime.datetime.astimezone>`::

    >>> from zoneinfo import ZoneInfo
    >>> from datetime import datetime, timedelta

    >>> dt = datetime(2020, 10, 31, 12, tzinfo=ZoneInfo("America/Los_Angeles"))
    >>> print(dt)
    2020-10-31 12:00:00-07:00

    >>> dt.tzname()
    'PDT'

Datetimes constructed in this way are compatible with datetime arithmetic and
handle daylight saving time transitions with no further intervention::

    >>> dt_add = dt + timedelta(days=1)

    >>> print(dt_add)
    2020-11-01 12:00:00-08:00

    >>> dt_add.tzname()
    'PST'

These time zones also support the :attr:`~datetime.datetime.fold` attribute
introduced in :pep:`495`.  During offset transitions which induce ambiguous
times (such as a daylight saving time to standard time transition), the offset
from *before* the transition is used when ``fold=0``, and the offset *after*
the transition is used when ``fold=1``, for example::

    >>> dt = datetime(2020, 11, 1, 1, tzinfo=ZoneInfo("America/Los_Angeles"))
    >>> print(dt)
    2020-11-01 01:00:00-07:00

    >>> print(dt.replace(fold=1))
    2020-11-01 01:00:00-08:00

When converting from another time zone, the fold will be set to the correct
value::

    >>> from datetime import timezone
    >>> LOS_ANGELES = ZoneInfo("America/Los_Angeles")
    >>> dt_utc = datetime(2020, 11, 1, 8, tzinfo=timezone.utc)

    >>> # Before the PDT -> PST transition
    >>> print(dt_utc.astimezone(LOS_ANGELES))
    2020-11-01 01:00:00-07:00

    >>> # After the PDT -> PST transition
    >>> print((dt_utc + timedelta(hours=1)).astimezone(LOS_ANGELES))
    2020-11-01 01:00:00-08:00

Data sources
------------

The ``zoneinfo`` module does not directly provide time zone data, and instead
pulls time zone information from the system time zone database or the
first-party PyPI package :pypi:`tzdata`, if available. Some systems, including
notably Windows systems, do not have an IANA database available, and so for
projects targeting cross-platform compatibility that require time zone data, it
is recommended to declare a dependency on tzdata. If neither system data nor
tzdata are available, all calls to :class:`ZoneInfo` will raise
:exc:`ZoneInfoNotFoundError`.

.. _zoneinfo_data_configuration:

Configuring the data sources
****************************

When ``ZoneInfo(key)`` is called, the constructor first searches the
directories specified in :data:`TZPATH` for a file matching ``key``, and on
failure looks for a match in the tzdata package. This behavior can be
configured in three ways:

1. The default :data:`TZPATH` when not otherwise specified can be configured at
   :ref:`compile time <zoneinfo_data_compile_time_config>`.
2. :data:`TZPATH` can be configured using :ref:`an environment variable
   <zoneinfo_data_environment_var>`.
3. At :ref:`runtime <zoneinfo_data_runtime_config>`, the search path can be
   manipulated using the :func:`reset_tzpath` function.

.. _zoneinfo_data_compile_time_config:

Compile-time configuration
^^^^^^^^^^^^^^^^^^^^^^^^^^

The default :data:`TZPATH` includes several common deployment locations for the
time zone database (except on Windows, where there are no "well-known"
locations for time zone data). On POSIX systems, downstream distributors and
those building Python from source who know where their system
time zone data is deployed may change the default time zone path by specifying
the compile-time option ``TZPATH`` (or, more likely, the :option:`configure
flag --with-tzpath <--with-tzpath>`), which should be a string delimited by
:data:`os.pathsep`.

On all platforms, the configured value is available as the ``TZPATH`` key in
:func:`sysconfig.get_config_var`.

.. _zoneinfo_data_environment_var:

Environment configuration
^^^^^^^^^^^^^^^^^^^^^^^^^

When initializing :data:`TZPATH` (either at import time or whenever
:func:`reset_tzpath` is called with no arguments), the ``zoneinfo`` module will
use the environment variable ``PYTHONTZPATH``, if it exists, to set the search
path.

.. envvar:: PYTHONTZPATH

    This is an :data:`os.pathsep`-separated string containing the time zone
    search path to use. It must consist of only absolute rather than relative
    paths. Relative components specified in ``PYTHONTZPATH`` will not be used,
    but otherwise the behavior when a relative path is specified is
    implementation-defined; CPython will raise :exc:`InvalidTZPathWarning`, but
    other implementations are free to silently ignore the erroneous component
    or raise an exception.

To set the system to ignore the system data and use the tzdata package
instead, set ``PYTHONTZPATH=""``.

.. _zoneinfo_data_runtime_config:

Runtime configuration
^^^^^^^^^^^^^^^^^^^^^

The TZ search path can also be configured at runtime using the
:func:`reset_tzpath` function. This is generally not an advisable operation,
though it is reasonable to use it in test functions that require the use of a
specific time zone path (or require disabling access to the system time zones).


The ``ZoneInfo`` class
----------------------

.. class:: ZoneInfo(key)

    A concrete :class:`datetime.tzinfo` subclass that represents an IANA time
    zone specified by the string ``key``. Calls to the primary constructor will
    always return objects that compare identically; put another way, barring
    cache invalidation via :meth:`ZoneInfo.clear_cache`, for all values of
    ``key``, the following assertion will always be true:

    .. code-block:: python

        a = ZoneInfo(key)
        b = ZoneInfo(key)
        assert a is b

    ``key`` must be in the form of a relative, normalized POSIX path, with no
    up-level references. The constructor will raise :exc:`ValueError` if a
    non-conforming key is passed.

    If no file matching ``key`` is found, the constructor will raise
    :exc:`ZoneInfoNotFoundError`.


The ``ZoneInfo`` class has two alternate constructors:

.. classmethod:: ZoneInfo.from_file(fobj, /, key=None)

    Constructs a ``ZoneInfo`` object from a file-like object returning bytes
    (e.g. a file opened in binary mode or an :class:`io.BytesIO` object).
    Unlike the primary constructor, this always constructs a new object.

    The ``key`` parameter sets the name of the zone for the purposes of
    :py:meth:`~object.__str__` and :py:meth:`~object.__repr__`.

    Objects created via this constructor cannot be pickled (see `pickling`_).

.. classmethod:: ZoneInfo.no_cache(key)

    An alternate constructor that bypasses the constructor's cache. It is
    identical to the primary constructor, but returns a new object on each
    call. This is most likely to be useful for testing or demonstration
    purposes, but it can also be used to create a system with a different cache
    invalidation strategy.

    Objects created via this constructor will also bypass the cache of a
    deserializing process when unpickled.

    .. TODO: Add "See `cache_behavior`_" reference when that section is ready.

    .. caution::

        Using this constructor may change the semantics of your datetimes in
        surprising ways, only use it if you know that you need to.

The following class methods are also available:

.. classmethod:: ZoneInfo.clear_cache(*, only_keys=None)

    A method for invalidating the cache on the ``ZoneInfo`` class. If no
    arguments are passed, all caches are invalidated and the next call to
    the primary constructor for each key will return a new instance.

    If an iterable of key names is passed to the ``only_keys`` parameter, only
    the specified keys will be removed from the cache. Keys passed to
    ``only_keys`` but not found in the cache are ignored.

    .. TODO: Add "See `cache_behavior`_" reference when that section is ready.

    .. warning::

        Invoking this function may change the semantics of datetimes using
        ``ZoneInfo`` in surprising ways; this modifies module state
        and thus may have wide-ranging effects. Only use it if you know that you
        need to.

The class has one attribute:

.. attribute:: ZoneInfo.key

    This is a read-only :term:`attribute` that returns the value of ``key``
    passed to the constructor, which should be a lookup key in the IANA time
    zone database (e.g. ``America/New_York``, ``Europe/Paris`` or
    ``Asia/Tokyo``).

    For zones constructed from file without specifying a ``key`` parameter,
    this will be set to ``None``.

    .. note::

        Although it is a somewhat common practice to expose these to end users,
        these values are designed to be primary keys for representing the
        relevant zones and not necessarily user-facing elements.  Projects like
        CLDR (the Unicode Common Locale Data Repository) can be used to get
        more user-friendly strings from these keys.

String representations
**********************

The string representation returned when calling :py:class:`str` on a
:class:`ZoneInfo` object defaults to using the :attr:`ZoneInfo.key` attribute (see
the note on usage in the attribute documentation)::

    >>> zone = ZoneInfo("Pacific/Kwajalein")
    >>> str(zone)
    'Pacific/Kwajalein'

    >>> dt = datetime(2020, 4, 1, 3, 15, tzinfo=zone)
    >>> f"{dt.isoformat()} [{dt.tzinfo}]"
    '2020-04-01T03:15:00+12:00 [Pacific/Kwajalein]'

For objects constructed from a file without specifying a ``key`` parameter,
``str`` falls back to calling :func:`repr`. ``ZoneInfo``'s ``repr`` is
implementation-defined and not necessarily stable between versions, but it is
guaranteed not to be a valid ``ZoneInfo`` key.

.. _pickling:

Pickle serialization
********************

Rather than serializing all transition data, ``ZoneInfo`` objects are
serialized by key, and ``ZoneInfo`` objects constructed from files (even those
with a value for ``key`` specified) cannot be pickled.

The behavior of a ``ZoneInfo`` file depends on how it was constructed:

1. ``ZoneInfo(key)``: When constructed with the primary constructor, a
   ``ZoneInfo`` object is serialized by key, and when deserialized, the
   deserializing process uses the primary and thus it is expected that these
   are expected to be the same object as other references to the same time
   zone.  For example, if ``europe_berlin_pkl`` is a string containing a pickle
   constructed from ``ZoneInfo("Europe/Berlin")``, one would expect the
   following behavior:

   .. code-block:: pycon

       >>> a = ZoneInfo("Europe/Berlin")
       >>> b = pickle.loads(europe_berlin_pkl)
       >>> a is b
       True

2. ``ZoneInfo.no_cache(key)``: When constructed from the cache-bypassing
   constructor, the ``ZoneInfo`` object is also serialized by key, but when
   deserialized, the deserializing process uses the cache bypassing
   constructor. If ``europe_berlin_pkl_nc`` is a string containing a pickle
   constructed from ``ZoneInfo.no_cache("Europe/Berlin")``, one would expect
   the following behavior:

   .. code-block:: pycon

       >>> a = ZoneInfo("Europe/Berlin")
       >>> b = pickle.loads(europe_berlin_pkl_nc)
       >>> a is b
       False

3. ``ZoneInfo.from_file(fobj, /, key=None)``: When constructed from a file, the
   ``ZoneInfo`` object raises an exception on pickling. If an end user wants to
   pickle a ``ZoneInfo`` constructed from a file, it is recommended that they
   use a wrapper type or a custom serialization function: either serializing by
   key or storing the contents of the file object and serializing that.

This method of serialization requires that the time zone data for the required
key be available on both the serializing and deserializing side, similar to the
way that references to classes and functions are expected to exist in both the
serializing and deserializing environments. It also means that no guarantees
are made about the consistency of results when unpickling a ``ZoneInfo``
pickled in an environment with a different version of the time zone data.

Functions
---------

.. function:: available_timezones()

    Get a set containing all the valid keys for IANA time zones available
    anywhere on the time zone path. This is recalculated on every call to the
    function.

    This function only includes canonical zone names and does not include
    "special" zones such as those under the ``posix/`` and ``right/``
    directories, or the ``posixrules`` zone.

    .. caution::

        This function may open a large number of files, as the best way to
        determine if a file on the time zone path is a valid time zone is to
        read the "magic string" at the beginning.

    .. note::

        These values are not designed to be exposed to end-users; for user
        facing elements, applications should use something like CLDR (the
        Unicode Common Locale Data Repository) to get more user-friendly
        strings. See also the cautionary note on :attr:`ZoneInfo.key`.

.. function:: reset_tzpath(to=None)

    Sets or resets the time zone search path (:data:`TZPATH`) for the module.
    When called with no arguments, :data:`TZPATH` is set to the default value.

    Calling ``reset_tzpath`` will not invalidate the :class:`ZoneInfo` cache,
    and so calls to the primary ``ZoneInfo`` constructor will only use the new
    ``TZPATH`` in the case of a cache miss.

    The ``to`` parameter must be a :term:`sequence` of strings or
    :class:`os.PathLike` and not a string, all of which must be absolute paths.
    :exc:`ValueError` will be raised if something other than an absolute path
    is passed.

Globals
-------

.. data:: TZPATH

    A read-only sequence representing the time zone search path -- when
    constructing a ``ZoneInfo`` from a key, the key is joined to each entry in
    the ``TZPATH``, and the first file found is used.

    ``TZPATH`` may contain only absolute paths, never relative paths,
    regardless of how it is configured.

    The object that ``zoneinfo.TZPATH`` points to may change in response to a
    call to :func:`reset_tzpath`, so it is recommended to use
    ``zoneinfo.TZPATH`` rather than importing ``TZPATH`` from ``zoneinfo`` or
    assigning a long-lived variable to ``zoneinfo.TZPATH``.

    For more information on configuring the time zone search path, see
    :ref:`zoneinfo_data_configuration`.

Exceptions and warnings
-----------------------

.. exception:: ZoneInfoNotFoundError

    Raised when construction of a :class:`ZoneInfo` object fails because the
    specified key could not be found on the system. This is a subclass of
    :exc:`KeyError`.

.. exception:: InvalidTZPathWarning

    Raised when :envvar:`PYTHONTZPATH` contains an invalid component that will
    be filtered out, such as a relative path.

.. Links and references:


================================================
File: /Doc/reference/executionmodel.rst
================================================

.. _execmodel:

***************
Execution model
***************

.. index::
   single: execution model
   pair: code; block

.. _prog_structure:

Structure of a program
======================

.. index:: block

A Python program is constructed from code blocks.
A :dfn:`block` is a piece of Python program text that is executed as a unit.
The following are blocks: a module, a function body, and a class definition.
Each command typed interactively is a block.  A script file (a file given as
standard input to the interpreter or specified as a command line argument to the
interpreter) is a code block.  A script command (a command specified on the
interpreter command line with the :option:`-c` option) is a code block.
A module run as a top level script (as module ``__main__``) from the command
line using a :option:`-m` argument is also a code block. The string
argument passed to the built-in functions :func:`eval` and :func:`exec` is a
code block.

.. index:: pair: execution; frame

A code block is executed in an :dfn:`execution frame`.  A frame contains some
administrative information (used for debugging) and determines where and how
execution continues after the code block's execution has completed.

.. _naming:

Naming and binding
==================

.. index::
   single: namespace
   single: scope

.. _bind_names:

Binding of names
----------------

.. index::
   single: name
   pair: binding; name

:dfn:`Names` refer to objects.  Names are introduced by name binding operations.

.. index:: single: from; import statement

The following constructs bind names:

* formal parameters to functions,
* class definitions,
* function definitions,
* assignment expressions,
* :ref:`targets <assignment>` that are identifiers if occurring in
  an assignment:

  + :keyword:`for` loop header,
  + after :keyword:`!as` in a :keyword:`with` statement, :keyword:`except`
    clause, :keyword:`except* <except_star>` clause, or in the as-pattern in structural pattern matching,
  + in a capture pattern in structural pattern matching

* :keyword:`import` statements.
* :keyword:`type` statements.
* :ref:`type parameter lists <type-params>`.

The :keyword:`!import` statement of the form ``from ... import *`` binds all
names defined in the imported module, except those beginning with an underscore.
This form may only be used at the module level.

A target occurring in a :keyword:`del` statement is also considered bound for
this purpose (though the actual semantics are to unbind the name).

Each assignment or import statement occurs within a block defined by a class or
function definition or at the module level (the top-level code block).

.. index:: pair: free; variable

If a name is bound in a block, it is a local variable of that block, unless
declared as :keyword:`nonlocal` or :keyword:`global`.  If a name is bound at
the module level, it is a global variable.  (The variables of the module code
block are local and global.)  If a variable is used in a code block but not
defined there, it is a :term:`free variable`.

Each occurrence of a name in the program text refers to the :dfn:`binding` of
that name established by the following name resolution rules.

.. _resolve_names:

Resolution of names
-------------------

.. index:: scope

A :dfn:`scope` defines the visibility of a name within a block.  If a local
variable is defined in a block, its scope includes that block.  If the
definition occurs in a function block, the scope extends to any blocks contained
within the defining one, unless a contained block introduces a different binding
for the name.

.. index:: single: environment

When a name is used in a code block, it is resolved using the nearest enclosing
scope.  The set of all such scopes visible to a code block is called the block's
:dfn:`environment`.

.. index::
   single: NameError (built-in exception)
   single: UnboundLocalError

When a name is not found at all, a :exc:`NameError` exception is raised.
If the current scope is a function scope, and the name refers to a local
variable that has not yet been bound to a value at the point where the name is
used, an :exc:`UnboundLocalError` exception is raised.
:exc:`UnboundLocalError` is a subclass of :exc:`NameError`.

If a name binding operation occurs anywhere within a code block, all uses of the
name within the block are treated as references to the current block.  This can
lead to errors when a name is used within a block before it is bound.  This rule
is subtle.  Python lacks declarations and allows name binding operations to
occur anywhere within a code block.  The local variables of a code block can be
determined by scanning the entire text of the block for name binding operations.
See :ref:`the FAQ entry on UnboundLocalError <faq-unboundlocalerror>`
for examples.

If the :keyword:`global` statement occurs within a block, all uses of the names
specified in the statement refer to the bindings of those names in the top-level
namespace.  Names are resolved in the top-level namespace by searching the
global namespace, i.e. the namespace of the module containing the code block,
and the builtins namespace, the namespace of the module :mod:`builtins`.  The
global namespace is searched first.  If the names are not found there, the
builtins namespace is searched next. If the names are also not found in the
builtins namespace, new variables are created in the global namespace.
The global statement must precede all uses of the listed names.

The :keyword:`global` statement has the same scope as a name binding operation
in the same block.  If the nearest enclosing scope for a free variable contains
a global statement, the free variable is treated as a global.

.. XXX say more about "nonlocal" semantics here

The :keyword:`nonlocal` statement causes corresponding names to refer
to previously bound variables in the nearest enclosing function scope.
:exc:`SyntaxError` is raised at compile time if the given name does not
exist in any enclosing function scope. :ref:`Type parameters <type-params>`
cannot be rebound with the :keyword:`!nonlocal` statement.

.. index:: pair: module; __main__

The namespace for a module is automatically created the first time a module is
imported.  The main module for a script is always called :mod:`__main__`.

Class definition blocks and arguments to :func:`exec` and :func:`eval` are
special in the context of name resolution.
A class definition is an executable statement that may use and define names.
These references follow the normal rules for name resolution with an exception
that unbound local variables are looked up in the global namespace.
The namespace of the class definition becomes the attribute dictionary of
the class. The scope of names defined in a class block is limited to the
class block; it does not extend to the code blocks of methods. This includes
comprehensions and generator expressions, but it does not include
:ref:`annotation scopes <annotation-scopes>`,
which have access to their enclosing class scopes.
This means that the following will fail::

   class A:
       a = 42
       b = list(a + i for i in range(10))

However, the following will succeed::

   class A:
       type Alias = Nested
       class Nested: pass

   print(A.Alias.__value__)  # <type 'A.Nested'>

.. _annotation-scopes:

Annotation scopes
-----------------

:term:`Annotations <annotation>`, :ref:`type parameter lists <type-params>`
and :keyword:`type` statements
introduce *annotation scopes*, which behave mostly like function scopes,
but with some exceptions discussed below.

Annotation scopes are used in the following contexts:

* :term:`Function annotations <function annotation>`.
* :term:`Variable annotations <variable annotation>`.
* Type parameter lists for :ref:`generic type aliases <generic-type-aliases>`.
* Type parameter lists for :ref:`generic functions <generic-functions>`.
  A generic function's annotations are
  executed within the annotation scope, but its defaults and decorators are not.
* Type parameter lists for :ref:`generic classes <generic-classes>`.
  A generic class's base classes and
  keyword arguments are executed within the annotation scope, but its decorators are not.
* The bounds, constraints, and default values for type parameters
  (:ref:`lazily evaluated <lazy-evaluation>`).
* The value of type aliases (:ref:`lazily evaluated <lazy-evaluation>`).

Annotation scopes differ from function scopes in the following ways:

* Annotation scopes have access to their enclosing class namespace.
  If an annotation scope is immediately within a class scope, or within another
  annotation scope that is immediately within a class scope, the code in the
  annotation scope can use names defined in the class scope as if it were
  executed directly within the class body. This contrasts with regular
  functions defined within classes, which cannot access names defined in the class scope.
* Expressions in annotation scopes cannot contain :keyword:`yield`, ``yield from``,
  :keyword:`await`, or :token:`:= <python-grammar:assignment_expression>`
  expressions. (These expressions are allowed in other scopes contained within the
  annotation scope.)
* Names defined in annotation scopes cannot be rebound with :keyword:`nonlocal`
  statements in inner scopes. This includes only type parameters, as no other
  syntactic elements that can appear within annotation scopes can introduce new names.
* While annotation scopes have an internal name, that name is not reflected in the
  :term:`qualified name` of objects defined within the scope.
  Instead, the :attr:`~definition.__qualname__`
  of such objects is as if the object were defined in the enclosing scope.

.. versionadded:: 3.12
   Annotation scopes were introduced in Python 3.12 as part of :pep:`695`.

.. versionchanged:: 3.13
   Annotation scopes are also used for type parameter defaults, as
   introduced by :pep:`696`.

.. versionchanged:: 3.14
   Annotation scopes are now also used for annotations, as specified in
   :pep:`649` and :pep:`749`.

.. _lazy-evaluation:

Lazy evaluation
---------------

Most annotation scopes are *lazily evaluated*. This includes annotations,
the values of type aliases created through the :keyword:`type` statement, and
the bounds, constraints, and default values of type
variables created through the :ref:`type parameter syntax <type-params>`.
This means that they are not evaluated when the type alias or type variable is
created, or when the object carrying annotations is created. Instead, they
are only evaluated when necessary, for example when the ``__value__``
attribute on a type alias is accessed.

Example:

.. doctest::

   >>> type Alias = 1/0
   >>> Alias.__value__
   Traceback (most recent call last):
     ...
   ZeroDivisionError: division by zero
   >>> def func[T: 1/0](): pass
   >>> T = func.__type_params__[0]
   >>> T.__bound__
   Traceback (most recent call last):
     ...
   ZeroDivisionError: division by zero

Here the exception is raised only when the ``__value__`` attribute
of the type alias or the ``__bound__`` attribute of the type variable
is accessed.

This behavior is primarily useful for references to types that have not
yet been defined when the type alias or type variable is created. For example,
lazy evaluation enables creation of mutually recursive type aliases::

   from typing import Literal

   type SimpleExpr = int | Parenthesized
   type Parenthesized = tuple[Literal["("], Expr, Literal[")"]]
   type Expr = SimpleExpr | tuple[SimpleExpr, Literal["+", "-"], Expr]

Lazily evaluated values are evaluated in :ref:`annotation scope <annotation-scopes>`,
which means that names that appear inside the lazily evaluated value are looked up
as if they were used in the immediately enclosing scope.

.. versionadded:: 3.12

.. _restrict_exec:

Builtins and restricted execution
---------------------------------

.. index:: pair: restricted; execution

.. impl-detail::

   Users should not touch ``__builtins__``; it is strictly an implementation
   detail.  Users wanting to override values in the builtins namespace should
   :keyword:`import` the :mod:`builtins` module and modify its
   attributes appropriately.

The builtins namespace associated with the execution of a code block
is actually found by looking up the name ``__builtins__`` in its
global namespace; this should be a dictionary or a module (in the
latter case the module's dictionary is used).  By default, when in the
:mod:`__main__` module, ``__builtins__`` is the built-in module
:mod:`builtins`; when in any other module, ``__builtins__`` is an
alias for the dictionary of the :mod:`builtins` module itself.


.. _dynamic-features:

Interaction with dynamic features
---------------------------------

Name resolution of free variables occurs at runtime, not at compile time.
This means that the following code will print 42::

   i = 10
   def f():
       print(i)
   i = 42
   f()

.. XXX from * also invalid with relative imports (at least currently)

The :func:`eval` and :func:`exec` functions do not have access to the full
environment for resolving names.  Names may be resolved in the local and global
namespaces of the caller.  Free variables are not resolved in the nearest
enclosing namespace, but in the global namespace.  [#]_ The :func:`exec` and
:func:`eval` functions have optional arguments to override the global and local
namespace.  If only one namespace is specified, it is used for both.

.. XXX(ncoghlan) above is only accurate for string execution. When executing code objects,
   closure cells may now be passed explicitly to resolve co_freevars references.
   Docs issue: https://github.com/python/cpython/issues/122826

.. _exceptions:

Exceptions
==========

.. index:: single: exception

.. index::
   single: raise an exception
   single: handle an exception
   single: exception handler
   single: errors
   single: error handling

Exceptions are a means of breaking out of the normal flow of control of a code
block in order to handle errors or other exceptional conditions.  An exception
is *raised* at the point where the error is detected; it may be *handled* by the
surrounding code block or by any code block that directly or indirectly invoked
the code block where the error occurred.

The Python interpreter raises an exception when it detects a run-time error
(such as division by zero).  A Python program can also explicitly raise an
exception with the :keyword:`raise` statement. Exception handlers are specified
with the :keyword:`try` ... :keyword:`except` statement.  The :keyword:`finally`
clause of such a statement can be used to specify cleanup code which does not
handle the exception, but is executed whether an exception occurred or not in
the preceding code.

.. index:: single: termination model

Python uses the "termination" model of error handling: an exception handler can
find out what happened and continue execution at an outer level, but it cannot
repair the cause of the error and retry the failing operation (except by
re-entering the offending piece of code from the top).

.. index:: single: SystemExit (built-in exception)

When an exception is not handled at all, the interpreter terminates execution of
the program, or returns to its interactive main loop.  In either case, it prints
a stack traceback, except when the exception is :exc:`SystemExit`.

Exceptions are identified by class instances.  The :keyword:`except` clause is
selected depending on the class of the instance: it must reference the class of
the instance or a :term:`non-virtual base class <abstract base class>` thereof.
The instance can be received by the handler and can carry additional information
about the exceptional condition.

.. note::

   Exception messages are not part of the Python API.  Their contents may change
   from one version of Python to the next without warning and should not be
   relied on by code which will run under multiple versions of the interpreter.

See also the description of the :keyword:`try` statement in section :ref:`try`
and :keyword:`raise` statement in section :ref:`raise`.


.. rubric:: Footnotes

.. [#] This limitation occurs because the code that is executed by these operations
       is not available at the time the module is compiled.


================================================
File: /Doc/reference/grammar.rst
================================================
.. _full-grammar-specification:

Full Grammar specification
==========================

This is the full Python grammar, derived directly from the grammar
used to generate the CPython parser (see :source:`Grammar/python.gram`).
The version here omits details related to code generation and
error recovery.

The notation is a mixture of `EBNF
<https://en.wikipedia.org/wiki/Extended_Backus%E2%80%93Naur_form>`_
and `PEG <https://en.wikipedia.org/wiki/Parsing_expression_grammar>`_.
In particular, ``&`` followed by a symbol, token or parenthesized
group indicates a positive lookahead (i.e., is required to match but
not consumed), while ``!`` indicates a negative lookahead (i.e., is
required *not* to match).  We use the ``|`` separator to mean PEG's
"ordered choice" (written as ``/`` in traditional PEG grammars). See
:pep:`617` for more details on the grammar's syntax.

.. literalinclude:: ../../Grammar/python.gram
  :language: peg


================================================
File: /Doc/reference/import.rst
================================================

.. _importsystem:

*****************
The import system
*****************

.. index:: single: import machinery

Python code in one :term:`module` gains access to the code in another module
by the process of :term:`importing` it.  The :keyword:`import` statement is
the most common way of invoking the import machinery, but it is not the only
way.  Functions such as :func:`importlib.import_module` and built-in
:func:`__import__` can also be used to invoke the import machinery.

The :keyword:`import` statement combines two operations; it searches for the
named module, then it binds the results of that search to a name in the local
scope.  The search operation of the :keyword:`!import` statement is defined as
a call to the :func:`__import__` function, with the appropriate arguments.
The return value of :func:`__import__` is used to perform the name
binding operation of the :keyword:`!import` statement.  See the
:keyword:`!import` statement for the exact details of that name binding
operation.

A direct call to :func:`__import__` performs only the module search and, if
found, the module creation operation.  While certain side-effects may occur,
such as the importing of parent packages, and the updating of various caches
(including :data:`sys.modules`), only the :keyword:`import` statement performs
a name binding operation.

When an :keyword:`import` statement is executed, the standard builtin
:func:`__import__` function is called. Other mechanisms for invoking the
import system (such as :func:`importlib.import_module`) may choose to bypass
:func:`__import__` and use their own solutions to implement import semantics.

When a module is first imported, Python searches for the module and if found,
it creates a module object [#fnmo]_, initializing it.  If the named module
cannot be found, a :exc:`ModuleNotFoundError` is raised.  Python implements various
strategies to search for the named module when the import machinery is
invoked.  These strategies can be modified and extended by using various hooks
described in the sections below.

.. versionchanged:: 3.3
   The import system has been updated to fully implement the second phase
   of :pep:`302`. There is no longer any implicit import machinery - the full
   import system is exposed through :data:`sys.meta_path`. In addition,
   native namespace package support has been implemented (see :pep:`420`).


:mod:`importlib`
================

The :mod:`importlib` module provides a rich API for interacting with the
import system.  For example :func:`importlib.import_module` provides a
recommended, simpler API than built-in :func:`__import__` for invoking the
import machinery.  Refer to the :mod:`importlib` library documentation for
additional detail.



Packages
========

.. index::
    single: package

Python has only one type of module object, and all modules are of this type,
regardless of whether the module is implemented in Python, C, or something
else.  To help organize modules and provide a naming hierarchy, Python has a
concept of :term:`packages <package>`.

You can think of packages as the directories on a file system and modules as
files within directories, but don't take this analogy too literally since
packages and modules need not originate from the file system.  For the
purposes of this documentation, we'll use this convenient analogy of
directories and files.  Like file system directories, packages are organized
hierarchically, and packages may themselves contain subpackages, as well as
regular modules.

It's important to keep in mind that all packages are modules, but not all
modules are packages.  Or put another way, packages are just a special kind of
module.  Specifically, any module that contains a ``__path__`` attribute is
considered a package.

All modules have a name.  Subpackage names are separated from their parent
package name by a dot, akin to Python's standard attribute access syntax.  Thus
you might have a package called :mod:`email`, which in turn has a subpackage
called :mod:`email.mime` and a module within that subpackage called
:mod:`email.mime.text`.


Regular packages
----------------

.. index::
    pair: package; regular

Python defines two types of packages, :term:`regular packages <regular
package>` and :term:`namespace packages <namespace package>`.  Regular
packages are traditional packages as they existed in Python 3.2 and earlier.
A regular package is typically implemented as a directory containing an
``__init__.py`` file.  When a regular package is imported, this
``__init__.py`` file is implicitly executed, and the objects it defines are
bound to names in the package's namespace.  The ``__init__.py`` file can
contain the same Python code that any other module can contain, and Python
will add some additional attributes to the module when it is imported.

For example, the following file system layout defines a top level ``parent``
package with three subpackages::

    parent/
        __init__.py
        one/
            __init__.py
        two/
            __init__.py
        three/
            __init__.py

Importing ``parent.one`` will implicitly execute ``parent/__init__.py`` and
``parent/one/__init__.py``.  Subsequent imports of ``parent.two`` or
``parent.three`` will execute ``parent/two/__init__.py`` and
``parent/three/__init__.py`` respectively.


Namespace packages
------------------

.. index::
    pair: package; namespace
    pair: package; portion

A namespace package is a composite of various :term:`portions <portion>`,
where each portion contributes a subpackage to the parent package.  Portions
may reside in different locations on the file system.  Portions may also be
found in zip files, on the network, or anywhere else that Python searches
during import.  Namespace packages may or may not correspond directly to
objects on the file system; they may be virtual modules that have no concrete
representation.

Namespace packages do not use an ordinary list for their ``__path__``
attribute. They instead use a custom iterable type which will automatically
perform a new search for package portions on the next import attempt within
that package if the path of their parent package (or :data:`sys.path` for a
top level package) changes.

With namespace packages, there is no ``parent/__init__.py`` file.  In fact,
there may be multiple ``parent`` directories found during import search, where
each one is provided by a different portion.  Thus ``parent/one`` may not be
physically located next to ``parent/two``.  In this case, Python will create a
namespace package for the top-level ``parent`` package whenever it or one of
its subpackages is imported.

See also :pep:`420` for the namespace package specification.


Searching
=========

To begin the search, Python needs the :term:`fully qualified <qualified name>`
name of the module (or package, but for the purposes of this discussion, the
difference is immaterial) being imported.  This name may come from various
arguments to the :keyword:`import` statement, or from the parameters to the
:func:`importlib.import_module` or :func:`__import__` functions.

This name will be used in various phases of the import search, and it may be
the dotted path to a submodule, e.g. ``foo.bar.baz``.  In this case, Python
first tries to import ``foo``, then ``foo.bar``, and finally ``foo.bar.baz``.
If any of the intermediate imports fail, a :exc:`ModuleNotFoundError` is raised.


The module cache
----------------

.. index::
    single: sys.modules

The first place checked during import search is :data:`sys.modules`.  This
mapping serves as a cache of all modules that have been previously imported,
including the intermediate paths.  So if ``foo.bar.baz`` was previously
imported, :data:`sys.modules` will contain entries for ``foo``, ``foo.bar``,
and ``foo.bar.baz``.  Each key will have as its value the corresponding module
object.

During import, the module name is looked up in :data:`sys.modules` and if
present, the associated value is the module satisfying the import, and the
process completes.  However, if the value is ``None``, then a
:exc:`ModuleNotFoundError` is raised.  If the module name is missing, Python will
continue searching for the module.

:data:`sys.modules` is writable.  Deleting a key may not destroy the
associated module (as other modules may hold references to it),
but it will invalidate the cache entry for the named module, causing
Python to search anew for the named module upon its next
import. The key can also be assigned to ``None``, forcing the next import
of the module to result in a :exc:`ModuleNotFoundError`.

Beware though, as if you keep a reference to the module object,
invalidate its cache entry in :data:`sys.modules`, and then re-import the
named module, the two module objects will *not* be the same. By contrast,
:func:`importlib.reload` will reuse the *same* module object, and simply
reinitialise the module contents by rerunning the module's code.


.. _finders-and-loaders:

Finders and loaders
-------------------

.. index::
    single: finder
    single: loader
    single: module spec

If the named module is not found in :data:`sys.modules`, then Python's import
protocol is invoked to find and load the module.  This protocol consists of
two conceptual objects, :term:`finders <finder>` and :term:`loaders <loader>`.
A finder's job is to determine whether it can find the named module using
whatever strategy it knows about. Objects that implement both of these
interfaces are referred to as :term:`importers <importer>` - they return
themselves when they find that they can load the requested module.

Python includes a number of default finders and importers.  The first one
knows how to locate built-in modules, and the second knows how to locate
frozen modules.  A third default finder searches an :term:`import path`
for modules.  The :term:`import path` is a list of locations that may
name file system paths or zip files.  It can also be extended to search
for any locatable resource, such as those identified by URLs.

The import machinery is extensible, so new finders can be added to extend the
range and scope of module searching.

Finders do not actually load modules.  If they can find the named module, they
return a :dfn:`module spec`, an encapsulation of the module's import-related
information, which the import machinery then uses when loading the module.

The following sections describe the protocol for finders and loaders in more
detail, including how you can create and register new ones to extend the
import machinery.

.. versionchanged:: 3.4
   In previous versions of Python, finders returned :term:`loaders <loader>`
   directly, whereas now they return module specs which *contain* loaders.
   Loaders are still used during import but have fewer responsibilities.

Import hooks
------------

.. index::
   single: import hooks
   single: meta hooks
   single: path hooks
   pair: hooks; import
   pair: hooks; meta
   pair: hooks; path

The import machinery is designed to be extensible; the primary mechanism for
this are the *import hooks*.  There are two types of import hooks: *meta
hooks* and *import path hooks*.

Meta hooks are called at the start of import processing, before any other
import processing has occurred, other than :data:`sys.modules` cache look up.
This allows meta hooks to override :data:`sys.path` processing, frozen
modules, or even built-in modules.  Meta hooks are registered by adding new
finder objects to :data:`sys.meta_path`, as described below.

Import path hooks are called as part of :data:`sys.path` (or
``package.__path__``) processing, at the point where their associated path
item is encountered.  Import path hooks are registered by adding new callables
to :data:`sys.path_hooks` as described below.


The meta path
-------------

.. index::
    single: sys.meta_path
    pair: finder; find_spec

When the named module is not found in :data:`sys.modules`, Python next
searches :data:`sys.meta_path`, which contains a list of meta path finder
objects.  These finders are queried in order to see if they know how to handle
the named module.  Meta path finders must implement a method called
:meth:`~importlib.abc.MetaPathFinder.find_spec` which takes three arguments:
a name, an import path, and (optionally) a target module.  The meta path
finder can use any strategy it wants to determine whether it can handle
the named module or not.

If the meta path finder knows how to handle the named module, it returns a
spec object.  If it cannot handle the named module, it returns ``None``.  If
:data:`sys.meta_path` processing reaches the end of its list without returning
a spec, then a :exc:`ModuleNotFoundError` is raised.  Any other exceptions
raised are simply propagated up, aborting the import process.

The :meth:`~importlib.abc.MetaPathFinder.find_spec` method of meta path
finders is called with two or three arguments.  The first is the fully
qualified name of the module being imported, for example ``foo.bar.baz``.
The second argument is the path entries to use for the module search.  For
top-level modules, the second argument is ``None``, but for submodules or
subpackages, the second argument is the value of the parent package's
``__path__`` attribute. If the appropriate ``__path__`` attribute cannot
be accessed, a :exc:`ModuleNotFoundError` is raised.  The third argument
is an existing module object that will be the target of loading later.
The import system passes in a target module only during reload.

The meta path may be traversed multiple times for a single import request.
For example, assuming none of the modules involved has already been cached,
importing ``foo.bar.baz`` will first perform a top level import, calling
``mpf.find_spec("foo", None, None)`` on each meta path finder (``mpf``). After
``foo`` has been imported, ``foo.bar`` will be imported by traversing the
meta path a second time, calling
``mpf.find_spec("foo.bar", foo.__path__, None)``. Once ``foo.bar`` has been
imported, the final traversal will call
``mpf.find_spec("foo.bar.baz", foo.bar.__path__, None)``.

Some meta path finders only support top level imports. These importers will
always return ``None`` when anything other than ``None`` is passed as the
second argument.

Python's default :data:`sys.meta_path` has three meta path finders, one that
knows how to import built-in modules, one that knows how to import frozen
modules, and one that knows how to import modules from an :term:`import path`
(i.e. the :term:`path based finder`).

.. versionchanged:: 3.4
   The :meth:`~importlib.abc.MetaPathFinder.find_spec` method of meta path
   finders replaced :meth:`!find_module`, which
   is now deprecated.  While it will continue to work without change, the
   import machinery will try it only if the finder does not implement
   :meth:`~importlib.abc.MetaPathFinder.find_spec`.

.. versionchanged:: 3.10
   Use of :meth:`!find_module` by the import system
   now raises :exc:`ImportWarning`.

.. versionchanged:: 3.12
   :meth:`!find_module` has been removed.
   Use :meth:`~importlib.abc.MetaPathFinder.find_spec` instead.


Loading
=======

If and when a module spec is found, the import machinery will use it (and
the loader it contains) when loading the module.  Here is an approximation
of what happens during the loading portion of import::

    module = None
    if spec.loader is not None and hasattr(spec.loader, 'create_module'):
        # It is assumed 'exec_module' will also be defined on the loader.
        module = spec.loader.create_module(spec)
    if module is None:
        module = ModuleType(spec.name)
    # The import-related module attributes get set here:
    _init_module_attrs(spec, module)

    if spec.loader is None:
        # unsupported
        raise ImportError
    if spec.origin is None and spec.submodule_search_locations is not None:
        # namespace package
        sys.modules[spec.name] = module
    elif not hasattr(spec.loader, 'exec_module'):
        module = spec.loader.load_module(spec.name)
    else:
        sys.modules[spec.name] = module
        try:
            spec.loader.exec_module(module)
        except BaseException:
            try:
                del sys.modules[spec.name]
            except KeyError:
                pass
            raise
    return sys.modules[spec.name]

Note the following details:

* If there is an existing module object with the given name in
  :data:`sys.modules`, import will have already returned it.

* The module will exist in :data:`sys.modules` before the loader
  executes the module code.  This is crucial because the module code may
  (directly or indirectly) import itself; adding it to :data:`sys.modules`
  beforehand prevents unbounded recursion in the worst case and multiple
  loading in the best.

* If loading fails, the failing module -- and only the failing module --
  gets removed from :data:`sys.modules`.  Any module already in the
  :data:`sys.modules` cache, and any module that was successfully loaded
  as a side-effect, must remain in the cache.  This contrasts with
  reloading where even the failing module is left in :data:`sys.modules`.

* After the module is created but before execution, the import machinery
  sets the import-related module attributes ("_init_module_attrs" in
  the pseudo-code example above), as summarized in a
  :ref:`later section <import-mod-attrs>`.

* Module execution is the key moment of loading in which the module's
  namespace gets populated.  Execution is entirely delegated to the
  loader, which gets to decide what gets populated and how.

* The module created during loading and passed to exec_module() may
  not be the one returned at the end of import [#fnlo]_.

.. versionchanged:: 3.4
   The import system has taken over the boilerplate responsibilities of
   loaders.  These were previously performed by the
   :meth:`importlib.abc.Loader.load_module` method.

Loaders
-------

Module loaders provide the critical function of loading: module execution.
The import machinery calls the :meth:`importlib.abc.Loader.exec_module`
method with a single argument, the module object to execute.  Any value
returned from :meth:`~importlib.abc.Loader.exec_module` is ignored.

Loaders must satisfy the following requirements:

* If the module is a Python module (as opposed to a built-in module or a
  dynamically loaded extension), the loader should execute the module's code
  in the module's global name space (``module.__dict__``).

* If the loader cannot execute the module, it should raise an
  :exc:`ImportError`, although any other exception raised during
  :meth:`~importlib.abc.Loader.exec_module` will be propagated.

In many cases, the finder and loader can be the same object; in such cases the
:meth:`~importlib.abc.MetaPathFinder.find_spec` method would just return a
spec with the loader set to ``self``.

Module loaders may opt in to creating the module object during loading
by implementing a :meth:`~importlib.abc.Loader.create_module` method.
It takes one argument, the module spec, and returns the new module object
to use during loading.  ``create_module()`` does not need to set any attributes
on the module object.  If the method returns ``None``, the
import machinery will create the new module itself.

.. versionadded:: 3.4
   The :meth:`~importlib.abc.Loader.create_module` method of loaders.

.. versionchanged:: 3.4
   The :meth:`~importlib.abc.Loader.load_module` method was replaced by
   :meth:`~importlib.abc.Loader.exec_module` and the import
   machinery assumed all the boilerplate responsibilities of loading.

   For compatibility with existing loaders, the import machinery will use
   the ``load_module()`` method of loaders if it exists and the loader does
   not also implement ``exec_module()``.  However, ``load_module()`` has been
   deprecated and loaders should implement ``exec_module()`` instead.

   The ``load_module()`` method must implement all the boilerplate loading
   functionality described above in addition to executing the module.  All
   the same constraints apply, with some additional clarification:

   * If there is an existing module object with the given name in
     :data:`sys.modules`, the loader must use that existing module.
     (Otherwise, :func:`importlib.reload` will not work correctly.)  If the
     named module does not exist in :data:`sys.modules`, the loader
     must create a new module object and add it to :data:`sys.modules`.

   * The module *must* exist in :data:`sys.modules` before the loader
     executes the module code, to prevent unbounded recursion or multiple
     loading.

   * If loading fails, the loader must remove any modules it has inserted
     into :data:`sys.modules`, but it must remove **only** the failing
     module(s), and only if the loader itself has loaded the module(s)
     explicitly.

.. versionchanged:: 3.5
   A :exc:`DeprecationWarning` is raised when ``exec_module()`` is defined but
   ``create_module()`` is not.

.. versionchanged:: 3.6
   An :exc:`ImportError` is raised when ``exec_module()`` is defined but
   ``create_module()`` is not.

.. versionchanged:: 3.10
   Use of ``load_module()`` will raise :exc:`ImportWarning`.

Submodules
----------

When a submodule is loaded using any mechanism (e.g. ``importlib`` APIs, the
``import`` or ``import-from`` statements, or built-in ``__import__()``) a
binding is placed in the parent module's namespace to the submodule object.
For example, if package ``spam`` has a submodule ``foo``, after importing
``spam.foo``, ``spam`` will have an attribute ``foo`` which is bound to the
submodule.  Let's say you have the following directory structure::

    spam/
        __init__.py
        foo.py

and ``spam/__init__.py`` has the following line in it::

    from .foo import Foo

then executing the following puts name bindings for ``foo`` and ``Foo`` in the
``spam`` module::

    >>> import spam
    >>> spam.foo
    <module 'spam.foo' from '/tmp/imports/spam/foo.py'>
    >>> spam.Foo
    <class 'spam.foo.Foo'>

Given Python's familiar name binding rules this might seem surprising, but
it's actually a fundamental feature of the import system.  The invariant
holding is that if you have ``sys.modules['spam']`` and
``sys.modules['spam.foo']`` (as you would after the above import), the latter
must appear as the ``foo`` attribute of the former.

.. _module-specs:

Module specs
------------

The import machinery uses a variety of information about each module
during import, especially before loading.  Most of the information is
common to all modules.  The purpose of a module's spec is to encapsulate
this import-related information on a per-module basis.

Using a spec during import allows state to be transferred between import
system components, e.g. between the finder that creates the module spec
and the loader that executes it.  Most importantly, it allows the
import machinery to perform the boilerplate operations of loading,
whereas without a module spec the loader had that responsibility.

The module's spec is exposed as :attr:`module.__spec__`. Setting
:attr:`!__spec__` appropriately applies equally to
:ref:`modules initialized during interpreter startup <programs>`.
The one exception is ``__main__``, where :attr:`!__spec__` is
:ref:`set to None in some cases <main_spec>`.

See :class:`~importlib.machinery.ModuleSpec` for details on the contents of
the module spec.

.. versionadded:: 3.4

.. _package-path-rules:

__path__ attributes on modules
------------------------------

The :attr:`~module.__path__` attribute should be a (possibly empty)
:term:`sequence` of strings enumerating the locations where the package's
submodules will be found. By definition, if a module has a :attr:`!__path__`
attribute, it is a :term:`package`.

A package's :attr:`~module.__path__` attribute is used during imports of its
subpackages.
Within the import machinery, it functions much the same as :data:`sys.path`,
i.e. providing a list of locations to search for modules during import.
However, :attr:`!__path__` is typically much more constrained than
:data:`!sys.path`.

The same rules used for :data:`sys.path` also apply to a package's
:attr:`!__path__`. :data:`sys.path_hooks` (described below) are
consulted when traversing a package's :attr:`!__path__`.

A package's ``__init__.py`` file may set or alter the package's
:attr:`~module.__path__`
attribute, and this was typically the way namespace packages were implemented
prior to :pep:`420`.  With the adoption of :pep:`420`, namespace packages no
longer need to supply ``__init__.py`` files containing only :attr:`!__path__`
manipulation code; the import machinery automatically sets :attr:`!__path__`
correctly for the namespace package.

Module reprs
------------

By default, all modules have a usable repr, however depending on the
attributes set above, and in the module's spec, you can more explicitly
control the repr of module objects.

If the module has a spec (``__spec__``), the import machinery will try
to generate a repr from it.  If that fails or there is no spec, the import
system will craft a default repr using whatever information is available
on the module.  It will try to use the ``module.__name__``,
``module.__file__``, and ``module.__loader__`` as input into the repr,
with defaults for whatever information is missing.

Here are the exact rules used:

* If the module has a ``__spec__`` attribute, the information in the spec
  is used to generate the repr.  The "name", "loader", "origin", and
  "has_location" attributes are consulted.

* If the module has a ``__file__`` attribute, this is used as part of the
  module's repr.

* If the module has no ``__file__`` but does have a ``__loader__`` that is not
  ``None``, then the loader's repr is used as part of the module's repr.

* Otherwise, just use the module's ``__name__`` in the repr.

.. versionchanged:: 3.12
   Use of :meth:`!module_repr`, having been deprecated since Python 3.4, was
   removed in Python 3.12 and is no longer called during the resolution of a
   module's repr.

.. _pyc-invalidation:

Cached bytecode invalidation
----------------------------

Before Python loads cached bytecode from a ``.pyc`` file, it checks whether the
cache is up-to-date with the source ``.py`` file. By default, Python does this
by storing the source's last-modified timestamp and size in the cache file when
writing it. At runtime, the import system then validates the cache file by
checking the stored metadata in the cache file against the source's
metadata.

Python also supports "hash-based" cache files, which store a hash of the source
file's contents rather than its metadata. There are two variants of hash-based
``.pyc`` files: checked and unchecked. For checked hash-based ``.pyc`` files,
Python validates the cache file by hashing the source file and comparing the
resulting hash with the hash in the cache file. If a checked hash-based cache
file is found to be invalid, Python regenerates it and writes a new checked
hash-based cache file. For unchecked hash-based ``.pyc`` files, Python simply
assumes the cache file is valid if it exists. Hash-based ``.pyc`` files
validation behavior may be overridden with the :option:`--check-hash-based-pycs`
flag.

.. versionchanged:: 3.7
   Added hash-based ``.pyc`` files. Previously, Python only supported
   timestamp-based invalidation of bytecode caches.


The Path Based Finder
=====================

.. index::
    single: path based finder

As mentioned previously, Python comes with several default meta path finders.
One of these, called the :term:`path based finder`
(:class:`~importlib.machinery.PathFinder`), searches an :term:`import path`,
which contains a list of :term:`path entries <path entry>`.  Each path
entry names a location to search for modules.

The path based finder itself doesn't know how to import anything. Instead, it
traverses the individual path entries, associating each of them with a
path entry finder that knows how to handle that particular kind of path.

The default set of path entry finders implement all the semantics for finding
modules on the file system, handling special file types such as Python source
code (``.py`` files), Python byte code (``.pyc`` files) and
shared libraries (e.g. ``.so`` files). When supported by the :mod:`zipimport`
module in the standard library, the default path entry finders also handle
loading all of these file types (other than shared libraries) from zipfiles.

Path entries need not be limited to file system locations.  They can refer to
URLs, database queries, or any other location that can be specified as a
string.

The path based finder provides additional hooks and protocols so that you
can extend and customize the types of searchable path entries.  For example,
if you wanted to support path entries as network URLs, you could write a hook
that implements HTTP semantics to find modules on the web.  This hook (a
callable) would return a :term:`path entry finder` supporting the protocol
described below, which was then used to get a loader for the module from the
web.

A word of warning: this section and the previous both use the term *finder*,
distinguishing between them by using the terms :term:`meta path finder` and
:term:`path entry finder`.  These two types of finders are very similar,
support similar protocols, and function in similar ways during the import
process, but it's important to keep in mind that they are subtly different.
In particular, meta path finders operate at the beginning of the import
process, as keyed off the :data:`sys.meta_path` traversal.

By contrast, path entry finders are in a sense an implementation detail
of the path based finder, and in fact, if the path based finder were to be
removed from :data:`sys.meta_path`, none of the path entry finder semantics
would be invoked.


Path entry finders
------------------

.. index::
    single: sys.path
    single: sys.path_hooks
    single: sys.path_importer_cache
    single: PYTHONPATH

The :term:`path based finder` is responsible for finding and loading
Python modules and packages whose location is specified with a string
:term:`path entry`.  Most path entries name locations in the file system,
but they need not be limited to this.

As a meta path finder, the :term:`path based finder` implements the
:meth:`~importlib.abc.MetaPathFinder.find_spec` protocol previously
described, however it exposes additional hooks that can be used to
customize how modules are found and loaded from the :term:`import path`.

Three variables are used by the :term:`path based finder`, :data:`sys.path`,
:data:`sys.path_hooks` and :data:`sys.path_importer_cache`.  The ``__path__``
attributes on package objects are also used.  These provide additional ways
that the import machinery can be customized.

:data:`sys.path` contains a list of strings providing search locations for
modules and packages.  It is initialized from the :envvar:`PYTHONPATH`
environment variable and various other installation- and
implementation-specific defaults.  Entries in :data:`sys.path` can name
directories on the file system, zip files, and potentially other "locations"
(see the :mod:`site` module) that should be searched for modules, such as
URLs, or database queries.  Only strings should be present on
:data:`sys.path`; all other data types are ignored.

The :term:`path based finder` is a :term:`meta path finder`, so the import
machinery begins the :term:`import path` search by calling the path
based finder's :meth:`~importlib.machinery.PathFinder.find_spec` method as
described previously.  When the ``path`` argument to
:meth:`~importlib.machinery.PathFinder.find_spec` is given, it will be a
list of string paths to traverse - typically a package's ``__path__``
attribute for an import within that package.  If the ``path`` argument is
``None``, this indicates a top level import and :data:`sys.path` is used.

The path based finder iterates over every entry in the search path, and
for each of these, looks for an appropriate :term:`path entry finder`
(:class:`~importlib.abc.PathEntryFinder`) for the
path entry.  Because this can be an expensive operation (e.g. there may be
``stat()`` call overheads for this search), the path based finder maintains
a cache mapping path entries to path entry finders.  This cache is maintained
in :data:`sys.path_importer_cache` (despite the name, this cache actually
stores finder objects rather than being limited to :term:`importer` objects).
In this way, the expensive search for a particular :term:`path entry`
location's :term:`path entry finder` need only be done once.  User code is
free to remove cache entries from :data:`sys.path_importer_cache` forcing
the path based finder to perform the path entry search again.

If the path entry is not present in the cache, the path based finder iterates
over every callable in :data:`sys.path_hooks`.  Each of the :term:`path entry
hooks <path entry hook>` in this list is called with a single argument, the
path entry to be searched.  This callable may either return a :term:`path
entry finder` that can handle the path entry, or it may raise
:exc:`ImportError`.  An :exc:`ImportError` is used by the path based finder to
signal that the hook cannot find a :term:`path entry finder`
for that :term:`path entry`.  The
exception is ignored and :term:`import path` iteration continues.  The hook
should expect either a string or bytes object; the encoding of bytes objects
is up to the hook (e.g. it may be a file system encoding, UTF-8, or something
else), and if the hook cannot decode the argument, it should raise
:exc:`ImportError`.

If :data:`sys.path_hooks` iteration ends with no :term:`path entry finder`
being returned, then the path based finder's
:meth:`~importlib.machinery.PathFinder.find_spec` method will store ``None``
in :data:`sys.path_importer_cache` (to indicate that there is no finder for
this path entry) and return ``None``, indicating that this
:term:`meta path finder` could not find the module.

If a :term:`path entry finder` *is* returned by one of the :term:`path entry
hook` callables on :data:`sys.path_hooks`, then the following protocol is used
to ask the finder for a module spec, which is then used when loading the
module.

The current working directory -- denoted by an empty string -- is handled
slightly differently from other entries on :data:`sys.path`. First, if the
current working directory is found to not exist, no value is stored in
:data:`sys.path_importer_cache`. Second, the value for the current working
directory is looked up fresh for each module lookup. Third, the path used for
:data:`sys.path_importer_cache` and returned by
:meth:`importlib.machinery.PathFinder.find_spec` will be the actual current
working directory and not the empty string.

Path entry finder protocol
--------------------------

In order to support imports of modules and initialized packages and also to
contribute portions to namespace packages, path entry finders must implement
the :meth:`~importlib.abc.PathEntryFinder.find_spec` method.

:meth:`~importlib.abc.PathEntryFinder.find_spec` takes two arguments: the
fully qualified name of the module being imported, and the (optional) target
module.  ``find_spec()`` returns a fully populated spec for the module.
This spec will always have "loader" set (with one exception).

To indicate to the import machinery that the spec represents a namespace
:term:`portion`, the path entry finder sets ``submodule_search_locations`` to
a list containing the portion.

.. versionchanged:: 3.4
   :meth:`~importlib.abc.PathEntryFinder.find_spec` replaced
   :meth:`!find_loader` and
   :meth:`!find_module`, both of which
   are now deprecated, but will be used if ``find_spec()`` is not defined.

   Older path entry finders may implement one of these two deprecated methods
   instead of ``find_spec()``.  The methods are still respected for the
   sake of backward compatibility.  However, if ``find_spec()`` is
   implemented on the path entry finder, the legacy methods are ignored.

   :meth:`!find_loader` takes one argument, the
   fully qualified name of the module being imported.  ``find_loader()``
   returns a 2-tuple where the first item is the loader and the second item
   is a namespace :term:`portion`.

   For backwards compatibility with other implementations of the import
   protocol, many path entry finders also support the same,
   traditional ``find_module()`` method that meta path finders support.
   However path entry finder ``find_module()`` methods are never called
   with a ``path`` argument (they are expected to record the appropriate
   path information from the initial call to the path hook).

   The ``find_module()`` method on path entry finders is deprecated,
   as it does not allow the path entry finder to contribute portions to
   namespace packages.  If both ``find_loader()`` and ``find_module()``
   exist on a path entry finder, the import system will always call
   ``find_loader()`` in preference to ``find_module()``.

.. versionchanged:: 3.10
    Calls to :meth:`!find_module` and
    :meth:`!find_loader` by the import
    system will raise :exc:`ImportWarning`.

.. versionchanged:: 3.12
    ``find_module()`` and ``find_loader()`` have been removed.


Replacing the standard import system
====================================

The most reliable mechanism for replacing the entire import system is to
delete the default contents of :data:`sys.meta_path`, replacing them
entirely with a custom meta path hook.

If it is acceptable to only alter the behaviour of import statements
without affecting other APIs that access the import system, then replacing
the builtin :func:`__import__` function may be sufficient. This technique
may also be employed at the module level to only alter the behaviour of
import statements within that module.

To selectively prevent the import of some modules from a hook early on the
meta path (rather than disabling the standard import system entirely),
it is sufficient to raise :exc:`ModuleNotFoundError` directly from
:meth:`~importlib.abc.MetaPathFinder.find_spec` instead of returning
``None``. The latter indicates that the meta path search should continue,
while raising an exception terminates it immediately.

.. _relativeimports:

Package Relative Imports
========================

Relative imports use leading dots. A single leading dot indicates a relative
import, starting with the current package. Two or more leading dots indicate a
relative import to the parent(s) of the current package, one level per dot
after the first. For example, given the following package layout::

    package/
        __init__.py
        subpackage1/
            __init__.py
            moduleX.py
            moduleY.py
        subpackage2/
            __init__.py
            moduleZ.py
        moduleA.py

In either ``subpackage1/moduleX.py`` or ``subpackage1/__init__.py``,
the following are valid relative imports::

    from .moduleY import spam
    from .moduleY import spam as ham
    from . import moduleY
    from ..subpackage1 import moduleY
    from ..subpackage2.moduleZ import eggs
    from ..moduleA import foo

Absolute imports may use either the ``import <>`` or ``from <> import <>``
syntax, but relative imports may only use the second form; the reason
for this is that::

    import XXX.YYY.ZZZ

should expose ``XXX.YYY.ZZZ`` as a usable expression, but .moduleY is
not a valid expression.


.. _import-dunder-main:

Special considerations for __main__
===================================

The :mod:`__main__` module is a special case relative to Python's import
system.  As noted :ref:`elsewhere <programs>`, the ``__main__`` module
is directly initialized at interpreter startup, much like :mod:`sys` and
:mod:`builtins`.  However, unlike those two, it doesn't strictly
qualify as a built-in module.  This is because the manner in which
``__main__`` is initialized depends on the flags and other options with
which the interpreter is invoked.

.. _main_spec:

__main__.__spec__
-----------------

Depending on how :mod:`__main__` is initialized, ``__main__.__spec__``
gets set appropriately or to ``None``.

When Python is started with the :option:`-m` option, ``__spec__`` is set
to the module spec of the corresponding module or package. ``__spec__`` is
also populated when the ``__main__`` module is loaded as part of executing a
directory, zipfile or other :data:`sys.path` entry.

In :ref:`the remaining cases <using-on-interface-options>`
``__main__.__spec__`` is set to ``None``, as the code used to populate the
:mod:`__main__` does not correspond directly with an importable module:

- interactive prompt
- :option:`-c` option
- running from stdin
- running directly from a source or bytecode file

Note that ``__main__.__spec__`` is always ``None`` in the last case,
*even if* the file could technically be imported directly as a module
instead. Use the :option:`-m` switch if valid module metadata is desired
in :mod:`__main__`.

Note also that even when ``__main__`` corresponds with an importable module
and ``__main__.__spec__`` is set accordingly, they're still considered
*distinct* modules. This is due to the fact that blocks guarded by
``if __name__ == "__main__":`` checks only execute when the module is used
to populate the ``__main__`` namespace, and not during normal import.


References
==========

The import machinery has evolved considerably since Python's early days.  The
original `specification for packages
<https://www.python.org/doc/essays/packages/>`_ is still available to read,
although some details have changed since the writing of that document.

The original specification for :data:`sys.meta_path` was :pep:`302`, with
subsequent extension in :pep:`420`.

:pep:`420` introduced :term:`namespace packages <namespace package>` for
Python 3.3.  :pep:`420` also introduced the :meth:`!find_loader` protocol as an
alternative to :meth:`!find_module`.

:pep:`366` describes the addition of the ``__package__`` attribute for
explicit relative imports in main modules.

:pep:`328` introduced absolute and explicit relative imports and initially
proposed ``__name__`` for semantics :pep:`366` would eventually specify for
``__package__``.

:pep:`338` defines executing modules as scripts.

:pep:`451` adds the encapsulation of per-module import state in spec
objects.  It also off-loads most of the boilerplate responsibilities of
loaders back onto the import machinery.  These changes allow the
deprecation of several APIs in the import system and also addition of new
methods to finders and loaders.

.. rubric:: Footnotes

.. [#fnmo] See :class:`types.ModuleType`.

.. [#fnlo] The importlib implementation avoids using the return value
   directly. Instead, it gets the module object by looking the module name up
   in :data:`sys.modules`.  The indirect effect of this is that an imported
   module may replace itself in :data:`sys.modules`.  This is
   implementation-specific behavior that is not guaranteed to work in other
   Python implementations.


================================================
File: /Doc/reference/index.rst
================================================
.. _reference-index:

#################################
  The Python Language Reference
#################################

This reference manual describes the syntax and "core semantics" of the
language. It is terse, but attempts to be exact and complete. The semantics of
non-essential built-in object types and of the built-in functions and modules
are described in :ref:`library-index`. For an informal introduction to the
language, see :ref:`tutorial-index`. For C or C++ programmers, two additional
manuals exist: :ref:`extending-index` describes the high-level picture of how to
write a Python extension module, and the :ref:`c-api-index` describes the
interfaces available to C/C++ programmers in detail.

.. toctree::
   :maxdepth: 2
   :numbered:

   introduction.rst
   lexical_analysis.rst
   datamodel.rst
   executionmodel.rst
   import.rst
   expressions.rst
   simple_stmts.rst
   compound_stmts.rst
   toplevel_components.rst
   grammar.rst


================================================
File: /Doc/reference/introduction.rst
================================================

.. _introduction:

************
Introduction
************

This reference manual describes the Python programming language. It is not
intended as a tutorial.

While I am trying to be as precise as possible, I chose to use English rather
than formal specifications for everything except syntax and lexical analysis.
This should make the document more understandable to the average reader, but
will leave room for ambiguities. Consequently, if you were coming from Mars and
tried to re-implement Python from this document alone, you might have to guess
things and in fact you would probably end up implementing quite a different
language. On the other hand, if you are using Python and wonder what the precise
rules about a particular area of the language are, you should definitely be able
to find them here. If you would like to see a more formal definition of the
language, maybe you could volunteer your time --- or invent a cloning machine
:-).

It is dangerous to add too many implementation details to a language reference
document --- the implementation may change, and other implementations of the
same language may work differently.  On the other hand, CPython is the one
Python implementation in widespread use (although alternate implementations
continue to gain support), and its particular quirks are sometimes worth being
mentioned, especially where the implementation imposes additional limitations.
Therefore, you'll find short "implementation notes" sprinkled throughout the
text.

Every Python implementation comes with a number of built-in and standard
modules.  These are documented in :ref:`library-index`.  A few built-in modules
are mentioned when they interact in a significant way with the language
definition.


.. _implementations:

Alternate Implementations
=========================

Though there is one Python implementation which is by far the most popular,
there are some alternate implementations which are of particular interest to
different audiences.

Known implementations include:

CPython
   This is the original and most-maintained implementation of Python, written in C.
   New language features generally appear here first.

Jython
   Python implemented in Java.  This implementation can be used as a scripting
   language for Java applications, or can be used to create applications using the
   Java class libraries.  It is also often used to create tests for Java libraries.
   More information can be found at `the Jython website <https://www.jython.org/>`_.

Python for .NET
   This implementation actually uses the CPython implementation, but is a managed
   .NET application and makes .NET libraries available.  It was created by Brian
   Lloyd.  For more information, see the `Python for .NET home page
   <https://pythonnet.github.io/>`_.

IronPython
   An alternate Python for .NET.  Unlike Python.NET, this is a complete Python
   implementation that generates IL, and compiles Python code directly to .NET
   assemblies.  It was created by Jim Hugunin, the original creator of Jython.  For
   more information, see `the IronPython website <https://ironpython.net/>`_.

PyPy
   An implementation of Python written completely in Python. It supports several
   advanced features not found in other implementations like stackless support
   and a Just in Time compiler. One of the goals of the project is to encourage
   experimentation with the language itself by making it easier to modify the
   interpreter (since it is written in Python).  Additional information is
   available on `the PyPy project's home page <https://pypy.org/>`_.

Each of these implementations varies in some way from the language as documented
in this manual, or introduces specific information beyond what's covered in the
standard Python documentation.  Please refer to the implementation-specific
documentation to determine what else you need to know about the specific
implementation you're using.


.. _notation:

Notation
========

.. index:: BNF, grammar, syntax, notation

The descriptions of lexical analysis and syntax use a modified
`Backus–Naur form (BNF) <https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form>`_ grammar
notation.  This uses the following style of definition:

.. productionlist:: notation
   name: `lc_letter` (`lc_letter` | "_")*
   lc_letter: "a"..."z"

The first line says that a ``name`` is an ``lc_letter`` followed by a sequence
of zero or more ``lc_letter``\ s and underscores.  An ``lc_letter`` in turn is
any of the single characters ``'a'`` through ``'z'``.  (This rule is actually
adhered to for the names defined in lexical and grammar rules in this document.)

Each rule begins with a name (which is the name defined by the rule) and
``::=``.  A vertical bar (``|``) is used to separate alternatives; it is the
least binding operator in this notation.  A star (``*``) means zero or more
repetitions of the preceding item; likewise, a plus (``+``) means one or more
repetitions, and a phrase enclosed in square brackets (``[ ]``) means zero or
one occurrences (in other words, the enclosed phrase is optional).  The ``*``
and ``+`` operators bind as tightly as possible; parentheses are used for
grouping.  Literal strings are enclosed in quotes.  White space is only
meaningful to separate tokens. Rules are normally contained on a single line;
rules with many alternatives may be formatted alternatively with each line after
the first beginning with a vertical bar.

.. index:: lexical definitions, ASCII

In lexical definitions (as the example above), two more conventions are used:
Two literal characters separated by three dots mean a choice of any single
character in the given (inclusive) range of ASCII characters.  A phrase between
angular brackets (``<...>``) gives an informal description of the symbol
defined; e.g., this could be used to describe the notion of 'control character'
if needed.

Even though the notation used is almost the same, there is a big difference
between the meaning of lexical and syntactic definitions: a lexical definition
operates on the individual characters of the input source, while a syntax
definition operates on the stream of tokens generated by the lexical analysis.
All uses of BNF in the next chapter ("Lexical Analysis") are lexical
definitions; uses in subsequent chapters are syntactic definitions.



================================================
File: /Doc/reference/lexical_analysis.rst
================================================

.. _lexical:

****************
Lexical analysis
****************

.. index:: lexical analysis, parser, token

A Python program is read by a *parser*.  Input to the parser is a stream of
*tokens*, generated by the *lexical analyzer*.  This chapter describes how the
lexical analyzer breaks a file into tokens.

Python reads program text as Unicode code points; the encoding of a source file
can be given by an encoding declaration and defaults to UTF-8, see :pep:`3120`
for details.  If the source file cannot be decoded, a :exc:`SyntaxError` is
raised.


.. _line-structure:

Line structure
==============

.. index:: line structure

A Python program is divided into a number of *logical lines*.


.. _logical-lines:

Logical lines
-------------

.. index:: logical line, physical line, line joining, NEWLINE token

The end of a logical line is represented by the token NEWLINE.  Statements
cannot cross logical line boundaries except where NEWLINE is allowed by the
syntax (e.g., between statements in compound statements). A logical line is
constructed from one or more *physical lines* by following the explicit or
implicit *line joining* rules.


.. _physical-lines:

Physical lines
--------------

A physical line is a sequence of characters terminated by an end-of-line
sequence.  In source files and strings, any of the standard platform line
termination sequences can be used - the Unix form using ASCII LF (linefeed),
the Windows form using the ASCII sequence CR LF (return followed by linefeed),
or the old Macintosh form using the ASCII CR (return) character.  All of these
forms can be used equally, regardless of platform. The end of input also serves
as an implicit terminator for the final physical line.

When embedding Python, source code strings should be passed to Python APIs using
the standard C conventions for newline characters (the ``\n`` character,
representing ASCII LF, is the line terminator).


.. _comments:

Comments
--------

.. index:: comment, hash character
   single: # (hash); comment

A comment starts with a hash character (``#``) that is not part of a string
literal, and ends at the end of the physical line.  A comment signifies the end
of the logical line unless the implicit line joining rules are invoked. Comments
are ignored by the syntax.


.. _encodings:

Encoding declarations
---------------------

.. index:: source character set, encoding declarations (source file)
   single: # (hash); source encoding declaration

If a comment in the first or second line of the Python script matches the
regular expression ``coding[=:]\s*([-\w.]+)``, this comment is processed as an
encoding declaration; the first group of this expression names the encoding of
the source code file. The encoding declaration must appear on a line of its
own. If it is the second line, the first line must also be a comment-only line.
The recommended forms of an encoding expression are ::

   # -*- coding: <encoding-name> -*-

which is recognized also by GNU Emacs, and ::

   # vim:fileencoding=<encoding-name>

which is recognized by Bram Moolenaar's VIM.

If no encoding declaration is found, the default encoding is UTF-8.  If the
implicit or explicit encoding of a file is UTF-8, an initial UTF-8 byte-order
mark (b'\xef\xbb\xbf') is ignored rather than being a syntax error.

If an encoding is declared, the encoding name must be recognized by Python
(see :ref:`standard-encodings`). The
encoding is used for all lexical analysis, including string literals, comments
and identifiers.


.. _explicit-joining:

Explicit line joining
---------------------

.. index:: physical line, line joining, line continuation, backslash character

Two or more physical lines may be joined into logical lines using backslash
characters (``\``), as follows: when a physical line ends in a backslash that is
not part of a string literal or comment, it is joined with the following forming
a single logical line, deleting the backslash and the following end-of-line
character.  For example::

   if 1900 < year < 2100 and 1 <= month <= 12 \
      and 1 <= day <= 31 and 0 <= hour < 24 \
      and 0 <= minute < 60 and 0 <= second < 60:   # Looks like a valid date
           return 1

A line ending in a backslash cannot carry a comment.  A backslash does not
continue a comment.  A backslash does not continue a token except for string
literals (i.e., tokens other than string literals cannot be split across
physical lines using a backslash).  A backslash is illegal elsewhere on a line
outside a string literal.


.. _implicit-joining:

Implicit line joining
---------------------

Expressions in parentheses, square brackets or curly braces can be split over
more than one physical line without using backslashes. For example::

   month_names = ['Januari', 'Februari', 'Maart',      # These are the
                  'April',   'Mei',      'Juni',       # Dutch names
                  'Juli',    'Augustus', 'September',  # for the months
                  'Oktober', 'November', 'December']   # of the year

Implicitly continued lines can carry comments.  The indentation of the
continuation lines is not important.  Blank continuation lines are allowed.
There is no NEWLINE token between implicit continuation lines.  Implicitly
continued lines can also occur within triple-quoted strings (see below); in that
case they cannot carry comments.


.. _blank-lines:

Blank lines
-----------

.. index:: single: blank line

A logical line that contains only spaces, tabs, formfeeds and possibly a
comment, is ignored (i.e., no NEWLINE token is generated).  During interactive
input of statements, handling of a blank line may differ depending on the
implementation of the read-eval-print loop.  In the standard interactive
interpreter, an entirely blank logical line (i.e. one containing not even
whitespace or a comment) terminates a multi-line statement.


.. _indentation:

Indentation
-----------

.. index:: indentation, leading whitespace, space, tab, grouping, statement grouping

Leading whitespace (spaces and tabs) at the beginning of a logical line is used
to compute the indentation level of the line, which in turn is used to determine
the grouping of statements.

Tabs are replaced (from left to right) by one to eight spaces such that the
total number of characters up to and including the replacement is a multiple of
eight (this is intended to be the same rule as used by Unix).  The total number
of spaces preceding the first non-blank character then determines the line's
indentation.  Indentation cannot be split over multiple physical lines using
backslashes; the whitespace up to the first backslash determines the
indentation.

Indentation is rejected as inconsistent if a source file mixes tabs and spaces
in a way that makes the meaning dependent on the worth of a tab in spaces; a
:exc:`TabError` is raised in that case.

**Cross-platform compatibility note:** because of the nature of text editors on
non-UNIX platforms, it is unwise to use a mixture of spaces and tabs for the
indentation in a single source file.  It should also be noted that different
platforms may explicitly limit the maximum indentation level.

A formfeed character may be present at the start of the line; it will be ignored
for the indentation calculations above.  Formfeed characters occurring elsewhere
in the leading whitespace have an undefined effect (for instance, they may reset
the space count to zero).

.. index:: INDENT token, DEDENT token

The indentation levels of consecutive lines are used to generate INDENT and
DEDENT tokens, using a stack, as follows.

Before the first line of the file is read, a single zero is pushed on the stack;
this will never be popped off again.  The numbers pushed on the stack will
always be strictly increasing from bottom to top.  At the beginning of each
logical line, the line's indentation level is compared to the top of the stack.
If it is equal, nothing happens. If it is larger, it is pushed on the stack, and
one INDENT token is generated.  If it is smaller, it *must* be one of the
numbers occurring on the stack; all numbers on the stack that are larger are
popped off, and for each number popped off a DEDENT token is generated.  At the
end of the file, a DEDENT token is generated for each number remaining on the
stack that is larger than zero.

Here is an example of a correctly (though confusingly) indented piece of Python
code::

   def perm(l):
           # Compute the list of all permutations of l
       if len(l) <= 1:
                     return [l]
       r = []
       for i in range(len(l)):
                s = l[:i] + l[i+1:]
                p = perm(s)
                for x in p:
                 r.append(l[i:i+1] + x)
       return r

The following example shows various indentation errors::

    def perm(l):                       # error: first line indented
   for i in range(len(l)):             # error: not indented
       s = l[:i] + l[i+1:]
           p = perm(l[:i] + l[i+1:])   # error: unexpected indent
           for x in p:
                   r.append(l[i:i+1] + x)
               return r                # error: inconsistent dedent

(Actually, the first three errors are detected by the parser; only the last
error is found by the lexical analyzer --- the indentation of ``return r`` does
not match a level popped off the stack.)


.. _whitespace:

Whitespace between tokens
-------------------------

Except at the beginning of a logical line or in string literals, the whitespace
characters space, tab and formfeed can be used interchangeably to separate
tokens.  Whitespace is needed between two tokens only if their concatenation
could otherwise be interpreted as a different token (e.g., ab is one token, but
a b is two tokens).


.. _other-tokens:

Other tokens
============

Besides NEWLINE, INDENT and DEDENT, the following categories of tokens exist:
*identifiers*, *keywords*, *literals*, *operators*, and *delimiters*. Whitespace
characters (other than line terminators, discussed earlier) are not tokens, but
serve to delimit tokens. Where ambiguity exists, a token comprises the longest
possible string that forms a legal token, when read from left to right.


.. _identifiers:

Identifiers and keywords
========================

.. index:: identifier, name

Identifiers (also referred to as *names*) are described by the following lexical
definitions.

The syntax of identifiers in Python is based on the Unicode standard annex
UAX-31, with elaboration and changes as defined below; see also :pep:`3131` for
further details.

Within the ASCII range (U+0001..U+007F), the valid characters for identifiers
include the uppercase and lowercase letters ``A`` through
``Z``, the underscore ``_`` and, except for the first character, the digits
``0`` through ``9``.
Python 3.0 introduced additional characters from outside the ASCII range (see
:pep:`3131`).  For these characters, the classification uses the version of the
Unicode Character Database as included in the :mod:`unicodedata` module.

Identifiers are unlimited in length.  Case is significant.

.. productionlist:: python-grammar
   identifier: `xid_start` `xid_continue`*
   id_start: <all characters in general categories Lu, Ll, Lt, Lm, Lo, Nl, the underscore, and characters with the Other_ID_Start property>
   id_continue: <all characters in `id_start`, plus characters in the categories Mn, Mc, Nd, Pc and others with the Other_ID_Continue property>
   xid_start: <all characters in `id_start` whose NFKC normalization is in "id_start xid_continue*">
   xid_continue: <all characters in `id_continue` whose NFKC normalization is in "id_continue*">

The Unicode category codes mentioned above stand for:

* *Lu* - uppercase letters
* *Ll* - lowercase letters
* *Lt* - titlecase letters
* *Lm* - modifier letters
* *Lo* - other letters
* *Nl* - letter numbers
* *Mn* - nonspacing marks
* *Mc* - spacing combining marks
* *Nd* - decimal numbers
* *Pc* - connector punctuations
* *Other_ID_Start* - explicit list of characters in `PropList.txt
  <https://www.unicode.org/Public/16.0.0/ucd/PropList.txt>`_ to support backwards
  compatibility
* *Other_ID_Continue* - likewise

All identifiers are converted into the normal form NFKC while parsing; comparison
of identifiers is based on NFKC.

A non-normative HTML file listing all valid identifier characters for Unicode
16.0.0 can be found at
https://www.unicode.org/Public/16.0.0/ucd/DerivedCoreProperties.txt


.. _keywords:

Keywords
--------

.. index::
   single: keyword
   single: reserved word

The following identifiers are used as reserved words, or *keywords* of the
language, and cannot be used as ordinary identifiers.  They must be spelled
exactly as written here:

.. sourcecode:: text

   False      await      else       import     pass
   None       break      except     in         raise
   True       class      finally    is         return
   and        continue   for        lambda     try
   as         def        from       nonlocal   while
   assert     del        global     not        with
   async      elif       if         or         yield


.. _soft-keywords:

Soft Keywords
-------------

.. index:: soft keyword, keyword

.. versionadded:: 3.10

Some identifiers are only reserved under specific contexts. These are known as
*soft keywords*.  The identifiers ``match``, ``case``, ``type`` and ``_`` can
syntactically act as keywords in certain contexts,
but this distinction is done at the parser level, not when tokenizing.

As soft keywords, their use in the grammar is possible while still
preserving compatibility with existing code that uses these names as
identifier names.

``match``, ``case``, and ``_`` are used in the :keyword:`match` statement.
``type`` is used in the :keyword:`type` statement.

.. versionchanged:: 3.12
   ``type`` is now a soft keyword.

.. index::
   single: _, identifiers
   single: __, identifiers
.. _id-classes:

Reserved classes of identifiers
-------------------------------

Certain classes of identifiers (besides keywords) have special meanings.  These
classes are identified by the patterns of leading and trailing underscore
characters:

``_*``
   Not imported by ``from module import *``.

``_``
   In a ``case`` pattern within a :keyword:`match` statement, ``_`` is a
   :ref:`soft keyword <soft-keywords>` that denotes a
   :ref:`wildcard <wildcard-patterns>`.

   Separately, the interactive interpreter makes the result of the last evaluation
   available in the variable ``_``.
   (It is stored in the :mod:`builtins` module, alongside built-in
   functions like ``print``.)

   Elsewhere, ``_`` is a regular identifier. It is often used to name
   "special" items, but it is not special to Python itself.

   .. note::

      The name ``_`` is often used in conjunction with internationalization;
      refer to the documentation for the :mod:`gettext` module for more
      information on this convention.

      It is also commonly used for unused variables.

``__*__``
   System-defined names, informally known as "dunder" names. These names are
   defined by the interpreter and its implementation (including the standard library).
   Current system names are discussed in the :ref:`specialnames` section and elsewhere.
   More will likely be defined in future versions of Python.  *Any* use of ``__*__`` names,
   in any context, that does not follow explicitly documented use, is subject to
   breakage without warning.

``__*``
   Class-private names.  Names in this category, when used within the context of a
   class definition, are re-written to use a mangled form to help avoid name
   clashes between "private" attributes of base and derived classes. See section
   :ref:`atom-identifiers`.


.. _literals:

Literals
========

.. index:: literal, constant

Literals are notations for constant values of some built-in types.


.. index:: string literal, bytes literal, ASCII
   single: ' (single quote); string literal
   single: " (double quote); string literal
   single: u'; string literal
   single: u"; string literal
.. _strings:

String and Bytes literals
-------------------------

String literals are described by the following lexical definitions:

.. productionlist:: python-grammar
   stringliteral: [`stringprefix`](`shortstring` | `longstring`)
   stringprefix: "r" | "u" | "R" | "U" | "f" | "F"
               : | "fr" | "Fr" | "fR" | "FR" | "rf" | "rF" | "Rf" | "RF"
   shortstring: "'" `shortstringitem`* "'" | '"' `shortstringitem`* '"'
   longstring: "'''" `longstringitem`* "'''" | '"""' `longstringitem`* '"""'
   shortstringitem: `shortstringchar` | `stringescapeseq`
   longstringitem: `longstringchar` | `stringescapeseq`
   shortstringchar: <any source character except "\" or newline or the quote>
   longstringchar: <any source character except "\">
   stringescapeseq: "\" <any source character>

.. productionlist:: python-grammar
   bytesliteral: `bytesprefix`(`shortbytes` | `longbytes`)
   bytesprefix: "b" | "B" | "br" | "Br" | "bR" | "BR" | "rb" | "rB" | "Rb" | "RB"
   shortbytes: "'" `shortbytesitem`* "'" | '"' `shortbytesitem`* '"'
   longbytes: "'''" `longbytesitem`* "'''" | '"""' `longbytesitem`* '"""'
   shortbytesitem: `shortbyteschar` | `bytesescapeseq`
   longbytesitem: `longbyteschar` | `bytesescapeseq`
   shortbyteschar: <any ASCII character except "\" or newline or the quote>
   longbyteschar: <any ASCII character except "\">
   bytesescapeseq: "\" <any ASCII character>

One syntactic restriction not indicated by these productions is that whitespace
is not allowed between the :token:`~python-grammar:stringprefix` or
:token:`~python-grammar:bytesprefix` and the rest of the literal. The source
character set is defined by the encoding declaration; it is UTF-8 if no encoding
declaration is given in the source file; see section :ref:`encodings`.

.. index:: triple-quoted string, Unicode Consortium, raw string
   single: """; string literal
   single: '''; string literal

In plain English: Both types of literals can be enclosed in matching single quotes
(``'``) or double quotes (``"``).  They can also be enclosed in matching groups
of three single or double quotes (these are generally referred to as
*triple-quoted strings*). The backslash (``\``) character is used to give special
meaning to otherwise ordinary characters like ``n``, which means 'newline' when
escaped (``\n``). It can also be used to escape characters that otherwise have a
special meaning, such as newline, backslash itself, or the quote character.
See :ref:`escape sequences <escape-sequences>` below for examples.

.. index::
   single: b'; bytes literal
   single: b"; bytes literal

Bytes literals are always prefixed with ``'b'`` or ``'B'``; they produce an
instance of the :class:`bytes` type instead of the :class:`str` type.  They
may only contain ASCII characters; bytes with a numeric value of 128 or greater
must be expressed with escapes.

.. index::
   single: r'; raw string literal
   single: r"; raw string literal

Both string and bytes literals may optionally be prefixed with a letter ``'r'``
or ``'R'``; such constructs are called :dfn:`raw string literals`
and :dfn:`raw bytes literals` respectively and treat backslashes as
literal characters.  As a result, in raw string literals, ``'\U'`` and ``'\u'``
escapes are not treated specially.

.. versionadded:: 3.3
   The ``'rb'`` prefix of raw bytes literals has been added as a synonym
   of ``'br'``.

   Support for the unicode legacy literal (``u'value'``) was reintroduced
   to simplify the maintenance of dual Python 2.x and 3.x codebases.
   See :pep:`414` for more information.

.. index::
   single: f'; formatted string literal
   single: f"; formatted string literal

A string literal with ``'f'`` or ``'F'`` in its prefix is a
:dfn:`formatted string literal`; see :ref:`f-strings`.  The ``'f'`` may be
combined with ``'r'``, but not with ``'b'`` or ``'u'``, therefore raw
formatted strings are possible, but formatted bytes literals are not.

In triple-quoted literals, unescaped newlines and quotes are allowed (and are
retained), except that three unescaped quotes in a row terminate the literal.  (A
"quote" is the character used to open the literal, i.e. either ``'`` or ``"``.)

.. index:: physical line, escape sequence, Standard C, C
   single: \ (backslash); escape sequence
   single: \\; escape sequence
   single: \a; escape sequence
   single: \b; escape sequence
   single: \f; escape sequence
   single: \n; escape sequence
   single: \r; escape sequence
   single: \t; escape sequence
   single: \v; escape sequence
   single: \x; escape sequence
   single: \N; escape sequence
   single: \u; escape sequence
   single: \U; escape sequence

.. _escape-sequences:


Escape sequences
^^^^^^^^^^^^^^^^

Unless an ``'r'`` or ``'R'`` prefix is present, escape sequences in string and
bytes literals are interpreted according to rules similar to those used by
Standard C.  The recognized escape sequences are:

+-------------------------+---------------------------------+-------+
| Escape Sequence         | Meaning                         | Notes |
+=========================+=================================+=======+
| ``\``\ <newline>        | Backslash and newline ignored   | \(1)  |
+-------------------------+---------------------------------+-------+
| ``\\``                  | Backslash (``\``)               |       |
+-------------------------+---------------------------------+-------+
| ``\'``                  | Single quote (``'``)            |       |
+-------------------------+---------------------------------+-------+
| ``\"``                  | Double quote (``"``)            |       |
+-------------------------+---------------------------------+-------+
| ``\a``                  | ASCII Bell (BEL)                |       |
+-------------------------+---------------------------------+-------+
| ``\b``                  | ASCII Backspace (BS)            |       |
+-------------------------+---------------------------------+-------+
| ``\f``                  | ASCII Formfeed (FF)             |       |
+-------------------------+---------------------------------+-------+
| ``\n``                  | ASCII Linefeed (LF)             |       |
+-------------------------+---------------------------------+-------+
| ``\r``                  | ASCII Carriage Return (CR)      |       |
+-------------------------+---------------------------------+-------+
| ``\t``                  | ASCII Horizontal Tab (TAB)      |       |
+-------------------------+---------------------------------+-------+
| ``\v``                  | ASCII Vertical Tab (VT)         |       |
+-------------------------+---------------------------------+-------+
| :samp:`\\\\{ooo}`       | Character with octal value      | (2,4) |
|                         | *ooo*                           |       |
+-------------------------+---------------------------------+-------+
| :samp:`\\x{hh}`         | Character with hex value *hh*   | (3,4) |
+-------------------------+---------------------------------+-------+

Escape sequences only recognized in string literals are:

+-------------------------+---------------------------------+-------+
| Escape Sequence         | Meaning                         | Notes |
+=========================+=================================+=======+
| :samp:`\\N\\{{name}\\}` | Character named *name* in the   | \(5)  |
|                         | Unicode database                |       |
+-------------------------+---------------------------------+-------+
| :samp:`\\u{xxxx}`       | Character with 16-bit hex value | \(6)  |
|                         | *xxxx*                          |       |
+-------------------------+---------------------------------+-------+
| :samp:`\\U{xxxxxxxx}`   | Character with 32-bit hex value | \(7)  |
|                         | *xxxxxxxx*                      |       |
+-------------------------+---------------------------------+-------+

Notes:

(1)
   A backslash can be added at the end of a line to ignore the newline::

      >>> 'This string will not include \
      ... backslashes or newline characters.'
      'This string will not include backslashes or newline characters.'

   The same result can be achieved using :ref:`triple-quoted strings <strings>`,
   or parentheses and :ref:`string literal concatenation <string-concatenation>`.


(2)
   As in Standard C, up to three octal digits are accepted.

   .. versionchanged:: 3.11
      Octal escapes with value larger than ``0o377`` produce a
      :exc:`DeprecationWarning`.

   .. versionchanged:: 3.12
      Octal escapes with value larger than ``0o377`` produce a
      :exc:`SyntaxWarning`. In a future Python version they will be eventually
      a :exc:`SyntaxError`.

(3)
   Unlike in Standard C, exactly two hex digits are required.

(4)
   In a bytes literal, hexadecimal and octal escapes denote the byte with the
   given value. In a string literal, these escapes denote a Unicode character
   with the given value.

(5)
   .. versionchanged:: 3.3
      Support for name aliases [#]_ has been added.

(6)
   Exactly four hex digits are required.

(7)
   Any Unicode character can be encoded this way.  Exactly eight hex digits
   are required.


.. index:: unrecognized escape sequence

Unlike Standard C, all unrecognized escape sequences are left in the string
unchanged, i.e., *the backslash is left in the result*.  (This behavior is
useful when debugging: if an escape sequence is mistyped, the resulting output
is more easily recognized as broken.)  It is also important to note that the
escape sequences only recognized in string literals fall into the category of
unrecognized escapes for bytes literals.

.. versionchanged:: 3.6
   Unrecognized escape sequences produce a :exc:`DeprecationWarning`.

.. versionchanged:: 3.12
   Unrecognized escape sequences produce a :exc:`SyntaxWarning`. In a future
   Python version they will be eventually a :exc:`SyntaxError`.

Even in a raw literal, quotes can be escaped with a backslash, but the
backslash remains in the result; for example, ``r"\""`` is a valid string
literal consisting of two characters: a backslash and a double quote; ``r"\"``
is not a valid string literal (even a raw string cannot end in an odd number of
backslashes).  Specifically, *a raw literal cannot end in a single backslash*
(since the backslash would escape the following quote character).  Note also
that a single backslash followed by a newline is interpreted as those two
characters as part of the literal, *not* as a line continuation.


.. _string-concatenation:

String literal concatenation
----------------------------

Multiple adjacent string or bytes literals (delimited by whitespace), possibly
using different quoting conventions, are allowed, and their meaning is the same
as their concatenation.  Thus, ``"hello" 'world'`` is equivalent to
``"helloworld"``.  This feature can be used to reduce the number of backslashes
needed, to split long strings conveniently across long lines, or even to add
comments to parts of strings, for example::

   re.compile("[A-Za-z_]"       # letter or underscore
              "[A-Za-z0-9_]*"   # letter, digit or underscore
             )

Note that this feature is defined at the syntactical level, but implemented at
compile time.  The '+' operator must be used to concatenate string expressions
at run time.  Also note that literal concatenation can use different quoting
styles for each component (even mixing raw strings and triple quoted strings),
and formatted string literals may be concatenated with plain string literals.


.. index::
   single: formatted string literal
   single: interpolated string literal
   single: string; formatted literal
   single: string; interpolated literal
   single: f-string
   single: fstring
   single: {} (curly brackets); in formatted string literal
   single: ! (exclamation); in formatted string literal
   single: : (colon); in formatted string literal
   single: = (equals); for help in debugging using string literals

.. _f-strings:
.. _formatted-string-literals:

f-strings
---------

.. versionadded:: 3.6

A :dfn:`formatted string literal` or :dfn:`f-string` is a string literal
that is prefixed with ``'f'`` or ``'F'``.  These strings may contain
replacement fields, which are expressions delimited by curly braces ``{}``.
While other string literals always have a constant value, formatted strings
are really expressions evaluated at run time.

Escape sequences are decoded like in ordinary string literals (except when
a literal is also marked as a raw string).  After decoding, the grammar
for the contents of the string is:

.. productionlist:: python-grammar
   f_string: (`literal_char` | "{{" | "}}" | `replacement_field`)*
   replacement_field: "{" `f_expression` ["="] ["!" `conversion`] [":" `format_spec`] "}"
   f_expression: (`conditional_expression` | "*" `or_expr`)
               :   ("," `conditional_expression` | "," "*" `or_expr`)* [","]
               : | `yield_expression`
   conversion: "s" | "r" | "a"
   format_spec: (`literal_char` | `replacement_field`)*
   literal_char: <any code point except "{", "}" or NULL>

The parts of the string outside curly braces are treated literally,
except that any doubled curly braces ``'{{'`` or ``'}}'`` are replaced
with the corresponding single curly brace.  A single opening curly
bracket ``'{'`` marks a replacement field, which starts with a
Python expression. To display both the expression text and its value after
evaluation, (useful in debugging), an equal sign ``'='`` may be added after the
expression. A conversion field, introduced by an exclamation point ``'!'`` may
follow.  A format specifier may also be appended, introduced by a colon ``':'``.
A replacement field ends with a closing curly bracket ``'}'``.

Expressions in formatted string literals are treated like regular
Python expressions surrounded by parentheses, with a few exceptions.
An empty expression is not allowed, and both :keyword:`lambda`  and
assignment expressions ``:=`` must be surrounded by explicit parentheses.
Each expression is evaluated in the context where the formatted string literal
appears, in order from left to right.  Replacement expressions can contain
newlines in both single-quoted and triple-quoted f-strings and they can contain
comments.  Everything that comes after a ``#`` inside a replacement field
is a comment (even closing braces and quotes). In that case, replacement fields
must be closed in a different line.

.. code-block:: text

   >>> f"abc{a # This is a comment }"
   ... + 3}"
   'abc5'

.. versionchanged:: 3.7
   Prior to Python 3.7, an :keyword:`await` expression and comprehensions
   containing an :keyword:`async for` clause were illegal in the expressions
   in formatted string literals due to a problem with the implementation.

.. versionchanged:: 3.12
   Prior to Python 3.12, comments were not allowed inside f-string replacement
   fields.

When the equal sign ``'='`` is provided, the output will have the expression
text, the ``'='`` and the evaluated value. Spaces after the opening brace
``'{'``, within the expression and after the ``'='`` are all retained in the
output. By default, the ``'='`` causes the :func:`repr` of the expression to be
provided, unless there is a format specified. When a format is specified it
defaults to the :func:`str` of the expression unless a conversion ``'!r'`` is
declared.

.. versionadded:: 3.8
   The equal sign ``'='``.

If a conversion is specified, the result of evaluating the expression
is converted before formatting.  Conversion ``'!s'`` calls :func:`str` on
the result, ``'!r'`` calls :func:`repr`, and ``'!a'`` calls :func:`ascii`.

The result is then formatted using the :func:`format` protocol.  The
format specifier is passed to the :meth:`~object.__format__` method of the
expression or conversion result.  An empty string is passed when the
format specifier is omitted.  The formatted result is then included in
the final value of the whole string.

Top-level format specifiers may include nested replacement fields. These nested
fields may include their own conversion fields and :ref:`format specifiers
<formatspec>`, but may not include more deeply nested replacement fields. The
:ref:`format specifier mini-language <formatspec>` is the same as that used by
the :meth:`str.format` method.

Formatted string literals may be concatenated, but replacement fields
cannot be split across literals.

Some examples of formatted string literals::

   >>> name = "Fred"
   >>> f"He said his name is {name!r}."
   "He said his name is 'Fred'."
   >>> f"He said his name is {repr(name)}."  # repr() is equivalent to !r
   "He said his name is 'Fred'."
   >>> width = 10
   >>> precision = 4
   >>> value = decimal.Decimal("12.34567")
   >>> f"result: {value:{width}.{precision}}"  # nested fields
   'result:      12.35'
   >>> today = datetime(year=2017, month=1, day=27)
   >>> f"{today:%B %d, %Y}"  # using date format specifier
   'January 27, 2017'
   >>> f"{today=:%B %d, %Y}" # using date format specifier and debugging
   'today=January 27, 2017'
   >>> number = 1024
   >>> f"{number:#0x}"  # using integer format specifier
   '0x400'
   >>> foo = "bar"
   >>> f"{ foo = }" # preserves whitespace
   " foo = 'bar'"
   >>> line = "The mill's closed"
   >>> f"{line = }"
   'line = "The mill\'s closed"'
   >>> f"{line = :20}"
   "line = The mill's closed   "
   >>> f"{line = !r:20}"
   'line = "The mill\'s closed" '


Reusing the outer f-string quoting type inside a replacement field is
permitted::

   >>> a = dict(x=2)
   >>> f"abc {a["x"]} def"
   'abc 2 def'

.. versionchanged:: 3.12
   Prior to Python 3.12, reuse of the same quoting type of the outer f-string
   inside a replacement field was not possible.

Backslashes are also allowed in replacement fields and are evaluated the same
way as in any other context::

   >>> a = ["a", "b", "c"]
   >>> print(f"List a contains:\n{"\n".join(a)}")
   List a contains:
   a
   b
   c

.. versionchanged:: 3.12
   Prior to Python 3.12, backslashes were not permitted inside an f-string
   replacement field.

Formatted string literals cannot be used as docstrings, even if they do not
include expressions.

::

   >>> def foo():
   ...     f"Not a docstring"
   ...
   >>> foo.__doc__ is None
   True

See also :pep:`498` for the proposal that added formatted string literals,
and :meth:`str.format`, which uses a related format string mechanism.


.. _numbers:

Numeric literals
----------------

.. index:: number, numeric literal, integer literal
   floating-point literal, hexadecimal literal
   octal literal, binary literal, decimal literal, imaginary literal, complex literal

There are three types of numeric literals: integers, floating-point numbers, and
imaginary numbers.  There are no complex literals (complex numbers can be formed
by adding a real number and an imaginary number).

Note that numeric literals do not include a sign; a phrase like ``-1`` is
actually an expression composed of the unary operator '``-``' and the literal
``1``.


.. index::
   single: 0b; integer literal
   single: 0o; integer literal
   single: 0x; integer literal
   single: _ (underscore); in numeric literal

.. _integers:

Integer literals
----------------

Integer literals are described by the following lexical definitions:

.. productionlist:: python-grammar
   integer: `decinteger` | `bininteger` | `octinteger` | `hexinteger`
   decinteger: `nonzerodigit` (["_"] `digit`)* | "0"+ (["_"] "0")*
   bininteger: "0" ("b" | "B") (["_"] `bindigit`)+
   octinteger: "0" ("o" | "O") (["_"] `octdigit`)+
   hexinteger: "0" ("x" | "X") (["_"] `hexdigit`)+
   nonzerodigit: "1"..."9"
   digit: "0"..."9"
   bindigit: "0" | "1"
   octdigit: "0"..."7"
   hexdigit: `digit` | "a"..."f" | "A"..."F"

There is no limit for the length of integer literals apart from what can be
stored in available memory.

Underscores are ignored for determining the numeric value of the literal.  They
can be used to group digits for enhanced readability.  One underscore can occur
between digits, and after base specifiers like ``0x``.

Note that leading zeros in a non-zero decimal number are not allowed. This is
for disambiguation with C-style octal literals, which Python used before version
3.0.

Some examples of integer literals::

   7     2147483647                        0o177    0b100110111
   3     79228162514264337593543950336     0o377    0xdeadbeef
         100_000_000_000                   0b_1110_0101

.. versionchanged:: 3.6
   Underscores are now allowed for grouping purposes in literals.


.. index::
   single: . (dot); in numeric literal
   single: e; in numeric literal
   single: _ (underscore); in numeric literal
.. _floating:

Floating-point literals
-----------------------

Floating-point literals are described by the following lexical definitions:

.. productionlist:: python-grammar
   floatnumber: `pointfloat` | `exponentfloat`
   pointfloat: [`digitpart`] `fraction` | `digitpart` "."
   exponentfloat: (`digitpart` | `pointfloat`) `exponent`
   digitpart: `digit` (["_"] `digit`)*
   fraction: "." `digitpart`
   exponent: ("e" | "E") ["+" | "-"] `digitpart`

Note that the integer and exponent parts are always interpreted using radix 10.
For example, ``077e010`` is legal, and denotes the same number as ``77e10``. The
allowed range of floating-point literals is implementation-dependent.  As in
integer literals, underscores are supported for digit grouping.

Some examples of floating-point literals::

   3.14    10.    .001    1e100    3.14e-10    0e0    3.14_15_93

.. versionchanged:: 3.6
   Underscores are now allowed for grouping purposes in literals.


.. index::
   single: j; in numeric literal
.. _imaginary:

Imaginary literals
------------------

Imaginary literals are described by the following lexical definitions:

.. productionlist:: python-grammar
   imagnumber: (`floatnumber` | `digitpart`) ("j" | "J")

An imaginary literal yields a complex number with a real part of 0.0.  Complex
numbers are represented as a pair of floating-point numbers and have the same
restrictions on their range.  To create a complex number with a nonzero real
part, add a floating-point number to it, e.g., ``(3+4j)``.  Some examples of
imaginary literals::

   3.14j   10.j    10j     .001j   1e100j   3.14e-10j   3.14_15_93j


.. _operators:

Operators
=========

.. index:: single: operators

The following tokens are operators:

.. code-block:: none


   +       -       *       **      /       //      %      @
   <<      >>      &       |       ^       ~       :=
   <       >       <=      >=      ==      !=


.. _delimiters:

Delimiters
==========

.. index:: single: delimiters

The following tokens serve as delimiters in the grammar:

.. code-block:: none

   (       )       [       ]       {       }
   ,       :       !       .       ;       @       =
   ->      +=      -=      *=      /=      //=     %=
   @=      &=      |=      ^=      >>=     <<=     **=

The period can also occur in floating-point and imaginary literals.  A sequence
of three periods has a special meaning as an ellipsis literal. The second half
of the list, the augmented assignment operators, serve lexically as delimiters,
but also perform an operation.

The following printing ASCII characters have special meaning as part of other
tokens or are otherwise significant to the lexical analyzer:

.. code-block:: none

   '       "       #       \

The following printing ASCII characters are not used in Python.  Their
occurrence outside string literals and comments is an unconditional error:

.. code-block:: none

   $       ?       `


.. rubric:: Footnotes

.. [#] https://www.unicode.org/Public/16.0.0/ucd/NameAliases.txt


================================================
File: /Doc/reference/simple_stmts.rst
================================================

.. _simple:

*****************
Simple statements
*****************

.. index:: pair: simple; statement

A simple statement is comprised within a single logical line. Several simple
statements may occur on a single line separated by semicolons.  The syntax for
simple statements is:

.. productionlist:: python-grammar
   simple_stmt: `expression_stmt`
              : | `assert_stmt`
              : | `assignment_stmt`
              : | `augmented_assignment_stmt`
              : | `annotated_assignment_stmt`
              : | `pass_stmt`
              : | `del_stmt`
              : | `return_stmt`
              : | `yield_stmt`
              : | `raise_stmt`
              : | `break_stmt`
              : | `continue_stmt`
              : | `import_stmt`
              : | `future_stmt`
              : | `global_stmt`
              : | `nonlocal_stmt`
              : | `type_stmt`


.. _exprstmts:

Expression statements
=====================

.. index::
   pair: expression; statement
   pair: expression; list
.. index:: pair: expression; list

Expression statements are used (mostly interactively) to compute and write a
value, or (usually) to call a procedure (a function that returns no meaningful
result; in Python, procedures return the value ``None``).  Other uses of
expression statements are allowed and occasionally useful.  The syntax for an
expression statement is:

.. productionlist:: python-grammar
   expression_stmt: `starred_expression`

An expression statement evaluates the expression list (which may be a single
expression).

.. index::
   pair: built-in function; repr
   pair: object; None
   pair: string; conversion
   single: output
   pair: standard; output
   pair: writing; values
   pair: procedure; call

In interactive mode, if the value is not ``None``, it is converted to a string
using the built-in :func:`repr` function and the resulting string is written to
standard output on a line by itself (except if the result is ``None``, so that
procedure calls do not cause any output.)

.. _assignment:

Assignment statements
=====================

.. index::
   single: = (equals); assignment statement
   pair: assignment; statement
   pair: binding; name
   pair: rebinding; name
   pair: object; mutable
   pair: attribute; assignment

Assignment statements are used to (re)bind names to values and to modify
attributes or items of mutable objects:

.. productionlist:: python-grammar
   assignment_stmt: (`target_list` "=")+ (`starred_expression` | `yield_expression`)
   target_list: `target` ("," `target`)* [","]
   target: `identifier`
         : | "(" [`target_list`] ")"
         : | "[" [`target_list`] "]"
         : | `attributeref`
         : | `subscription`
         : | `slicing`
         : | "*" `target`

(See section :ref:`primaries` for the syntax definitions for *attributeref*,
*subscription*, and *slicing*.)

An assignment statement evaluates the expression list (remember that this can be
a single expression or a comma-separated list, the latter yielding a tuple) and
assigns the single resulting object to each of the target lists, from left to
right.

.. index::
   single: target
   pair: target; list

Assignment is defined recursively depending on the form of the target (list).
When a target is part of a mutable object (an attribute reference, subscription
or slicing), the mutable object must ultimately perform the assignment and
decide about its validity, and may raise an exception if the assignment is
unacceptable.  The rules observed by various types and the exceptions raised are
given with the definition of the object types (see section :ref:`types`).

.. index:: triple: target; list; assignment
   single: , (comma); in target list
   single: * (asterisk); in assignment target list
   single: [] (square brackets); in assignment target list
   single: () (parentheses); in assignment target list

Assignment of an object to a target list, optionally enclosed in parentheses or
square brackets, is recursively defined as follows.

* If the target list is a single target with no trailing comma,
  optionally in parentheses, the object is assigned to that target.

* Else:

  * If the target list contains one target prefixed with an asterisk, called a
    "starred" target: The object must be an iterable with at least as many items
    as there are targets in the target list, minus one.  The first items of the
    iterable are assigned, from left to right, to the targets before the starred
    target.  The final items of the iterable are assigned to the targets after
    the starred target.  A list of the remaining items in the iterable is then
    assigned to the starred target (the list can be empty).

  * Else: The object must be an iterable with the same number of items as there
    are targets in the target list, and the items are assigned, from left to
    right, to the corresponding targets.

Assignment of an object to a single target is recursively defined as follows.

* If the target is an identifier (name):

  * If the name does not occur in a :keyword:`global` or :keyword:`nonlocal`
    statement in the current code block: the name is bound to the object in the
    current local namespace.

  * Otherwise: the name is bound to the object in the global namespace or the
    outer namespace determined by :keyword:`nonlocal`, respectively.

  .. index:: single: destructor

  The name is rebound if it was already bound.  This may cause the reference
  count for the object previously bound to the name to reach zero, causing the
  object to be deallocated and its destructor (if it has one) to be called.

  .. index:: pair: attribute; assignment

* If the target is an attribute reference: The primary expression in the
  reference is evaluated.  It should yield an object with assignable attributes;
  if this is not the case, :exc:`TypeError` is raised.  That object is then
  asked to assign the assigned object to the given attribute; if it cannot
  perform the assignment, it raises an exception (usually but not necessarily
  :exc:`AttributeError`).

  .. _attr-target-note:

  Note: If the object is a class instance and the attribute reference occurs on
  both sides of the assignment operator, the right-hand side expression, ``a.x`` can access
  either an instance attribute or (if no instance attribute exists) a class
  attribute.  The left-hand side target ``a.x`` is always set as an instance attribute,
  creating it if necessary.  Thus, the two occurrences of ``a.x`` do not
  necessarily refer to the same attribute: if the right-hand side expression refers to a
  class attribute, the left-hand side creates a new instance attribute as the target of the
  assignment::

     class Cls:
         x = 3             # class variable
     inst = Cls()
     inst.x = inst.x + 1   # writes inst.x as 4 leaving Cls.x as 3

  This description does not necessarily apply to descriptor attributes, such as
  properties created with :func:`property`.

  .. index::
     pair: subscription; assignment
     pair: object; mutable

* If the target is a subscription: The primary expression in the reference is
  evaluated.  It should yield either a mutable sequence object (such as a list)
  or a mapping object (such as a dictionary).  Next, the subscript expression is
  evaluated.

  .. index::
     pair: object; sequence
     pair: object; list

  If the primary is a mutable sequence object (such as a list), the subscript
  must yield an integer.  If it is negative, the sequence's length is added to
  it.  The resulting value must be a nonnegative integer less than the
  sequence's length, and the sequence is asked to assign the assigned object to
  its item with that index.  If the index is out of range, :exc:`IndexError` is
  raised (assignment to a subscripted sequence cannot add new items to a list).

  .. index::
     pair: object; mapping
     pair: object; dictionary

  If the primary is a mapping object (such as a dictionary), the subscript must
  have a type compatible with the mapping's key type, and the mapping is then
  asked to create a key/value pair which maps the subscript to the assigned
  object.  This can either replace an existing key/value pair with the same key
  value, or insert a new key/value pair (if no key with the same value existed).

  For user-defined objects, the :meth:`~object.__setitem__` method is called with
  appropriate arguments.

  .. index:: pair: slicing; assignment

* If the target is a slicing: The primary expression in the reference is
  evaluated.  It should yield a mutable sequence object (such as a list).  The
  assigned object should be a sequence object of the same type.  Next, the lower
  and upper bound expressions are evaluated, insofar they are present; defaults
  are zero and the sequence's length.  The bounds should evaluate to integers.
  If either bound is negative, the sequence's length is added to it.  The
  resulting bounds are clipped to lie between zero and the sequence's length,
  inclusive.  Finally, the sequence object is asked to replace the slice with
  the items of the assigned sequence.  The length of the slice may be different
  from the length of the assigned sequence, thus changing the length of the
  target sequence, if the target sequence allows it.

.. impl-detail::

   In the current implementation, the syntax for targets is taken to be the same
   as for expressions, and invalid syntax is rejected during the code generation
   phase, causing less detailed error messages.

Although the definition of assignment implies that overlaps between the
left-hand side and the right-hand side are 'simultaneous' (for example ``a, b =
b, a`` swaps two variables), overlaps *within* the collection of assigned-to
variables occur left-to-right, sometimes resulting in confusion.  For instance,
the following program prints ``[0, 2]``::

   x = [0, 1]
   i = 0
   i, x[i] = 1, 2         # i is updated, then x[i] is updated
   print(x)


.. seealso::

   :pep:`3132` - Extended Iterable Unpacking
      The specification for the ``*target`` feature.


.. _augassign:

Augmented assignment statements
-------------------------------

.. index::
   pair: augmented; assignment
   single: statement; assignment, augmented
   single: +=; augmented assignment
   single: -=; augmented assignment
   single: *=; augmented assignment
   single: /=; augmented assignment
   single: %=; augmented assignment
   single: &=; augmented assignment
   single: ^=; augmented assignment
   single: |=; augmented assignment
   single: **=; augmented assignment
   single: //=; augmented assignment
   single: >>=; augmented assignment
   single: <<=; augmented assignment

Augmented assignment is the combination, in a single statement, of a binary
operation and an assignment statement:

.. productionlist:: python-grammar
   augmented_assignment_stmt: `augtarget` `augop` (`expression_list` | `yield_expression`)
   augtarget: `identifier` | `attributeref` | `subscription` | `slicing`
   augop: "+=" | "-=" | "*=" | "@=" | "/=" | "//=" | "%=" | "**="
        : | ">>=" | "<<=" | "&=" | "^=" | "|="

(See section :ref:`primaries` for the syntax definitions of the last three
symbols.)

An augmented assignment evaluates the target (which, unlike normal assignment
statements, cannot be an unpacking) and the expression list, performs the binary
operation specific to the type of assignment on the two operands, and assigns
the result to the original target.  The target is only evaluated once.

An augmented assignment statement like ``x += 1`` can be rewritten as ``x = x +
1`` to achieve a similar, but not exactly equal effect. In the augmented
version, ``x`` is only evaluated once. Also, when possible, the actual operation
is performed *in-place*, meaning that rather than creating a new object and
assigning that to the target, the old object is modified instead.

Unlike normal assignments, augmented assignments evaluate the left-hand side
*before* evaluating the right-hand side.  For example, ``a[i] += f(x)`` first
looks-up ``a[i]``, then it evaluates ``f(x)`` and performs the addition, and
lastly, it writes the result back to ``a[i]``.

With the exception of assigning to tuples and multiple targets in a single
statement, the assignment done by augmented assignment statements is handled the
same way as normal assignments. Similarly, with the exception of the possible
*in-place* behavior, the binary operation performed by augmented assignment is
the same as the normal binary operations.

For targets which are attribute references, the same :ref:`caveat about class
and instance attributes <attr-target-note>` applies as for regular assignments.


.. _annassign:

Annotated assignment statements
-------------------------------

.. index::
   pair: annotated; assignment
   single: statement; assignment, annotated
   single: : (colon); annotated variable

:term:`Annotation <variable annotation>` assignment is the combination, in a single
statement, of a variable or attribute annotation and an optional assignment statement:

.. productionlist:: python-grammar
   annotated_assignment_stmt: `augtarget` ":" `expression`
                            : ["=" (`starred_expression` | `yield_expression`)]

The difference from normal :ref:`assignment` is that only a single target is allowed.

The assignment target is considered "simple" if it consists of a single
name that is not enclosed in parentheses.
For simple assignment targets, if in class or module scope,
the annotations are gathered in a lazily evaluated
:ref:`annotation scope <annotation-scopes>`. The annotations can be
evaluated using the :attr:`~object.__annotations__` attribute of a
class or module, or using the facilities in the :mod:`annotationlib`
module.

If the assignment target is not simple (an attribute, subscript node, or
parenthesized name), the annotation is never evaluated.

If a name is annotated in a function scope, then this name is local for
that scope. Annotations are never evaluated and stored in function scopes.

If the right hand side is present, an annotated
assignment performs the actual assignment as if there was no annotation
present. If the right hand side is not present for an expression
target, then the interpreter evaluates the target except for the last
:meth:`~object.__setitem__` or :meth:`~object.__setattr__` call.

.. seealso::

   :pep:`526` - Syntax for Variable Annotations
      The proposal that added syntax for annotating the types of variables
      (including class variables and instance variables), instead of expressing
      them through comments.

   :pep:`484` - Type hints
      The proposal that added the :mod:`typing` module to provide a standard
      syntax for type annotations that can be used in static analysis tools and
      IDEs.

.. versionchanged:: 3.8
   Now annotated assignments allow the same expressions in the right hand side as
   regular assignments. Previously, some expressions (like un-parenthesized
   tuple expressions) caused a syntax error.

.. versionchanged:: 3.14
   Annotations are now lazily evaluated in a separate :ref:`annotation scope <annotation-scopes>`.
   If the assignment target is not simple, annotations are never evaluated.


.. _assert:

The :keyword:`!assert` statement
================================

.. index::
   ! pair: statement; assert
   pair: debugging; assertions
   single: , (comma); expression list

Assert statements are a convenient way to insert debugging assertions into a
program:

.. productionlist:: python-grammar
   assert_stmt: "assert" `expression` ["," `expression`]

The simple form, ``assert expression``, is equivalent to ::

   if __debug__:
       if not expression: raise AssertionError

The extended form, ``assert expression1, expression2``, is equivalent to ::

   if __debug__:
       if not expression1: raise AssertionError(expression2)

.. index::
   single: __debug__
   pair: exception; AssertionError

These equivalences assume that :const:`__debug__` and :exc:`AssertionError` refer to
the built-in variables with those names.  In the current implementation, the
built-in variable ``__debug__`` is ``True`` under normal circumstances,
``False`` when optimization is requested (command line option :option:`-O`).  The current
code generator emits no code for an :keyword:`assert` statement when optimization is
requested at compile time.  Note that it is unnecessary to include the source
code for the expression that failed in the error message; it will be displayed
as part of the stack trace.

Assignments to :const:`__debug__` are illegal.  The value for the built-in variable
is determined when the interpreter starts.


.. _pass:

The :keyword:`!pass` statement
==============================

.. index::
   pair: statement; pass
   pair: null; operation
           pair: null; operation

.. productionlist:: python-grammar
   pass_stmt: "pass"

:keyword:`pass` is a null operation --- when it is executed, nothing happens.
It is useful as a placeholder when a statement is required syntactically, but no
code needs to be executed, for example::

   def f(arg): pass    # a function that does nothing (yet)

   class C: pass       # a class with no methods (yet)


.. _del:

The :keyword:`!del` statement
=============================

.. index::
   ! pair: statement; del
   pair: deletion; target
   triple: deletion; target; list

.. productionlist:: python-grammar
   del_stmt: "del" `target_list`

Deletion is recursively defined very similar to the way assignment is defined.
Rather than spelling it out in full details, here are some hints.

Deletion of a target list recursively deletes each target, from left to right.

.. index::
   pair: statement; global
   pair: unbinding; name

Deletion of a name removes the binding of that name from the local or global
namespace, depending on whether the name occurs in a :keyword:`global` statement
in the same code block.  If the name is unbound, a :exc:`NameError` exception
will be raised.

.. index:: pair: attribute; deletion

Deletion of attribute references, subscriptions and slicings is passed to the
primary object involved; deletion of a slicing is in general equivalent to
assignment of an empty slice of the right type (but even this is determined by
the sliced object).

.. versionchanged:: 3.2
   Previously it was illegal to delete a name from the local namespace if it
   occurs as a free variable in a nested block.


.. _return:

The :keyword:`!return` statement
================================

.. index::
   ! pair: statement; return
   pair: function; definition
   pair: class; definition

.. productionlist:: python-grammar
   return_stmt: "return" [`expression_list`]

:keyword:`return` may only occur syntactically nested in a function definition,
not within a nested class definition.

If an expression list is present, it is evaluated, else ``None`` is substituted.

:keyword:`return` leaves the current function call with the expression list (or
``None``) as return value.

.. index:: pair: keyword; finally

When :keyword:`return` passes control out of a :keyword:`try` statement with a
:keyword:`finally` clause, that :keyword:`!finally` clause is executed before
really leaving the function.

In a generator function, the :keyword:`return` statement indicates that the
generator is done and will cause :exc:`StopIteration` to be raised. The returned
value (if any) is used as an argument to construct :exc:`StopIteration` and
becomes the :attr:`StopIteration.value` attribute.

In an asynchronous generator function, an empty :keyword:`return` statement
indicates that the asynchronous generator is done and will cause
:exc:`StopAsyncIteration` to be raised.  A non-empty :keyword:`!return`
statement is a syntax error in an asynchronous generator function.

.. _yield:

The :keyword:`!yield` statement
===============================

.. index::
   pair: statement; yield
   single: generator; function
   single: generator; iterator
   single: function; generator
   pair: exception; StopIteration

.. productionlist:: python-grammar
   yield_stmt: `yield_expression`

A :keyword:`yield` statement is semantically equivalent to a :ref:`yield
expression <yieldexpr>`. The ``yield`` statement can be used to omit the
parentheses that would otherwise be required in the equivalent yield expression
statement. For example, the yield statements ::

  yield <expr>
  yield from <expr>

are equivalent to the yield expression statements ::

  (yield <expr>)
  (yield from <expr>)

Yield expressions and statements are only used when defining a :term:`generator`
function, and are only used in the body of the generator function.  Using :keyword:`yield`
in a function definition is sufficient to cause that definition to create a
generator function instead of a normal function.

For full details of :keyword:`yield` semantics, refer to the
:ref:`yieldexpr` section.

.. _raise:

The :keyword:`!raise` statement
===============================

.. index::
   ! pair: statement; raise
   single: exception
   pair: raising; exception
   single: __traceback__ (exception attribute)

.. productionlist:: python-grammar
   raise_stmt: "raise" [`expression` ["from" `expression`]]

If no expressions are present, :keyword:`raise` re-raises the
exception that is currently being handled, which is also known as the *active exception*.
If there isn't currently an active exception, a :exc:`RuntimeError` exception is raised
indicating that this is an error.

Otherwise, :keyword:`raise` evaluates the first expression as the exception
object.  It must be either a subclass or an instance of :class:`BaseException`.
If it is a class, the exception instance will be obtained when needed by
instantiating the class with no arguments.

The :dfn:`type` of the exception is the exception instance's class, the
:dfn:`value` is the instance itself.

.. index:: pair: object; traceback

A traceback object is normally created automatically when an exception is raised
and attached to it as the :attr:`~BaseException.__traceback__` attribute.
You can create an exception and set your own traceback in one step using the
:meth:`~BaseException.with_traceback` exception method (which returns the
same exception instance, with its traceback set to its argument), like so::

   raise Exception("foo occurred").with_traceback(tracebackobj)

.. index:: pair: exception; chaining
           __cause__ (exception attribute)
           __context__ (exception attribute)

The ``from`` clause is used for exception chaining: if given, the second
*expression* must be another exception class or instance. If the second
expression is an exception instance, it will be attached to the raised
exception as the :attr:`~BaseException.__cause__` attribute (which is writable). If the
expression is an exception class, the class will be instantiated and the
resulting exception instance will be attached to the raised exception as the
:attr:`!__cause__` attribute. If the raised exception is not handled, both
exceptions will be printed:

.. code-block:: pycon

   >>> try:
   ...     print(1 / 0)
   ... except Exception as exc:
   ...     raise RuntimeError("Something bad happened") from exc
   ...
   Traceback (most recent call last):
     File "<stdin>", line 2, in <module>
       print(1 / 0)
             ~~^~~
   ZeroDivisionError: division by zero

   The above exception was the direct cause of the following exception:

   Traceback (most recent call last):
     File "<stdin>", line 4, in <module>
       raise RuntimeError("Something bad happened") from exc
   RuntimeError: Something bad happened

A similar mechanism works implicitly if a new exception is raised when
an exception is already being handled.  An exception may be handled
when an :keyword:`except` or :keyword:`finally` clause, or a
:keyword:`with` statement, is used.  The previous exception is then
attached as the new exception's :attr:`~BaseException.__context__` attribute:

.. code-block:: pycon

   >>> try:
   ...     print(1 / 0)
   ... except:
   ...     raise RuntimeError("Something bad happened")
   ...
   Traceback (most recent call last):
     File "<stdin>", line 2, in <module>
       print(1 / 0)
             ~~^~~
   ZeroDivisionError: division by zero

   During handling of the above exception, another exception occurred:

   Traceback (most recent call last):
     File "<stdin>", line 4, in <module>
       raise RuntimeError("Something bad happened")
   RuntimeError: Something bad happened

Exception chaining can be explicitly suppressed by specifying :const:`None` in
the ``from`` clause:

.. doctest::

   >>> try:
   ...     print(1 / 0)
   ... except:
   ...     raise RuntimeError("Something bad happened") from None
   ...
   Traceback (most recent call last):
     File "<stdin>", line 4, in <module>
   RuntimeError: Something bad happened

Additional information on exceptions can be found in section :ref:`exceptions`,
and information about handling exceptions is in section :ref:`try`.

.. versionchanged:: 3.3
    :const:`None` is now permitted as ``Y`` in ``raise X from Y``.

    Added the :attr:`~BaseException.__suppress_context__` attribute to suppress
    automatic display of the exception context.

.. versionchanged:: 3.11
    If the traceback of the active exception is modified in an :keyword:`except`
    clause, a subsequent ``raise`` statement re-raises the exception with the
    modified traceback. Previously, the exception was re-raised with the
    traceback it had when it was caught.

.. _break:

The :keyword:`!break` statement
===============================

.. index::
   ! pair: statement; break
   pair: statement; for
   pair: statement; while
   pair: loop; statement

.. productionlist:: python-grammar
   break_stmt: "break"

:keyword:`break` may only occur syntactically nested in a :keyword:`for` or
:keyword:`while` loop, but not nested in a function or class definition within
that loop.

.. index:: pair: keyword; else
           pair: loop control; target

It terminates the nearest enclosing loop, skipping the optional :keyword:`!else`
clause if the loop has one.

If a :keyword:`for` loop is terminated by :keyword:`break`, the loop control
target keeps its current value.

.. index:: pair: keyword; finally

When :keyword:`break` passes control out of a :keyword:`try` statement with a
:keyword:`finally` clause, that :keyword:`!finally` clause is executed before
really leaving the loop.


.. _continue:

The :keyword:`!continue` statement
==================================

.. index::
   ! pair: statement; continue
   pair: statement; for
   pair: statement; while
   pair: loop; statement
   pair: keyword; finally

.. productionlist:: python-grammar
   continue_stmt: "continue"

:keyword:`continue` may only occur syntactically nested in a :keyword:`for` or
:keyword:`while` loop, but not nested in a function or class definition within
that loop.  It continues with the next cycle of the nearest enclosing loop.

When :keyword:`continue` passes control out of a :keyword:`try` statement with a
:keyword:`finally` clause, that :keyword:`!finally` clause is executed before
really starting the next loop cycle.


.. _import:
.. _from:

The :keyword:`!import` statement
================================

.. index::
   ! pair: statement; import
   single: module; importing
   pair: name; binding
   pair: keyword; from
   pair: keyword; as
   pair: exception; ImportError
   single: , (comma); import statement

.. productionlist:: python-grammar
   import_stmt: "import" `module` ["as" `identifier`] ("," `module` ["as" `identifier`])*
              : | "from" `relative_module` "import" `identifier` ["as" `identifier`]
              : ("," `identifier` ["as" `identifier`])*
              : | "from" `relative_module` "import" "(" `identifier` ["as" `identifier`]
              : ("," `identifier` ["as" `identifier`])* [","] ")"
              : | "from" `relative_module` "import" "*"
   module: (`identifier` ".")* `identifier`
   relative_module: "."* `module` | "."+

The basic import statement (no :keyword:`from` clause) is executed in two
steps:

#. find a module, loading and initializing it if necessary
#. define a name or names in the local namespace for the scope where
   the :keyword:`import` statement occurs.

When the statement contains multiple clauses (separated by
commas) the two steps are carried out separately for each clause, just
as though the clauses had been separated out into individual import
statements.

The details of the first step, finding and loading modules, are described in
greater detail in the section on the :ref:`import system <importsystem>`,
which also describes the various types of packages and modules that can
be imported, as well as all the hooks that can be used to customize
the import system. Note that failures in this step may indicate either
that the module could not be located, *or* that an error occurred while
initializing the module, which includes execution of the module's code.

If the requested module is retrieved successfully, it will be made
available in the local namespace in one of three ways:

.. index:: single: as; import statement

* If the module name is followed by :keyword:`!as`, then the name
  following :keyword:`!as` is bound directly to the imported module.
* If no other name is specified, and the module being imported is a top
  level module, the module's name is bound in the local namespace as a
  reference to the imported module
* If the module being imported is *not* a top level module, then the name
  of the top level package that contains the module is bound in the local
  namespace as a reference to the top level package. The imported module
  must be accessed using its full qualified name rather than directly


.. index::
   pair: name; binding
   single: from; import statement

The :keyword:`from` form uses a slightly more complex process:

#. find the module specified in the :keyword:`from` clause, loading and
   initializing it if necessary;
#. for each of the identifiers specified in the :keyword:`import` clauses:

   #. check if the imported module has an attribute by that name
   #. if not, attempt to import a submodule with that name and then
      check the imported module again for that attribute
   #. if the attribute is not found, :exc:`ImportError` is raised.
   #. otherwise, a reference to that value is stored in the local namespace,
      using the name in the :keyword:`!as` clause if it is present,
      otherwise using the attribute name

Examples::

   import foo                 # foo imported and bound locally
   import foo.bar.baz         # foo, foo.bar, and foo.bar.baz imported, foo bound locally
   import foo.bar.baz as fbb  # foo, foo.bar, and foo.bar.baz imported, foo.bar.baz bound as fbb
   from foo.bar import baz    # foo, foo.bar, and foo.bar.baz imported, foo.bar.baz bound as baz
   from foo import attr       # foo imported and foo.attr bound as attr

.. index:: single: * (asterisk); import statement

If the list of identifiers is replaced by a star (``'*'``), all public
names defined in the module are bound in the local namespace for the scope
where the :keyword:`import` statement occurs.

.. index:: single: __all__ (optional module attribute)

The *public names* defined by a module are determined by checking the module's
namespace for a variable named ``__all__``; if defined, it must be a sequence
of strings which are names defined or imported by that module.  The names
given in ``__all__`` are all considered public and are required to exist.  If
``__all__`` is not defined, the set of public names includes all names found
in the module's namespace which do not begin with an underscore character
(``'_'``).  ``__all__`` should contain the entire public API. It is intended
to avoid accidentally exporting items that are not part of the API (such as
library modules which were imported and used within the module).

The wild card form of import --- ``from module import *`` --- is only allowed at
the module level.  Attempting to use it in class or function definitions will
raise a :exc:`SyntaxError`.

.. index::
    single: relative; import

When specifying what module to import you do not have to specify the absolute
name of the module. When a module or package is contained within another
package it is possible to make a relative import within the same top package
without having to mention the package name. By using leading dots in the
specified module or package after :keyword:`from` you can specify how high to
traverse up the current package hierarchy without specifying exact names. One
leading dot means the current package where the module making the import
exists. Two dots means up one package level. Three dots is up two levels, etc.
So if you execute ``from . import mod`` from a module in the ``pkg`` package
then you will end up importing ``pkg.mod``. If you execute ``from ..subpkg2
import mod`` from within ``pkg.subpkg1`` you will import ``pkg.subpkg2.mod``.
The specification for relative imports is contained in
the :ref:`relativeimports` section.

:func:`importlib.import_module` is provided to support applications that
determine dynamically the modules to be loaded.

.. audit-event:: import module,filename,sys.path,sys.meta_path,sys.path_hooks import

.. _future:

Future statements
-----------------

.. index::
   pair: future; statement
   single: __future__; future statement

A :dfn:`future statement` is a directive to the compiler that a particular
module should be compiled using syntax or semantics that will be available in a
specified future release of Python where the feature becomes standard.

The future statement is intended to ease migration to future versions of Python
that introduce incompatible changes to the language.  It allows use of the new
features on a per-module basis before the release in which the feature becomes
standard.

.. productionlist:: python-grammar
   future_stmt: "from" "__future__" "import" `feature` ["as" `identifier`]
              : ("," `feature` ["as" `identifier`])*
              : | "from" "__future__" "import" "(" `feature` ["as" `identifier`]
              : ("," `feature` ["as" `identifier`])* [","] ")"
   feature: `identifier`

A future statement must appear near the top of the module.  The only lines that
can appear before a future statement are:

* the module docstring (if any),
* comments,
* blank lines, and
* other future statements.

The only feature that requires using the future statement is
``annotations`` (see :pep:`563`).

All historical features enabled by the future statement are still recognized
by Python 3.  The list includes ``absolute_import``, ``division``,
``generators``, ``generator_stop``, ``unicode_literals``,
``print_function``, ``nested_scopes`` and ``with_statement``.  They are
all redundant because they are always enabled, and only kept for
backwards compatibility.

A future statement is recognized and treated specially at compile time: Changes
to the semantics of core constructs are often implemented by generating
different code.  It may even be the case that a new feature introduces new
incompatible syntax (such as a new reserved word), in which case the compiler
may need to parse the module differently.  Such decisions cannot be pushed off
until runtime.

For any given release, the compiler knows which feature names have been defined,
and raises a compile-time error if a future statement contains a feature not
known to it.

The direct runtime semantics are the same as for any import statement: there is
a standard module :mod:`__future__`, described later, and it will be imported in
the usual way at the time the future statement is executed.

The interesting runtime semantics depend on the specific feature enabled by the
future statement.

Note that there is nothing special about the statement::

   import __future__ [as name]

That is not a future statement; it's an ordinary import statement with no
special semantics or syntax restrictions.

Code compiled by calls to the built-in functions :func:`exec` and :func:`compile`
that occur in a module :mod:`!M` containing a future statement will, by default,
use the new syntax or semantics associated with the future statement.  This can
be controlled by optional arguments to :func:`compile` --- see the documentation
of that function for details.

A future statement typed at an interactive interpreter prompt will take effect
for the rest of the interpreter session.  If an interpreter is started with the
:option:`-i` option, is passed a script name to execute, and the script includes
a future statement, it will be in effect in the interactive session started
after the script is executed.

.. seealso::

   :pep:`236` - Back to the __future__
      The original proposal for the __future__ mechanism.


.. _global:

The :keyword:`!global` statement
================================

.. index::
   ! pair: statement; global
   triple: global; name; binding
   single: , (comma); identifier list

.. productionlist:: python-grammar
   global_stmt: "global" `identifier` ("," `identifier`)*

The :keyword:`global` statement causes the listed identifiers to be interpreted
as globals. It would be impossible to assign to a global variable without
:keyword:`!global`, although free variables may refer to globals without being
declared global.

The :keyword:`global` statement applies to the entire scope of a function or
class body. A :exc:`SyntaxError` is raised if a variable is used or
assigned to prior to its global declaration in the scope.

.. index::
   pair: built-in function; exec
   pair: built-in function; eval
   pair: built-in function; compile

**Programmer's note:** :keyword:`global` is a directive to the parser.  It
applies only to code parsed at the same time as the :keyword:`!global` statement.
In particular, a :keyword:`!global` statement contained in a string or code
object supplied to the built-in :func:`exec` function does not affect the code
block *containing* the function call, and code contained in such a string is
unaffected by :keyword:`!global` statements in the code containing the function
call.  The same applies to the :func:`eval` and :func:`compile` functions.


.. _nonlocal:

The :keyword:`!nonlocal` statement
==================================

.. index:: pair: statement; nonlocal
   single: , (comma); identifier list

.. productionlist:: python-grammar
   nonlocal_stmt: "nonlocal" `identifier` ("," `identifier`)*

When the definition of a function or class is nested (enclosed) within
the definitions of other functions, its nonlocal scopes are the local
scopes of the enclosing functions. The :keyword:`nonlocal` statement
causes the listed identifiers to refer to names previously bound in
nonlocal scopes. It allows encapsulated code to rebind such nonlocal
identifiers.  If a name is bound in more than one nonlocal scope, the
nearest binding is used. If a name is not bound in any nonlocal scope,
or if there is no nonlocal scope, a :exc:`SyntaxError` is raised.

The :keyword:`nonlocal` statement applies to the entire scope of a function or
class body. A :exc:`SyntaxError` is raised if a variable is used or
assigned to prior to its nonlocal declaration in the scope.

.. seealso::

   :pep:`3104` - Access to Names in Outer Scopes
      The specification for the :keyword:`nonlocal` statement.

**Programmer's note:** :keyword:`nonlocal` is a directive to the parser
and applies only to code parsed along with it.  See the note for the
:keyword:`global` statement.


.. _type:

The :keyword:`!type` statement
==============================

.. index:: pair: statement; type

.. productionlist:: python-grammar
   type_stmt: 'type' `identifier` [`type_params`] "=" `expression`

The :keyword:`!type` statement declares a type alias, which is an instance
of :class:`typing.TypeAliasType`.

For example, the following statement creates a type alias::

   type Point = tuple[float, float]

This code is roughly equivalent to::

   annotation-def VALUE_OF_Point():
       return tuple[float, float]
   Point = typing.TypeAliasType("Point", VALUE_OF_Point())

``annotation-def`` indicates an :ref:`annotation scope <annotation-scopes>`, which behaves
mostly like a function, but with several small differences.

The value of the
type alias is evaluated in the annotation scope. It is not evaluated when the
type alias is created, but only when the value is accessed through the type alias's
:attr:`!__value__` attribute (see :ref:`lazy-evaluation`).
This allows the type alias to refer to names that are not yet defined.

Type aliases may be made generic by adding a :ref:`type parameter list <type-params>`
after the name. See :ref:`generic-type-aliases` for more.

:keyword:`!type` is a :ref:`soft keyword <soft-keywords>`.

.. versionadded:: 3.12

.. seealso::

   :pep:`695` - Type Parameter Syntax
      Introduced the :keyword:`!type` statement and syntax for
      generic classes and functions.


================================================
File: /Doc/reference/toplevel_components.rst
================================================

.. _top-level:

********************
Top-level components
********************

.. index:: single: interpreter

The Python interpreter can get its input from a number of sources: from a script
passed to it as standard input or as program argument, typed in interactively,
from a module source file, etc.  This chapter gives the syntax used in these
cases.


.. _programs:

Complete Python programs
========================

.. index:: single: program

.. index::
   pair: module; sys
   pair: module; __main__
   pair: module; builtins

While a language specification need not prescribe how the language interpreter
is invoked, it is useful to have a notion of a complete Python program.  A
complete Python program is executed in a minimally initialized environment: all
built-in and standard modules are available, but none have been initialized,
except for :mod:`sys` (various system services), :mod:`builtins` (built-in
functions, exceptions and ``None``) and :mod:`__main__`.  The latter is used to
provide the local and global namespace for execution of the complete program.

The syntax for a complete Python program is that for file input, described in
the next section.

.. index::
   single: interactive mode
   pair: module; __main__

The interpreter may also be invoked in interactive mode; in this case, it does
not read and execute a complete program but reads and executes one statement
(possibly compound) at a time.  The initial environment is identical to that of
a complete program; each statement is executed in the namespace of
:mod:`__main__`.

.. index::
   single: UNIX
   single: Windows
   single: command line
   single: standard input

A complete program can be passed to the interpreter
in three forms: with the :option:`-c` *string* command line option, as a file
passed as the first command line argument, or as standard input.  If the file
or standard input is a tty device, the interpreter enters interactive mode;
otherwise, it executes the file as a complete program.


.. _file-input:

File input
==========

All input read from non-interactive files has the same form:

.. productionlist:: python-grammar
   file_input: (NEWLINE | `statement`)*

This syntax is used in the following situations:

* when parsing a complete Python program (from a file or from a string);

* when parsing a module;

* when parsing a string passed to the :func:`exec` function;


.. _interactive:

Interactive input
=================

Input in interactive mode is parsed using the following grammar:

.. productionlist:: python-grammar
   interactive_input: [`stmt_list`] NEWLINE | `compound_stmt` NEWLINE

Note that a (top-level) compound statement must be followed by a blank line in
interactive mode; this is needed to help the parser detect the end of the input.


.. _expression-input:

Expression input
================

.. index:: single: input
.. index:: pair: built-in function; eval

:func:`eval` is used for expression input.  It ignores leading whitespace. The
string argument to :func:`eval` must have the following form:

.. productionlist:: python-grammar
   eval_input: `expression_list` NEWLINE*


================================================
File: /Doc/tools/check-warnings.py
================================================
#!/usr/bin/env python3
"""
Check the output of running Sphinx in nit-picky mode (missing references).
"""

from __future__ import annotations

import argparse
import itertools
import os
import re
import subprocess
import sys
from pathlib import Path
from typing import TextIO

# Fail if NEWS nit found before this line number
NEWS_NIT_THRESHOLD = 1700

# Exclude these whether they're dirty or clean,
# because they trigger a rebuild of dirty files.
EXCLUDE_FILES = {
    "Doc/whatsnew/changelog.rst",
}

# Subdirectories of Doc/ to exclude.
EXCLUDE_SUBDIRS = {
    ".env",
    ".venv",
    "env",
    "includes",
    "venv",
}

# Regex pattern to match the parts of a Sphinx warning
WARNING_PATTERN = re.compile(
    r"(?P<file>([A-Za-z]:[\\/])?[^:]+):(?P<line>\d+): WARNING: (?P<msg>.+)"
)

# Regex pattern to match the line numbers in a Git unified diff
DIFF_PATTERN = re.compile(
    r"^@@ -(?P<linea>\d+)(?:,(?P<removed>\d+))? \+(?P<lineb>\d+)(?:,(?P<added>\d+))? @@",
    flags=re.MULTILINE,
)


def get_diff_files(ref_a: str, ref_b: str, filter_mode: str = "") -> set[Path]:
    """List the files changed between two Git refs, filtered by change type."""
    added_files_result = subprocess.run(
        [
            "git",
            "diff",
            f"--diff-filter={filter_mode}",
            "--name-only",
            f"{ref_a}...{ref_b}",
            "--",
        ],
        stdout=subprocess.PIPE,
        check=True,
        text=True,
        encoding="UTF-8",
    )

    added_files = added_files_result.stdout.strip().split("\n")
    return {Path(file.strip()) for file in added_files if file.strip()}


def get_diff_lines(ref_a: str, ref_b: str, file: Path) -> list[int]:
    """List the lines changed between two Git refs for a specific file."""
    diff_output = subprocess.run(
        [
            "git",
            "diff",
            "--unified=0",
            f"{ref_a}...{ref_b}",
            "--",
            str(file),
        ],
        stdout=subprocess.PIPE,
        check=True,
        text=True,
        encoding="UTF-8",
    )

    # Scrape line offsets + lengths from diff and convert to line numbers
    line_matches = DIFF_PATTERN.finditer(diff_output.stdout)
    # Removed and added line counts are 1 if not printed
    line_match_values = [
        line_match.groupdict(default=1) for line_match in line_matches
    ]
    line_ints = [
        (int(match_value["lineb"]), int(match_value["added"]))
        for match_value in line_match_values
    ]
    line_ranges = [
        range(line_b, line_b + added) for line_b, added in line_ints
    ]
    line_numbers = list(itertools.chain(*line_ranges))

    return line_numbers


def get_para_line_numbers(file_obj: TextIO) -> list[list[int]]:
    """Get the line numbers of text in a file object, grouped by paragraph."""
    paragraphs = []
    prev_line = None
    for lineno, line in enumerate(file_obj):
        lineno = lineno + 1
        if prev_line is None or (line.strip() and not prev_line.strip()):
            paragraph = [lineno - 1]
            paragraphs.append(paragraph)
        paragraph.append(lineno)
        prev_line = line
    return paragraphs


def filter_and_parse_warnings(
    warnings: list[str], files: set[Path]
) -> list[re.Match[str]]:
    """Get the warnings matching passed files and parse them with regex."""
    filtered_warnings = [
        warning
        for warning in warnings
        if any(str(file) in warning for file in files)
    ]
    warning_matches = [
        WARNING_PATTERN.fullmatch(warning.strip())
        for warning in filtered_warnings
    ]
    non_null_matches = [warning for warning in warning_matches if warning]
    return non_null_matches


def filter_warnings_by_diff(
    warnings: list[re.Match[str]], ref_a: str, ref_b: str, file: Path
) -> list[re.Match[str]]:
    """Filter the passed per-file warnings to just those on changed lines."""
    diff_lines = get_diff_lines(ref_a, ref_b, file)
    with file.open(encoding="UTF-8") as file_obj:
        paragraphs = get_para_line_numbers(file_obj)
    touched_paras = [
        para_lines
        for para_lines in paragraphs
        if set(diff_lines) & set(para_lines)
    ]
    touched_para_lines = set(itertools.chain(*touched_paras))
    warnings_infile = [
        warning for warning in warnings if str(file) in warning["file"]
    ]
    warnings_touched = [
        warning
        for warning in warnings_infile
        if int(warning["line"]) in touched_para_lines
    ]
    return warnings_touched


def process_touched_warnings(
    warnings: list[str], ref_a: str, ref_b: str
) -> list[re.Match[str]]:
    """Filter a list of Sphinx warnings to those affecting touched lines."""
    added_files, modified_files = tuple(
        get_diff_files(ref_a, ref_b, filter_mode=mode) for mode in ("A", "M")
    )

    warnings_added = filter_and_parse_warnings(warnings, added_files)
    warnings_modified = filter_and_parse_warnings(warnings, modified_files)

    modified_files_warned = {
        file
        for file in modified_files
        if any(str(file) in warning["file"] for warning in warnings_modified)
    }

    warnings_modified_touched = [
        filter_warnings_by_diff(warnings_modified, ref_a, ref_b, file)
        for file in modified_files_warned
    ]
    warnings_touched = warnings_added + list(
        itertools.chain(*warnings_modified_touched)
    )

    return warnings_touched


def annotate_diff(
    warnings: list[str], ref_a: str = "main", ref_b: str = "HEAD"
) -> None:
    """
    Convert Sphinx warning messages to GitHub Actions for changed paragraphs.

    Converts lines like:
        .../Doc/library/cgi.rst:98: WARNING: reference target not found
    to:
        ::warning file=.../Doc/library/cgi.rst,line=98::reference target not found

    See:
    https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#setting-a-warning-message
    """
    warnings_touched = process_touched_warnings(warnings, ref_a, ref_b)
    print("Emitting doc warnings matching modified lines:")
    for warning in warnings_touched:
        print("::warning file={file},line={line}::{msg}".format_map(warning))
        print(warning[0])
    if not warnings_touched:
        print("None")


def fail_if_regression(
    warnings: list[str],
    files_with_expected_nits: set[str],
    files_with_nits: set[str],
) -> int:
    """
    Ensure some files always pass Sphinx nit-picky mode (no missing references).
    These are files which are *not* in .nitignore.
    """
    all_rst = {
        str(rst)
        for rst in Path("Doc/").rglob("*.rst")
        if rst.parts[1] not in EXCLUDE_SUBDIRS
    }
    should_be_clean = all_rst - files_with_expected_nits - EXCLUDE_FILES
    problem_files = sorted(should_be_clean & files_with_nits)
    if problem_files:
        print("\nError: must not contain warnings:\n")
        for filename in problem_files:
            print(filename)
            for warning in warnings:
                if filename in warning:
                    if match := WARNING_PATTERN.fullmatch(warning):
                        print("  {line}: {msg}".format_map(match))
        return -1
    return 0


def fail_if_improved(
    files_with_expected_nits: set[str], files_with_nits: set[str]
) -> int:
    """
    We may have fixed warnings in some files so that the files are now completely clean.
    Good news! Let's add them to .nitignore to prevent regression.
    """
    files_with_no_nits = files_with_expected_nits - files_with_nits
    if files_with_no_nits:
        print("\nCongratulations! You improved:\n")
        for filename in sorted(files_with_no_nits):
            print(filename)
        print("\nPlease remove from Doc/tools/.nitignore\n")
        return -1
    return 0


def fail_if_new_news_nit(warnings: list[str], threshold: int) -> int:
    """
    Ensure no warnings are found in the NEWS file before a given line number.
    """
    news_nits = (warning for warning in warnings if "/build/NEWS:" in warning)

    # Nits found before the threshold line
    new_news_nits = [
        nit for nit in news_nits if int(nit.split(":")[1]) <= threshold
    ]

    if new_news_nits:
        print("\nError: new NEWS nits:\n")
        for warning in new_news_nits:
            print(warning)
        return -1

    return 0


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--annotate-diff",
        nargs="*",
        metavar=("BASE_REF", "HEAD_REF"),
        help="Add GitHub Actions annotations on the diff for warnings on "
        "lines changed between the given refs (main and HEAD, by default)",
    )
    parser.add_argument(
        "--fail-if-regression",
        action="store_true",
        help="Fail if known-good files have warnings",
    )
    parser.add_argument(
        "--fail-if-improved",
        action="store_true",
        help="Fail if new files with no nits are found",
    )
    parser.add_argument(
        "--fail-if-new-news-nit",
        metavar="threshold",
        type=int,
        nargs="?",
        const=NEWS_NIT_THRESHOLD,
        help="Fail if new NEWS nit found before threshold line number",
    )

    args = parser.parse_args(argv)
    if args.annotate_diff is not None and len(args.annotate_diff) > 2:
        parser.error(
            "--annotate-diff takes between 0 and 2 ref args, not "
            f"{len(args.annotate_diff)} {tuple(args.annotate_diff)}"
        )
    exit_code = 0

    wrong_directory_msg = "Must run this script from the repo root"
    if not Path("Doc").exists() or not Path("Doc").is_dir():
        raise RuntimeError(wrong_directory_msg)

    with Path("Doc/sphinx-warnings.txt").open(encoding="UTF-8") as f:
        warnings = f.read().splitlines()

    cwd = str(Path.cwd()) + os.path.sep
    files_with_nits = {
        warning.removeprefix(cwd).split(":")[0]
        for warning in warnings
        if "Doc/" in warning
    }

    with Path("Doc/tools/.nitignore").open(encoding="UTF-8") as clean_files:
        files_with_expected_nits = {
            filename.strip()
            for filename in clean_files
            if filename.strip() and not filename.startswith("#")
        }

    if args.annotate_diff is not None:
        annotate_diff(warnings, *args.annotate_diff)

    if args.fail_if_regression:
        exit_code += fail_if_regression(
            warnings, files_with_expected_nits, files_with_nits
        )

    if args.fail_if_improved:
        exit_code += fail_if_improved(
            files_with_expected_nits, files_with_nits
        )

    if args.fail_if_new_news_nit:
        exit_code += fail_if_new_news_nit(warnings, args.fail_if_new_news_nit)

    return exit_code


if __name__ == "__main__":
    sys.exit(main())


================================================
File: /Doc/tools/.nitignore
================================================
# All RST files under Doc/ -- except these -- must pass Sphinx nit-picky mode,
# as tested on the CI via check-warnings.py in reusable-docs.yml.
# Keep lines sorted lexicographically to help avoid merge conflicts.

Doc/c-api/descriptor.rst
Doc/c-api/float.rst
Doc/c-api/init.rst
Doc/c-api/init_config.rst
Doc/c-api/intro.rst
Doc/c-api/module.rst
Doc/c-api/stable.rst
Doc/c-api/type.rst
Doc/c-api/typeobj.rst
Doc/extending/extending.rst
Doc/glossary.rst
Doc/library/ast.rst
Doc/library/asyncio-extending.rst
Doc/library/asyncio-subprocess.rst
Doc/library/collections.rst
Doc/library/dbm.rst
Doc/library/decimal.rst
Doc/library/email.charset.rst
Doc/library/email.compat32-message.rst
Doc/library/email.errors.rst
Doc/library/email.parser.rst
Doc/library/exceptions.rst
Doc/library/functools.rst
Doc/library/http.cookiejar.rst
Doc/library/http.server.rst
Doc/library/importlib.rst
Doc/library/logging.config.rst
Doc/library/logging.handlers.rst
Doc/library/lzma.rst
Doc/library/mmap.rst
Doc/library/multiprocessing.rst
Doc/library/optparse.rst
Doc/library/os.rst
Doc/library/pickletools.rst
Doc/library/platform.rst
Doc/library/plistlib.rst
Doc/library/profile.rst
Doc/library/pyexpat.rst
Doc/library/readline.rst
Doc/library/resource.rst
Doc/library/select.rst
Doc/library/signal.rst
Doc/library/smtplib.rst
Doc/library/socket.rst
Doc/library/ssl.rst
Doc/library/stdtypes.rst
Doc/library/subprocess.rst
Doc/library/termios.rst
Doc/library/test.rst
Doc/library/tkinter.rst
Doc/library/tkinter.scrolledtext.rst
Doc/library/tkinter.ttk.rst
Doc/library/unittest.mock.rst
Doc/library/unittest.rst
Doc/library/urllib.parse.rst
Doc/library/urllib.request.rst
Doc/library/wsgiref.rst
Doc/library/xml.dom.minidom.rst
Doc/library/xml.dom.pulldom.rst
Doc/library/xml.dom.rst
Doc/library/xml.sax.handler.rst
Doc/library/xml.sax.reader.rst
Doc/library/xml.sax.rst
Doc/library/xmlrpc.client.rst
Doc/library/xmlrpc.server.rst
Doc/library/zlib.rst
Doc/reference/compound_stmts.rst
Doc/reference/datamodel.rst
Doc/using/windows.rst
Doc/whatsnew/2.4.rst
Doc/whatsnew/2.5.rst
Doc/whatsnew/2.6.rst
Doc/whatsnew/2.7.rst
Doc/whatsnew/3.3.rst
Doc/whatsnew/3.4.rst
Doc/whatsnew/3.5.rst
Doc/whatsnew/3.6.rst
Doc/whatsnew/3.7.rst
Doc/whatsnew/3.8.rst
Doc/whatsnew/3.9.rst
Doc/whatsnew/3.10.rst
Doc/whatsnew/3.11.rst


================================================
File: /Doc/tools/extensions/audit_events.py
================================================
"""Support for documenting audit events."""

from __future__ import annotations

import re
from typing import TYPE_CHECKING

from docutils import nodes
from sphinx.errors import NoUri
from sphinx.locale import _ as sphinx_gettext
from sphinx.transforms.post_transforms import SphinxPostTransform
from sphinx.util import logging
from sphinx.util.docutils import SphinxDirective

if TYPE_CHECKING:
    from collections.abc import Iterator

    from sphinx.application import Sphinx
    from sphinx.builders import Builder
    from sphinx.environment import BuildEnvironment

logger = logging.getLogger(__name__)

# This list of sets are allowable synonyms for event argument names.
# If two names are in the same set, they are treated as equal for the
# purposes of warning. This won't help if the number of arguments is
# different!
_SYNONYMS = [
    frozenset({"file", "path", "fd"}),
]


class AuditEvents:
    def __init__(self) -> None:
        self.events: dict[str, list[str]] = {}
        self.sources: dict[str, list[tuple[str, str]]] = {}

    def __iter__(self) -> Iterator[tuple[str, list[str], tuple[str, str]]]:
        for name, args in self.events.items():
            for source in self.sources[name]:
                yield name, args, source

    def add_event(
        self, name, args: list[str], source: tuple[str, str]
    ) -> None:
        if name in self.events:
            self._check_args_match(name, args)
        else:
            self.events[name] = args
        self.sources.setdefault(name, []).append(source)

    def _check_args_match(self, name: str, args: list[str]) -> None:
        current_args = self.events[name]
        msg = (
            f"Mismatched arguments for audit-event {name}: "
            f"{current_args!r} != {args!r}"
        )
        if current_args == args:
            return
        if len(current_args) != len(args):
            logger.warning(msg)
            return
        for a1, a2 in zip(current_args, args, strict=False):
            if a1 == a2:
                continue
            if any(a1 in s and a2 in s for s in _SYNONYMS):
                continue
            logger.warning(msg)
            return

    def id_for(self, name) -> str:
        source_count = len(self.sources.get(name, ()))
        name_clean = re.sub(r"\W", "_", name)
        return f"audit_event_{name_clean}_{source_count}"

    def rows(self) -> Iterator[tuple[str, list[str], list[tuple[str, str]]]]:
        for name in sorted(self.events.keys()):
            yield name, self.events[name], self.sources[name]


def initialise_audit_events(app: Sphinx) -> None:
    """Initialise the audit_events attribute on the environment."""
    if not hasattr(app.env, "audit_events"):
        app.env.audit_events = AuditEvents()


def audit_events_purge(
    app: Sphinx, env: BuildEnvironment, docname: str
) -> None:
    """This is to remove traces of removed documents from env.audit_events."""
    fresh_audit_events = AuditEvents()
    for name, args, (doc, target) in env.audit_events:
        if doc != docname:
            fresh_audit_events.add_event(name, args, (doc, target))


def audit_events_merge(
    app: Sphinx,
    env: BuildEnvironment,
    docnames: list[str],
    other: BuildEnvironment,
) -> None:
    """In Sphinx parallel builds, this merges audit_events from subprocesses."""
    for name, args, source in other.audit_events:
        env.audit_events.add_event(name, args, source)


class AuditEvent(SphinxDirective):
    has_content = True
    required_arguments = 1
    optional_arguments = 2
    final_argument_whitespace = True

    _label = [
        sphinx_gettext(
            "Raises an :ref:`auditing event <auditing>` "
            "{name} with no arguments."
        ),
        sphinx_gettext(
            "Raises an :ref:`auditing event <auditing>` "
            "{name} with argument {args}."
        ),
        sphinx_gettext(
            "Raises an :ref:`auditing event <auditing>` "
            "{name} with arguments {args}."
        ),
    ]

    def run(self) -> list[nodes.paragraph]:
        name = self.arguments[0]
        if len(self.arguments) >= 2 and self.arguments[1]:
            args = [
                arg
                for argument in self.arguments[1].strip("'\"").split(",")
                if (arg := argument.strip())
            ]
        else:
            args = []
        ids = []
        try:
            target = self.arguments[2].strip("\"'")
        except (IndexError, TypeError):
            target = None
        if not target:
            target = self.env.audit_events.id_for(name)
            ids.append(target)
        self.env.audit_events.add_event(name, args, (self.env.docname, target))

        node = nodes.paragraph("", classes=["audit-hook"], ids=ids)
        self.set_source_info(node)
        if self.content:
            node.rawsource = '\n'.join(self.content)  # for gettext
            self.state.nested_parse(self.content, self.content_offset, node)
        else:
            num_args = min(2, len(args))
            text = self._label[num_args].format(
                name=f"``{name}``",
                args=", ".join(f"``{a}``" for a in args),
            )
            node.rawsource = text  # for gettext
            parsed, messages = self.state.inline_text(text, self.lineno)
            node += parsed
            node += messages
        return [node]


class audit_event_list(nodes.General, nodes.Element):  # noqa: N801
    pass


class AuditEventListDirective(SphinxDirective):
    def run(self) -> list[audit_event_list]:
        return [audit_event_list()]


class AuditEventListTransform(SphinxPostTransform):
    default_priority = 500

    def run(self) -> None:
        if self.document.next_node(audit_event_list) is None:
            return

        table = self._make_table(self.app.builder, self.env.docname)
        for node in self.document.findall(audit_event_list):
            node.replace_self(table)

    def _make_table(self, builder: Builder, docname: str) -> nodes.table:
        table = nodes.table(cols=3)
        group = nodes.tgroup(
            "",
            nodes.colspec(colwidth=30),
            nodes.colspec(colwidth=55),
            nodes.colspec(colwidth=15),
            cols=3,
        )
        head = nodes.thead()
        body = nodes.tbody()

        table += group
        group += head
        group += body

        head += nodes.row(
            "",
            nodes.entry("", nodes.paragraph("", "Audit event")),
            nodes.entry("", nodes.paragraph("", "Arguments")),
            nodes.entry("", nodes.paragraph("", "References")),
        )

        for name, args, sources in builder.env.audit_events.rows():
            body += self._make_row(builder, docname, name, args, sources)

        return table

    @staticmethod
    def _make_row(
        builder: Builder,
        docname: str,
        name: str,
        args: list[str],
        sources: list[tuple[str, str]],
    ) -> nodes.row:
        row = nodes.row()
        name_node = nodes.paragraph("", nodes.Text(name))
        row += nodes.entry("", name_node)

        args_node = nodes.paragraph()
        for arg in args:
            args_node += nodes.literal(arg, arg)
            args_node += nodes.Text(", ")
        if len(args_node.children) > 0:
            args_node.children.pop()  # remove trailing comma
        row += nodes.entry("", args_node)

        backlinks_node = nodes.paragraph()
        backlinks = enumerate(sorted(set(sources)), start=1)
        for i, (doc, label) in backlinks:
            if isinstance(label, str):
                ref = nodes.reference("", f"[{i}]", internal=True)
                try:
                    target = (
                        f"{builder.get_relative_uri(docname, doc)}#{label}"
                    )
                except NoUri:
                    continue
                else:
                    ref["refuri"] = target
                    backlinks_node += ref
        row += nodes.entry("", backlinks_node)
        return row


def setup(app: Sphinx):
    app.add_directive("audit-event", AuditEvent)
    app.add_directive("audit-event-table", AuditEventListDirective)
    app.add_post_transform(AuditEventListTransform)
    app.connect("builder-inited", initialise_audit_events)
    app.connect("env-purge-doc", audit_events_purge)
    app.connect("env-merge-info", audit_events_merge)
    return {
        "version": "1.0",
        "parallel_read_safe": True,
        "parallel_write_safe": True,
    }


================================================
File: /Doc/tools/extensions/availability.py
================================================
"""Support for documenting platform availability"""

from __future__ import annotations

from typing import TYPE_CHECKING

from docutils import nodes
from sphinx import addnodes
from sphinx.util import logging
from sphinx.util.docutils import SphinxDirective

if TYPE_CHECKING:
    from sphinx.application import Sphinx
    from sphinx.util.typing import ExtensionMetadata

logger = logging.getLogger("availability")

# known platform, libc, and threading implementations
_PLATFORMS = frozenset({
    "AIX",
    "Android",
    "BSD",
    "DragonFlyBSD",
    "Emscripten",
    "FreeBSD",
    "GNU/kFreeBSD",
    "iOS",
    "Linux",
    "macOS",
    "NetBSD",
    "OpenBSD",
    "POSIX",
    "Solaris",
    "Unix",
    "VxWorks",
    "WASI",
    "Windows",
})
_LIBC = frozenset({
    "BSD libc",
    "glibc",
    "musl",
})
_THREADING = frozenset({
    # POSIX platforms with pthreads
    "pthreads",
})
KNOWN_PLATFORMS = _PLATFORMS | _LIBC | _THREADING


class Availability(SphinxDirective):
    has_content = True
    required_arguments = 1
    optional_arguments = 0
    final_argument_whitespace = True

    def run(self) -> list[nodes.container]:
        title = "Availability"
        refnode = addnodes.pending_xref(
            title,
            nodes.inline(title, title, classes=["xref", "std", "std-ref"]),
            refdoc=self.env.docname,
            refdomain="std",
            refexplicit=True,
            reftarget="availability",
            reftype="ref",
            refwarn=True,
        )
        sep = nodes.Text(": ")
        parsed, msgs = self.state.inline_text(self.arguments[0], self.lineno)
        pnode = nodes.paragraph(title, "", refnode, sep, *parsed, *msgs)
        self.set_source_info(pnode)
        cnode = nodes.container("", pnode, classes=["availability"])
        self.set_source_info(cnode)
        if self.content:
            self.state.nested_parse(self.content, self.content_offset, cnode)
        self.parse_platforms()

        return [cnode]

    def parse_platforms(self) -> dict[str, str | bool]:
        """Parse platform information from arguments

        Arguments is a comma-separated string of platforms. A platform may
        be prefixed with "not " to indicate that a feature is not available.

        Example::

           .. availability:: Windows, Linux >= 4.2, not WASI

        Arguments like "Linux >= 3.17 with glibc >= 2.27" are currently not
        parsed into separate tokens.
        """
        platforms = {}
        for arg in self.arguments[0].rstrip(".").split(","):
            arg = arg.strip()
            platform, _, version = arg.partition(" >= ")
            if platform.startswith("not "):
                version = False
                platform = platform.removeprefix("not ")
            elif not version:
                version = True
            platforms[platform] = version

        if unknown := set(platforms).difference(KNOWN_PLATFORMS):
            logger.warning(
                "Unknown platform%s or syntax '%s' in '.. availability:: %s', "
                "see %s:KNOWN_PLATFORMS for a set of known platforms.",
                "s" if len(platforms) != 1 else "",
                " ".join(sorted(unknown)),
                self.arguments[0],
                __file__,
            )

        return platforms


def setup(app: Sphinx) -> ExtensionMetadata:
    app.add_directive("availability", Availability)

    return {
        "version": "1.0",
        "parallel_read_safe": True,
        "parallel_write_safe": True,
    }


================================================
File: /Doc/tools/extensions/c_annotations.py
================================================
"""Support annotations for C API elements.

* Reference count annotations for C API functions.
* Stable ABI annotations
* Limited API annotations

Configuration:
* Set ``refcount_file`` to the path to the reference count data file.
* Set ``stable_abi_file`` to the path to stable ABI list.
"""

from __future__ import annotations

import csv
import dataclasses
from pathlib import Path
from typing import TYPE_CHECKING

import sphinx
from docutils import nodes
from docutils.statemachine import StringList
from sphinx import addnodes
from sphinx.locale import _ as sphinx_gettext
from sphinx.util.docutils import SphinxDirective

if TYPE_CHECKING:
    from sphinx.application import Sphinx
    from sphinx.util.typing import ExtensionMetadata

ROLE_TO_OBJECT_TYPE = {
    "func": "function",
    "macro": "macro",
    "member": "member",
    "type": "type",
    "data": "var",
}


@dataclasses.dataclass(slots=True)
class RefCountEntry:
    # Name of the function.
    name: str
    # List of (argument name, type, refcount effect) tuples.
    # (Currently not used. If it was, a dataclass might work better.)
    args: list = dataclasses.field(default_factory=list)
    # Return type of the function.
    result_type: str = ""
    # Reference count effect for the return value.
    result_refs: int | None = None


@dataclasses.dataclass(frozen=True, slots=True)
class StableABIEntry:
    # Role of the object.
    # Source: Each [item_kind] in stable_abi.toml is mapped to a C Domain role.
    role: str
    # Name of the object.
    # Source: [<item_kind>.*] in stable_abi.toml.
    name: str
    # Version when the object was added to the stable ABI.
    # (Source: [<item_kind>.*.added] in stable_abi.toml.
    added: str
    # An explananatory blurb for the ifdef.
    # Source: ``feature_macro.*.doc`` in stable_abi.toml.
    ifdef_note: str
    # Defines how much of the struct is exposed. Only relevant for structs.
    # Source: [<item_kind>.*.struct_abi_kind] in stable_abi.toml.
    struct_abi_kind: str


def read_refcount_data(refcount_filename: Path) -> dict[str, RefCountEntry]:
    refcount_data = {}
    refcounts = refcount_filename.read_text(encoding="utf8")
    for line in refcounts.splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            # blank lines and comments
            continue

        # Each line is of the form
        # function ':' type ':' [param name] ':' [refcount effect] ':' [comment]
        parts = line.split(":", 4)
        if len(parts) != 5:
            raise ValueError(f"Wrong field count in {line!r}")
        function, type, arg, refcount, _comment = parts

        # Get the entry, creating it if needed:
        try:
            entry = refcount_data[function]
        except KeyError:
            entry = refcount_data[function] = RefCountEntry(function)
        if not refcount or refcount == "null":
            refcount = None
        else:
            refcount = int(refcount)
        # Update the entry with the new parameter
        # or the result information.
        if arg:
            entry.args.append((arg, type, refcount))
        else:
            entry.result_type = type
            entry.result_refs = refcount

    return refcount_data


def read_stable_abi_data(stable_abi_file: Path) -> dict[str, StableABIEntry]:
    stable_abi_data = {}
    with open(stable_abi_file, encoding="utf8") as fp:
        for record in csv.DictReader(fp):
            name = record["name"]
            stable_abi_data[name] = StableABIEntry(**record)

    return stable_abi_data


def add_annotations(app: Sphinx, doctree: nodes.document) -> None:
    state = app.env.domaindata["c_annotations"]
    refcount_data = state["refcount_data"]
    stable_abi_data = state["stable_abi_data"]
    for node in doctree.findall(addnodes.desc_content):
        par = node.parent
        if par["domain"] != "c":
            continue
        if not par[0].get("ids", None):
            continue
        name = par[0]["ids"][0].removeprefix("c.")
        objtype = par["objtype"]

        # Stable ABI annotation.
        if record := stable_abi_data.get(name):
            if ROLE_TO_OBJECT_TYPE[record.role] != objtype:
                msg = (
                    f"Object type mismatch in limited API annotation for {name}: "
                    f"{ROLE_TO_OBJECT_TYPE[record.role]!r} != {objtype!r}"
                )
                raise ValueError(msg)
            annotation = _stable_abi_annotation(record)
            node.insert(0, annotation)

        # Unstable API annotation.
        if name.startswith("PyUnstable"):
            annotation = _unstable_api_annotation()
            node.insert(0, annotation)

        # Return value annotation
        if objtype != "function":
            continue
        if name not in refcount_data:
            continue
        entry = refcount_data[name]
        if not entry.result_type.endswith("Object*"):
            continue
        annotation = _return_value_annotation(entry.result_refs)
        node.insert(0, annotation)


def _stable_abi_annotation(record: StableABIEntry) -> nodes.emphasis:
    """Create the Stable ABI annotation.

    These have two forms:
      Part of the `Stable ABI <link>`_.
      Part of the `Stable ABI <link>`_ since version X.Y.
    For structs, there's some more info in the message:
      Part of the `Limited API <link>`_ (as an opaque struct).
      Part of the `Stable ABI <link>`_ (including all members).
      Part of the `Limited API <link>`_ (Only some members are part
          of the stable ABI.).
    ... all of which can have "since version X.Y" appended.
    """
    stable_added = record.added
    message = sphinx_gettext("Part of the")
    message = message.center(len(message) + 2)
    emph_node = nodes.emphasis(message, message, classes=["stableabi"])
    ref_node = addnodes.pending_xref(
        "Stable ABI",
        refdomain="std",
        reftarget="stable",
        reftype="ref",
        refexplicit="False",
    )
    struct_abi_kind = record.struct_abi_kind
    if struct_abi_kind in {"opaque", "members"}:
        ref_node += nodes.Text(sphinx_gettext("Limited API"))
    else:
        ref_node += nodes.Text(sphinx_gettext("Stable ABI"))
    emph_node += ref_node
    if struct_abi_kind == "opaque":
        emph_node += nodes.Text(" " + sphinx_gettext("(as an opaque struct)"))
    elif struct_abi_kind == "full-abi":
        emph_node += nodes.Text(
            " " + sphinx_gettext("(including all members)")
        )
    if record.ifdef_note:
        emph_node += nodes.Text(f" {record.ifdef_note}")
    if stable_added == "3.2":
        # Stable ABI was introduced in 3.2.
        pass
    else:
        emph_node += nodes.Text(
            " " + sphinx_gettext("since version %s") % stable_added
        )
    emph_node += nodes.Text(".")
    if struct_abi_kind == "members":
        msg = " " + sphinx_gettext(
            "(Only some members are part of the stable ABI.)"
        )
        emph_node += nodes.Text(msg)
    return emph_node


def _unstable_api_annotation() -> nodes.admonition:
    ref_node = addnodes.pending_xref(
        "Unstable API",
        nodes.Text(sphinx_gettext("Unstable API")),
        refdomain="std",
        reftarget="unstable-c-api",
        reftype="ref",
        refexplicit="False",
    )
    emph_node = nodes.emphasis(
        "This is ",
        sphinx_gettext("This is") + " ",
        ref_node,
        nodes.Text(
            sphinx_gettext(
                ". It may change without warning in minor releases."
            )
        ),
    )
    return nodes.admonition(
        "",
        emph_node,
        classes=["unstable-c-api", "warning"],
    )


def _return_value_annotation(result_refs: int | None) -> nodes.emphasis:
    classes = ["refcount"]
    if result_refs is None:
        rc = sphinx_gettext("Return value: Always NULL.")
        classes.append("return_null")
    elif result_refs:
        rc = sphinx_gettext("Return value: New reference.")
        classes.append("return_new_ref")
    else:
        rc = sphinx_gettext("Return value: Borrowed reference.")
        classes.append("return_borrowed_ref")
    return nodes.emphasis(rc, rc, classes=classes)


class LimitedAPIList(SphinxDirective):
    has_content = False
    required_arguments = 0
    optional_arguments = 0
    final_argument_whitespace = True

    def run(self) -> list[nodes.Node]:
        state = self.env.domaindata["c_annotations"]
        content = [
            f"* :c:{record.role}:`{record.name}`"
            for record in state["stable_abi_data"].values()
        ]
        node = nodes.paragraph()
        self.state.nested_parse(StringList(content), 0, node)
        return [node]


def init_annotations(app: Sphinx) -> None:
    # Using domaindata is a bit hack-ish,
    # but allows storing state without a global variable or closure.
    app.env.domaindata["c_annotations"] = state = {}
    state["refcount_data"] = read_refcount_data(
        Path(app.srcdir, app.config.refcount_file)
    )
    state["stable_abi_data"] = read_stable_abi_data(
        Path(app.srcdir, app.config.stable_abi_file)
    )


def setup(app: Sphinx) -> ExtensionMetadata:
    app.add_config_value("refcount_file", "", "env", types={str})
    app.add_config_value("stable_abi_file", "", "env", types={str})
    app.add_directive("limited-api-list", LimitedAPIList)
    app.connect("builder-inited", init_annotations)
    app.connect("doctree-read", add_annotations)

    if sphinx.version_info[:2] < (7, 2):
        from docutils.parsers.rst import directives
        from sphinx.domains.c import CObject

        # monkey-patch C object...
        CObject.option_spec |= {
            "no-index-entry": directives.flag,
            "no-contents-entry": directives.flag,
        }

    return {
        "version": "1.0",
        "parallel_read_safe": True,
        "parallel_write_safe": True,
    }


================================================
File: /Doc/tools/extensions/glossary_search.py
================================================
"""Feature search results for glossary items prominently."""

from __future__ import annotations

import json
from pathlib import Path
from typing import TYPE_CHECKING

from docutils import nodes
from sphinx.addnodes import glossary
from sphinx.util import logging

if TYPE_CHECKING:
    from sphinx.application import Sphinx
    from sphinx.util.typing import ExtensionMetadata

logger = logging.getLogger(__name__)


def process_glossary_nodes(
    app: Sphinx,
    doctree: nodes.document,
    _docname: str,
) -> None:
    if app.builder.format != 'html' or app.builder.embedded:
        return

    if hasattr(app.env, 'glossary_terms'):
        terms = app.env.glossary_terms
    else:
        terms = app.env.glossary_terms = {}

    for node in doctree.findall(glossary):
        for glossary_item in node.findall(nodes.definition_list_item):
            term = glossary_item[0].astext()
            definition = glossary_item[-1]

            rendered = app.builder.render_partial(definition)
            terms[term.lower()] = {
                'title': term,
                'body': rendered['html_body'],
            }


def write_glossary_json(app: Sphinx, _exc: Exception) -> None:
    if not getattr(app.env, 'glossary_terms', None):
        return

    logger.info('Writing glossary.json', color='green')
    dest = Path(app.outdir, '_static', 'glossary.json')
    dest.parent.mkdir(exist_ok=True)
    dest.write_text(json.dumps(app.env.glossary_terms), encoding='utf-8')


def setup(app: Sphinx) -> ExtensionMetadata:
    app.connect('doctree-resolved', process_glossary_nodes)
    app.connect('build-finished', write_glossary_json)

    return {
        'version': '1.0',
        'parallel_read_safe': True,
        'parallel_write_safe': True,
    }


================================================
File: /Doc/tools/extensions/patchlevel.py
================================================
"""Extract version information from Include/patchlevel.h."""

import re
import sys
from pathlib import Path
from typing import Literal, NamedTuple

CPYTHON_ROOT = Path(
    __file__,  # cpython/Doc/tools/extensions/patchlevel.py
    "..",  # cpython/Doc/tools/extensions
    "..",  # cpython/Doc/tools
    "..",  # cpython/Doc
    "..",  # cpython
).resolve()
PATCHLEVEL_H = CPYTHON_ROOT / "Include" / "patchlevel.h"

RELEASE_LEVELS = {
    "PY_RELEASE_LEVEL_ALPHA": "alpha",
    "PY_RELEASE_LEVEL_BETA": "beta",
    "PY_RELEASE_LEVEL_GAMMA": "candidate",
    "PY_RELEASE_LEVEL_FINAL": "final",
}


class version_info(NamedTuple):  # noqa: N801
    major: int  #: Major release number
    minor: int  #: Minor release number
    micro: int  #: Patch release number
    releaselevel: Literal["alpha", "beta", "candidate", "final"]
    serial: int  #: Serial release number


def get_header_version_info() -> version_info:
    # Capture PY_ prefixed #defines.
    pat = re.compile(r"\s*#define\s+(PY_\w*)\s+(\w+)", re.ASCII)

    defines = {}
    patchlevel_h = PATCHLEVEL_H.read_text(encoding="utf-8")
    for line in patchlevel_h.splitlines():
        if (m := pat.match(line)) is not None:
            name, value = m.groups()
            defines[name] = value

    return version_info(
        major=int(defines["PY_MAJOR_VERSION"]),
        minor=int(defines["PY_MINOR_VERSION"]),
        micro=int(defines["PY_MICRO_VERSION"]),
        releaselevel=RELEASE_LEVELS[defines["PY_RELEASE_LEVEL"]],
        serial=int(defines["PY_RELEASE_SERIAL"]),
    )


def format_version_info(info: version_info) -> tuple[str, str]:
    version = f"{info.major}.{info.minor}"
    release = f"{info.major}.{info.minor}.{info.micro}"
    if info.releaselevel != "final":
        suffix = {"alpha": "a", "beta": "b", "candidate": "rc"}
        release += f"{suffix[info.releaselevel]}{info.serial}"
    return version, release


def get_version_info():
    try:
        info = get_header_version_info()
        return format_version_info(info)
    except OSError:
        version, release = format_version_info(sys.version_info)
        print(
            f"Failed to get version info from Include/patchlevel.h, "
            f"using version of this interpreter ({release}).",
            file=sys.stderr,
        )
        return version, release


if __name__ == "__main__":
    short_ver, full_ver = format_version_info(get_header_version_info())
    if sys.argv[1:2] == ["--short"]:
        print(short_ver)
    else:
        print(full_ver)


================================================
File: /Doc/tools/extensions/pyspecific.py
================================================
# -*- coding: utf-8 -*-
"""
    pyspecific.py
    ~~~~~~~~~~~~~

    Sphinx extension with Python doc-specific markup.

    :copyright: 2008-2014 by Georg Brandl.
    :license: Python license.
"""

import re
import io
from os import getenv, path
from time import asctime
from pprint import pformat

from docutils import nodes
from docutils.io import StringOutput
from docutils.parsers.rst import directives
from docutils.utils import new_document, unescape
from sphinx import addnodes
from sphinx.builders import Builder
from sphinx.domains.changeset import VersionChange, versionlabels, versionlabel_classes
from sphinx.domains.python import PyFunction, PyMethod, PyModule
from sphinx.locale import _ as sphinx_gettext
from sphinx.util.docutils import SphinxDirective
from sphinx.writers.text import TextWriter, TextTranslator
from sphinx.util.display import status_iterator


ISSUE_URI = 'https://bugs.python.org/issue?@action=redirect&bpo=%s'
GH_ISSUE_URI = 'https://github.com/python/cpython/issues/%s'
# Used in conf.py and updated here by python/release-tools/run_release.py
SOURCE_URI = 'https://github.com/python/cpython/tree/main/%s'

# monkey-patch reST parser to disable alphabetic and roman enumerated lists
from docutils.parsers.rst.states import Body
Body.enum.converters['loweralpha'] = \
    Body.enum.converters['upperalpha'] = \
    Body.enum.converters['lowerroman'] = \
    Body.enum.converters['upperroman'] = lambda x: None

# monkey-patch the productionlist directive to allow hyphens in group names
# https://github.com/sphinx-doc/sphinx/issues/11854
from sphinx.domains import std

std.token_re = re.compile(r'`((~?[\w-]*:)?\w+)`')

# backport :no-index:
PyModule.option_spec['no-index'] = directives.flag


# Support for marking up and linking to bugs.python.org issues

def issue_role(typ, rawtext, text, lineno, inliner, options={}, content=[]):
    issue = unescape(text)
    # sanity check: there are no bpo issues within these two values
    if 47261 < int(issue) < 400000:
        msg = inliner.reporter.error(f'The BPO ID {text!r} seems too high -- '
                                     'use :gh:`...` for GitHub IDs', line=lineno)
        prb = inliner.problematic(rawtext, rawtext, msg)
        return [prb], [msg]
    text = 'bpo-' + issue
    refnode = nodes.reference(text, text, refuri=ISSUE_URI % issue)
    return [refnode], []


# Support for marking up and linking to GitHub issues

def gh_issue_role(typ, rawtext, text, lineno, inliner, options={}, content=[]):
    issue = unescape(text)
    # sanity check: all GitHub issues have ID >= 32426
    # even though some of them are also valid BPO IDs
    if int(issue) < 32426:
        msg = inliner.reporter.error(f'The GitHub ID {text!r} seems too low -- '
                                     'use :issue:`...` for BPO IDs', line=lineno)
        prb = inliner.problematic(rawtext, rawtext, msg)
        return [prb], [msg]
    text = 'gh-' + issue
    refnode = nodes.reference(text, text, refuri=GH_ISSUE_URI % issue)
    return [refnode], []


# Support for marking up implementation details

class ImplementationDetail(SphinxDirective):

    has_content = True
    final_argument_whitespace = True

    # This text is copied to templates/dummy.html
    label_text = sphinx_gettext('CPython implementation detail:')

    def run(self):
        self.assert_has_content()
        pnode = nodes.compound(classes=['impl-detail'])
        content = self.content
        add_text = nodes.strong(self.label_text, self.label_text)
        self.state.nested_parse(content, self.content_offset, pnode)
        content = nodes.inline(pnode[0].rawsource, translatable=True)
        content.source = pnode[0].source
        content.line = pnode[0].line
        content += pnode[0].children
        pnode[0].replace_self(nodes.paragraph(
            '', '', add_text, nodes.Text(' '), content, translatable=False))
        return [pnode]


# Support for documenting decorators

class PyDecoratorMixin(object):
    def handle_signature(self, sig, signode):
        ret = super(PyDecoratorMixin, self).handle_signature(sig, signode)
        signode.insert(0, addnodes.desc_addname('@', '@'))
        return ret

    def needs_arglist(self):
        return False


class PyDecoratorFunction(PyDecoratorMixin, PyFunction):
    def run(self):
        # a decorator function is a function after all
        self.name = 'py:function'
        return PyFunction.run(self)


# TODO: Use sphinx.domains.python.PyDecoratorMethod when possible
class PyDecoratorMethod(PyDecoratorMixin, PyMethod):
    def run(self):
        self.name = 'py:method'
        return PyMethod.run(self)


class PyCoroutineMixin(object):
    def handle_signature(self, sig, signode):
        ret = super(PyCoroutineMixin, self).handle_signature(sig, signode)
        signode.insert(0, addnodes.desc_annotation('coroutine ', 'coroutine '))
        return ret


class PyAwaitableMixin(object):
    def handle_signature(self, sig, signode):
        ret = super(PyAwaitableMixin, self).handle_signature(sig, signode)
        signode.insert(0, addnodes.desc_annotation('awaitable ', 'awaitable '))
        return ret


class PyCoroutineFunction(PyCoroutineMixin, PyFunction):
    def run(self):
        self.name = 'py:function'
        return PyFunction.run(self)


class PyCoroutineMethod(PyCoroutineMixin, PyMethod):
    def run(self):
        self.name = 'py:method'
        return PyMethod.run(self)


class PyAwaitableFunction(PyAwaitableMixin, PyFunction):
    def run(self):
        self.name = 'py:function'
        return PyFunction.run(self)


class PyAwaitableMethod(PyAwaitableMixin, PyMethod):
    def run(self):
        self.name = 'py:method'
        return PyMethod.run(self)


class PyAbstractMethod(PyMethod):

    def handle_signature(self, sig, signode):
        ret = super(PyAbstractMethod, self).handle_signature(sig, signode)
        signode.insert(0, addnodes.desc_annotation('abstractmethod ',
                                                   'abstractmethod '))
        return ret

    def run(self):
        self.name = 'py:method'
        return PyMethod.run(self)


# Support for documenting version of changes, additions, deprecations

def expand_version_arg(argument, release):
    """Expand "next" to the current version"""
    if argument == 'next':
        return sphinx_gettext('{} (unreleased)').format(release)
    return argument


class PyVersionChange(VersionChange):
    def run(self):
        # Replace the 'next' special token with the current development version
        self.arguments[0] = expand_version_arg(self.arguments[0],
                                               self.config.release)
        return super().run()


class DeprecatedRemoved(VersionChange):
    required_arguments = 2

    _deprecated_label = sphinx_gettext('Deprecated since version %s, will be removed in version %s')
    _removed_label = sphinx_gettext('Deprecated since version %s, removed in version %s')

    def run(self):
        # Replace the first two arguments (deprecated version and removed version)
        # with a single tuple of both versions.
        version_deprecated = expand_version_arg(self.arguments[0],
                                                self.config.release)
        version_removed = self.arguments.pop(1)
        if version_removed == 'next':
            raise ValueError(
                'deprecated-removed:: second argument cannot be `next`')
        self.arguments[0] = version_deprecated, version_removed

        # Set the label based on if we have reached the removal version
        current_version = tuple(map(int, self.config.version.split('.')))
        removed_version = tuple(map(int,  version_removed.split('.')))
        if current_version < removed_version:
            versionlabels[self.name] = self._deprecated_label
            versionlabel_classes[self.name] = 'deprecated'
        else:
            versionlabels[self.name] = self._removed_label
            versionlabel_classes[self.name] = 'removed'
        try:
            return super().run()
        finally:
            # reset versionlabels and versionlabel_classes
            versionlabels[self.name] = ''
            versionlabel_classes[self.name] = ''


# Support for including Misc/NEWS

issue_re = re.compile('(?:[Ii]ssue #|bpo-)([0-9]+)', re.I)
gh_issue_re = re.compile('(?:gh-issue-|gh-)([0-9]+)', re.I)
whatsnew_re = re.compile(r"(?im)^what's new in (.*?)\??$")


class MiscNews(SphinxDirective):
    has_content = False
    required_arguments = 1
    optional_arguments = 0
    final_argument_whitespace = False
    option_spec = {}

    def run(self):
        fname = self.arguments[0]
        source = self.state_machine.input_lines.source(
            self.lineno - self.state_machine.input_offset - 1)
        source_dir = getenv('PY_MISC_NEWS_DIR')
        if not source_dir:
            source_dir = path.dirname(path.abspath(source))
        fpath = path.join(source_dir, fname)
        self.env.note_dependency(path.abspath(fpath))
        try:
            with io.open(fpath, encoding='utf-8') as fp:
                content = fp.read()
        except Exception:
            text = 'The NEWS file is not available.'
            node = nodes.strong(text, text)
            return [node]
        content = issue_re.sub(r':issue:`\1`', content)
        # Fallback handling for the GitHub issue
        content = gh_issue_re.sub(r':gh:`\1`', content)
        content = whatsnew_re.sub(r'\1', content)
        # remove first 3 lines as they are the main heading
        lines = ['.. default-role:: obj', ''] + content.splitlines()[3:]
        self.state_machine.insert_input(lines, fname)
        return []


# Support for building "topic help" for pydoc

pydoc_topic_labels = [
    'assert', 'assignment', 'assignment-expressions', 'async',  'atom-identifiers',
    'atom-literals', 'attribute-access', 'attribute-references', 'augassign', 'await',
    'binary', 'bitwise', 'bltin-code-objects', 'bltin-ellipsis-object',
    'bltin-null-object', 'bltin-type-objects', 'booleans',
    'break', 'callable-types', 'calls', 'class', 'comparisons', 'compound',
    'context-managers', 'continue', 'conversions', 'customization', 'debugger',
    'del', 'dict', 'dynamic-features', 'else', 'exceptions', 'execmodel',
    'exprlists', 'floating', 'for', 'formatstrings', 'function', 'global',
    'id-classes', 'identifiers', 'if', 'imaginary', 'import', 'in', 'integers',
    'lambda', 'lists', 'naming', 'nonlocal', 'numbers', 'numeric-types',
    'objects', 'operator-summary', 'pass', 'power', 'raise', 'return',
    'sequence-types', 'shifting', 'slicings', 'specialattrs', 'specialnames',
    'string-methods', 'strings', 'subscriptions', 'truth', 'try', 'types',
    'typesfunctions', 'typesmapping', 'typesmethods', 'typesmodules',
    'typesseq', 'typesseq-mutable', 'unary', 'while', 'with', 'yield'
]


class PydocTopicsBuilder(Builder):
    name = 'pydoc-topics'

    default_translator_class = TextTranslator

    def init(self):
        self.topics = {}
        self.secnumbers = {}

    def get_outdated_docs(self):
        return 'all pydoc topics'

    def get_target_uri(self, docname, typ=None):
        return ''  # no URIs

    def write(self, *ignored):
        writer = TextWriter(self)
        for label in status_iterator(pydoc_topic_labels,
                                     'building topics... ',
                                     length=len(pydoc_topic_labels)):
            if label not in self.env.domaindata['std']['labels']:
                self.env.logger.warning(f'label {label!r} not in documentation')
                continue
            docname, labelid, sectname = self.env.domaindata['std']['labels'][label]
            doctree = self.env.get_and_resolve_doctree(docname, self)
            document = new_document('<section node>')
            document.append(doctree.ids[labelid])
            destination = StringOutput(encoding='utf-8')
            writer.write(document, destination)
            self.topics[label] = writer.output

    def finish(self):
        f = open(path.join(self.outdir, 'topics.py'), 'wb')
        try:
            f.write('# -*- coding: utf-8 -*-\n'.encode('utf-8'))
            f.write(('# Autogenerated by Sphinx on %s\n' % asctime()).encode('utf-8'))
            f.write('# as part of the release process.\n'.encode('utf-8'))
            f.write(('topics = ' + pformat(self.topics) + '\n').encode('utf-8'))
        finally:
            f.close()


# Support for documenting Opcodes

opcode_sig_re = re.compile(r'(\w+(?:\+\d)?)(?:\s*\((.*)\))?')


def parse_opcode_signature(env, sig, signode):
    """Transform an opcode signature into RST nodes."""
    m = opcode_sig_re.match(sig)
    if m is None:
        raise ValueError
    opname, arglist = m.groups()
    signode += addnodes.desc_name(opname, opname)
    if arglist is not None:
        paramlist = addnodes.desc_parameterlist()
        signode += paramlist
        paramlist += addnodes.desc_parameter(arglist, arglist)
    return opname.strip()


# Support for documenting pdb commands

pdbcmd_sig_re = re.compile(r'([a-z()!]+)\s*(.*)')

# later...
# pdbargs_tokens_re = re.compile(r'''[a-zA-Z]+  |  # identifiers
#                                   [.,:]+     |  # punctuation
#                                   [\[\]()]   |  # parens
#                                   \s+           # whitespace
#                                   ''', re.X)


def parse_pdb_command(env, sig, signode):
    """Transform a pdb command signature into RST nodes."""
    m = pdbcmd_sig_re.match(sig)
    if m is None:
        raise ValueError
    name, args = m.groups()
    fullname = name.replace('(', '').replace(')', '')
    signode += addnodes.desc_name(name, name)
    if args:
        signode += addnodes.desc_addname(' '+args, ' '+args)
    return fullname


def parse_monitoring_event(env, sig, signode):
    """Transform a monitoring event signature into RST nodes."""
    signode += addnodes.desc_addname('sys.monitoring.events.', 'sys.monitoring.events.')
    signode += addnodes.desc_name(sig, sig)
    return sig


def patch_pairindextypes(app, _env) -> None:
    """Remove all entries from ``pairindextypes`` before writing POT files.

    We want to run this just before writing output files, as the check to
    circumvent is in ``I18nBuilder.write_doc()``.
    As such, we link this to ``env-check-consistency``, even though it has
    nothing to do with the environment consistency check.
    """
    if app.builder.name != 'gettext':
        return

    # allow translating deprecated index entries
    try:
        from sphinx.domains.python import pairindextypes
    except ImportError:
        pass
    else:
        # Sphinx checks if a 'pair' type entry on an index directive is one of
        # the Sphinx-translated pairindextypes values. As we intend to move
        # away from this, we need Sphinx to believe that these values don't
        # exist, by deleting them when using the gettext builder.
        pairindextypes.clear()


def setup(app):
    app.add_role('issue', issue_role)
    app.add_role('gh', gh_issue_role)
    app.add_directive('impl-detail', ImplementationDetail)
    app.add_directive('versionadded', PyVersionChange, override=True)
    app.add_directive('versionchanged', PyVersionChange, override=True)
    app.add_directive('versionremoved', PyVersionChange, override=True)
    app.add_directive('deprecated', PyVersionChange, override=True)
    app.add_directive('deprecated-removed', DeprecatedRemoved)
    app.add_builder(PydocTopicsBuilder)
    app.add_object_type('opcode', 'opcode', '%s (opcode)', parse_opcode_signature)
    app.add_object_type('pdbcommand', 'pdbcmd', '%s (pdb command)', parse_pdb_command)
    app.add_object_type('monitoring-event', 'monitoring-event', '%s (monitoring event)', parse_monitoring_event)
    app.add_directive_to_domain('py', 'decorator', PyDecoratorFunction)
    app.add_directive_to_domain('py', 'decoratormethod', PyDecoratorMethod)
    app.add_directive_to_domain('py', 'coroutinefunction', PyCoroutineFunction)
    app.add_directive_to_domain('py', 'coroutinemethod', PyCoroutineMethod)
    app.add_directive_to_domain('py', 'awaitablefunction', PyAwaitableFunction)
    app.add_directive_to_domain('py', 'awaitablemethod', PyAwaitableMethod)
    app.add_directive_to_domain('py', 'abstractmethod', PyAbstractMethod)
    app.add_directive('miscnews', MiscNews)
    app.connect('env-check-consistency', patch_pairindextypes)
    return {'version': '1.0', 'parallel_read_safe': True}


================================================
File: /Doc/tools/extensions/lexers/__init__.py
================================================
from .asdl_lexer import ASDLLexer
from .peg_lexer import PEGLexer


def setup(app):
    # Used for highlighting Parser/Python.asdl in library/ast.rst
    app.add_lexer("asdl", ASDLLexer)
    # Used for highlighting Grammar/python.gram in reference/grammar.rst
    app.add_lexer("peg", PEGLexer)

    return {
        "version": "1.0",
        "parallel_read_safe": True,
        "parallel_write_safe": True,
    }


================================================
File: /Doc/tools/extensions/lexers/asdl_lexer.py
================================================
from pygments.lexer import RegexLexer, bygroups, include
from pygments.token import Comment, Keyword, Name, Operator, Punctuation, Text


class ASDLLexer(RegexLexer):
    name = "ASDL"
    aliases = ["asdl"]
    filenames = ["*.asdl"]
    _name = r"([^\W\d]\w*)"
    _text_ws = r"(\s*)"

    tokens = {
        "ws": [
            (r"\n", Text),
            (r"\s+", Text),
            (r"--.*?$", Comment.Singleline),
        ],
        "root": [
            include("ws"),
            (
                r"(module)" + _text_ws + _name,
                bygroups(Keyword, Text, Name.Tag),
            ),
            (
                r"(\w+)(\*\s|\?\s|\s)(\w+)",
                bygroups(Name.Builtin.Pseudo, Operator, Name),
            ),
            # Keep in line with ``builtin_types`` from Parser/asdl.py.
            # ASDL's 4 builtin types are
            # constant, identifier, int, string
            ("constant|identifier|int|string", Name.Builtin),
            (r"attributes", Name.Builtin),
            (
                _name + _text_ws + "(=)",
                bygroups(Name, Text, Operator),
            ),
            (_name, Name.Class),
            (r"\|", Operator),
            (r"{|}|\(|\)", Punctuation),
            (r".", Text),
        ],
    }


================================================
File: /Doc/tools/extensions/lexers/peg_lexer.py
================================================
from pygments.lexer import RegexLexer, bygroups, include
from pygments.token import Comment, Keyword, Name, Operator, Punctuation, Text


class PEGLexer(RegexLexer):
    """Pygments Lexer for PEG grammar (.gram) files

    This lexer strips the following elements from the grammar:

        - Meta-tags
        - Variable assignments
        - Actions
        - Lookaheads
        - Rule types
        - Rule options
        - Rules named `invalid_*` or `incorrect_*`
    """

    name = "PEG"
    aliases = ["peg"]
    filenames = ["*.gram"]
    _name = r"([^\W\d]\w*)"
    _text_ws = r"(\s*)"

    tokens = {
        "ws": [(r"\n", Text), (r"\s+", Text), (r"#.*$", Comment.Singleline),],
        "lookaheads": [
            # Forced tokens
            (r"(&&)(?=\w+\s?)", bygroups(None)),
            (r"(&&)(?='.+'\s?)", bygroups(None)),
            (r'(&&)(?=".+"\s?)', bygroups(None)),
            (r"(&&)(?=\(.+\)\s?)", bygroups(None)),

            (r"(?<=\|\s)(&\w+\s?)", bygroups(None)),
            (r"(?<=\|\s)(&'.+'\s?)", bygroups(None)),
            (r'(?<=\|\s)(&".+"\s?)', bygroups(None)),
            (r"(?<=\|\s)(&\(.+\)\s?)", bygroups(None)),
        ],
        "metas": [
            (r"(@\w+ '''(.|\n)+?''')", bygroups(None)),
            (r"^(@.*)$", bygroups(None)),
        ],
        "actions": [
            (r"{(.|\n)+?}", bygroups(None)),
        ],
        "strings": [
            (r"'\w+?'", Keyword),
            (r'"\w+?"', Keyword),
            (r"'\W+?'", Text),
            (r'"\W+?"', Text),
        ],
        "variables": [
            (_name + _text_ws + "(=)", bygroups(None, None, None),),
            (_name + _text_ws + r"(\[[\w\d_\*]+?\])" + _text_ws + "(=)", bygroups(None, None, None, None, None),),
        ],
        "invalids": [
            (r"^(\s+\|\s+.*invalid_\w+.*\n)", bygroups(None)),
            (r"^(\s+\|\s+.*incorrect_\w+.*\n)", bygroups(None)),
            (r"^(#.*invalid syntax.*(?:.|\n)*)", bygroups(None),),
        ],
        "root": [
            include("invalids"),
            include("ws"),
            include("lookaheads"),
            include("metas"),
            include("actions"),
            include("strings"),
            include("variables"),
            (r"\b(?!(NULL|EXTRA))([A-Z_]+)\b\s*(?!\()", Text,),
            (
                r"^\s*" + _name + r"\s*" + r"(\[.*\])?" + r"\s*" + r"(\(.+\))?" + r"\s*(:)",
                bygroups(Name.Function, None, None, Punctuation),
            ),
            (_name, Name.Function),
            (r"[\||\.|\+|\*|\?]", Operator),
            (r"{|}|\(|\)|\[|\]", Punctuation),
            (r".", Text),
        ],
    }


================================================
File: /Doc/tools/static/changelog_search.js
================================================
document.addEventListener("DOMContentLoaded", function () {
  // add the search form and bind the events
  document
    .querySelector("h1")
    .insertAdjacentHTML(
      "afterend",
      [
        "<p>Filter entries by content:",
        '<input type="text" value="" id="searchbox" style="width: 50%">',
        '<input type="submit" id="searchbox-submit" value="Filter"></p>',
      ].join("\n"),
    );

  function doFilter() {
    let query;
    try {
      query = new RegExp(document.querySelector("#searchbox").value, "i");
    } catch (e) {
      return; // not a valid regex (yet)
    }
    // find headers for the versions (What's new in Python X.Y.Z?)
    const h2s = document.querySelectorAll("#changelog h2");
    for (const h2 of h2s) {
      let sections_found = 0;
      // find headers for the sections (Core, Library, etc.)
      const h3s = h2.parentNode.querySelectorAll("h3");
      for (const h3 of h3s) {
        let entries_found = 0;
        // find all the entries
        const lis = h3.parentNode.querySelectorAll("li");
        for (let li of lis) {
          // check if the query matches the entry
          if (query.test(li.textContent)) {
            li.style.display = "block";
            entries_found++;
          } else {
            li.style.display = "none";
          }
        }
        // if there are entries, show the section, otherwise hide it
        if (entries_found > 0) {
          h3.parentNode.style.display = "block";
          sections_found++;
        } else {
          h3.parentNode.style.display = "none";
        }
      }
      if (sections_found > 0) {
        h2.parentNode.style.display = "block";
      } else {
        h2.parentNode.style.display = "none";
      }
    }
  }
  document.querySelector("#searchbox").addEventListener("keyup", doFilter);
  document
    .querySelector("#searchbox-submit")
    .addEventListener("click", doFilter);
});


================================================
File: /Doc/tools/static/glossary_search.js
================================================
"use strict";

const GLOSSARY_PAGE = "glossary.html";

const glossary_search = async () => {
  const response = await fetch("_static/glossary.json");
  if (!response.ok) {
    throw new Error("Failed to fetch glossary.json");
  }
  const glossary = await response.json();

  const params = new URLSearchParams(document.location.search).get("q");
  if (!params) {
    return;
  }

  const searchParam = params.toLowerCase();
  const glossaryItem = glossary[searchParam];
  if (!glossaryItem) {
    return;
  }

  // set up the title text with a link to the glossary page
  const glossaryTitle = document.getElementById("glossary-title");
  glossaryTitle.textContent = "Glossary: " + glossaryItem.title;
  const linkTarget = searchParam.replace(/ /g, "-");
  glossaryTitle.href = GLOSSARY_PAGE + "#term-" + linkTarget;

  // rewrite any anchor links (to other glossary terms)
  // to have a full reference to the glossary page
  const glossaryBody = document.getElementById("glossary-body");
  glossaryBody.innerHTML = glossaryItem.body;
  const anchorLinks = glossaryBody.querySelectorAll('a[href^="#"]');
  anchorLinks.forEach(function (link) {
    const currentUrl = link.getAttribute("href");
    link.href = GLOSSARY_PAGE + currentUrl;
  });

  const glossaryResult = document.getElementById("glossary-result");
  glossaryResult.style.display = "";
};

if (document.readyState !== "loading") {
  glossary_search().catch(console.error);
} else {
  document.addEventListener("DOMContentLoaded", glossary_search);
}


================================================
File: /Doc/tools/static/rtd_switcher.js
================================================
 function onSwitch(event) {
     const option = event.target.selectedIndex;
     const item = event.target.options[option];
     window.location.href = item.dataset.url;
 }

 document.addEventListener("readthedocs-addons-data-ready", function(event) {
   const config = event.detail.data()
   const versionSelect = `
   <select id="version_select" aria-label="Python version">
   ${ config.versions.active.map(
       (version) => `
       <option
           value="${ version.slug }"
           ${ config.versions.current.slug === version.slug ? 'selected="selected"' : '' }
           data-url="${ version.urls.documentation }">
           ${ version.slug }
       </option>`
   ).join("\n") }
   </select>
   `;

   // Prepend the current language to the options on the selector
   let languages = config.projects.translations.concat(config.projects.current);
   languages = languages.sort((a, b) => a.language.name.localeCompare(b.language.name));

   const languageSelect = `
   <select id="language_select" aria-label="Language">
   ${ languages.map(
       (translation) => `
       <option
           value="${ translation.slug }"
           ${ config.projects.current.slug === translation.slug ? 'selected="selected"' : '' }
           data-url="${ translation.urls.documentation }">
           ${ translation.language.name }
       </option>`
   ).join("\n") }
   </select>
   `;

   // Query all the placeholders because there are different ones for Desktop/Mobile
   const versionPlaceholders = document.querySelectorAll(".version_switcher_placeholder");
   for (placeholder of versionPlaceholders) {
       placeholder.innerHTML = versionSelect;
       let selectElement = placeholder.querySelector("select");
       selectElement.addEventListener("change", onSwitch);
   }

   const languagePlaceholders = document.querySelectorAll(".language_switcher_placeholder");
   for (placeholder of languagePlaceholders) {
       placeholder.innerHTML = languageSelect;
       let selectElement = placeholder.querySelector("select");
       selectElement.addEventListener("change", onSwitch);
   }
 });


================================================
File: /Doc/tools/templates/customsourcelink.html
================================================
{%- if show_source and has_source and sourcename %}
  <div role="note" aria-label="source link">
    <h3>{{ _('This Page') }}</h3>
    <ul class="this-page-menu">
      <li><a href="{{ pathto('bugs') }}">{% trans %}Report a Bug{% endtrans %}</a></li>
      <li>
        <a href="https://github.com/python/cpython/blob/main/Doc/{{ sourcename|replace('.rst.txt', '.rst') }}"
            rel="nofollow">{{ _('Show Source') }}
        </a>
      </li>
    </ul>
  </div>
{%- endif %}


================================================
File: /Doc/tools/templates/download.html
================================================
{% extends "layout.html" %}
{% set title = _('Download') %}
{% if daily is defined %}
  {% set dl_base = pathto('archives', resource=True) %}
  {% set dl_version = version %}
{% else %}
  {#
    The link below returns HTTP 404 until the first related alpha release.
    This is expected; use daily documentation builds for CPython development.
  #}
  {% set dl_base = 'https://www.python.org/ftp/python/doc/' + release %}
  {% set dl_version = release %}
{% endif %}

{% block body %}
<h1>{% trans %}Download Python {{ dl_version }} Documentation{% endtrans %}</h1>

{% if last_updated %}<p><b>{% trans %}Last updated on: {{ last_updated }}.{% endtrans %}</b></p>{% endif %}

<p>{% trans %}To download an archive containing all the documents for this version of
Python in one of various formats, follow one of links in this table.{% endtrans %}</p>

<table class="docutils">
  <tr>
    <th>{% trans %}Format{% endtrans %}</th>
    <th>{% trans %}Packed as .zip{% endtrans %}</th>
    <th>{% trans %}Packed as .tar.bz2{% endtrans %}</th>
  </tr>
  <tr>
    <td>{% trans %}PDF{% endtrans %}</td>
    <td>{% trans download_size="17" %}<a href="{{ dl_base }}/python-{{ dl_version }}-docs-pdf-a4.zip">Download</a> (ca. {{ download_size }} MiB){% endtrans %}</td>
    <td>{% trans download_size="17" %}<a href="{{ dl_base }}/python-{{ dl_version }}-docs-pdf-a4.tar.bz2">Download</a> (ca. {{ download_size }} MiB){% endtrans %}</td>
  </tr>
  <tr>
    <td>{% trans %}HTML{% endtrans %}</td>
    <td>{% trans download_size="13" %}<a href="{{ dl_base }}/python-{{ dl_version }}-docs-html.zip">Download</a> (ca. {{ download_size }} MiB){% endtrans %}</td>
    <td>{% trans download_size="8" %}<a href="{{ dl_base }}/python-{{ dl_version }}-docs-html.tar.bz2">Download</a> (ca. {{ download_size }} MiB){% endtrans %}</td>
  </tr>
  <tr>
    <td>{% trans %}Plain text{% endtrans %}</td>
    <td>{% trans download_size="4" %}<a href="{{ dl_base }}/python-{{ dl_version }}-docs-text.zip">Download</a> (ca. {{ download_size }} MiB){% endtrans %}</td>
    <td>{% trans download_size="3" %}<a href="{{ dl_base }}/python-{{ dl_version }}-docs-text.tar.bz2">Download</a> (ca. {{ download_size }} MiB){% endtrans %}</td>
  </tr>
  <tr>
    <td>{% trans %}Texinfo{% endtrans %}</td>
    <td>{% trans download_size="9" %}<a href="{{ dl_base }}/python-{{ dl_version }}-docs-texinfo.zip">Download</a> (ca. {{ download_size }} MiB){% endtrans %}</td>
    <td>{% trans download_size="7" %}<a href="{{ dl_base }}/python-{{ dl_version }}-docs-texinfo.tar.bz2">Download</a> (ca. {{ download_size }} MiB){% endtrans %}</td>
 </tr>
 <tr>
    <td>{% trans %}EPUB{% endtrans %}</td>
    <td>{% trans download_size="6" %}<a href="{{ dl_base }}/python-{{ dl_version }}-docs.epub">Download</a> (ca. {{ download_size }} MiB){% endtrans %}</td>
    <td></td>
  </tr>
</table>

<p>{% trans %}These archives contain all the content in the documentation.{% endtrans %}</p>


<h2>{% trans %}Unpacking{% endtrans %}</h2>

<p>{% trans %}Unix users should download the .tar.bz2 archives; these are bzipped tar
archives and can be handled in the usual way using tar and the bzip2
program. The <a href="https://infozip.sourceforge.net">Info-ZIP</a> unzip program can be
used to handle the ZIP archives if desired. The .tar.bz2 archives provide the
best compression and fastest download times.{% endtrans %}</p>

<p>{% trans %}Windows users can use the ZIP archives since those are customary on that
platform. These are created on Unix using the Info-ZIP zip program.{% endtrans %}</p>


<h2>{% trans %}Problems{% endtrans %}</h2>

<p>{% trans %}If you have comments or suggestions for the Python documentation, please send
email to <a href="mailto:docs@python.org">docs@python.org</a>.{% endtrans %}</p>
{% endblock %}


================================================
File: /Doc/tools/templates/dummy.html
================================================
This file is not an actual template, but used to add some
texts in extensions to sphinx.pot file.

In extensions/pyspecific.py:

{% trans %}CPython implementation detail:{% endtrans %}
{% trans %}Deprecated since version {deprecated}, will be removed in version {removed}{% endtrans %}
{% trans %}Deprecated since version {deprecated}, removed in version {removed}{% endtrans %}

In extensions/c_annotations.py:

{% trans %}Part of the{% endtrans %}
{% trans %}Limited API{% endtrans %}
{% trans %}Stable ABI{% endtrans %}
{% trans %}(as an opaque struct){% endtrans %}
{% trans %}(including all members){% endtrans %}
{% trans %}since version %s{% endtrans %}
{% trans %}(Only some members are part of the stable ABI.){% endtrans %}
{% trans %}This is{% endtrans %}
{% trans %}Unstable API{% endtrans %}
{% trans %}. It may change without warning in minor releases.{% endtrans %}
{% trans %}Return value: Always NULL.{% endtrans %}
{% trans %}Return value: New reference.{% endtrans %}
{% trans %}Return value: Borrowed reference.{% endtrans %}

In docsbuild-scripts, when rewriting indexsidebar.html with actual versions:

{% trans %}in development{% endtrans %}
{% trans %}pre-release{% endtrans %}
{% trans %}stable{% endtrans %}
{% trans %}security-fixes{% endtrans %}
{% trans %}EOL{% endtrans %}


================================================
File: /Doc/tools/templates/indexcontent.html
================================================
{% extends "layout.html" %}
{%- block htmltitle -%}
<title>{{ shorttitle }}</title>
{%- endblock -%}
{% block body %}
  <h1>{{ docstitle|e }}</h1>
  <p>
  {% trans %}Welcome! This is the official documentation for Python {{ release }}.{% endtrans %}
  </p>
  <p><strong>{% trans %}Documentation sections:{% endtrans %}</strong></p>
  <table class="contentstable" align="center"><tr>
    <td width="50%">
      <p class="biglink"><a class="biglink" href="{{ pathto("whatsnew/" + version) }}">{% trans %}What's new in Python {{ version }}?{% endtrans %}</a><br/>
         <span class="linkdescr"> {% trans whatsnew_index=pathto("whatsnew/index") %}Or <a href="{{ whatsnew_index }}">all "What's new" documents since Python 2.0</a>{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("tutorial/index") }}">{% trans %}Tutorial{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}Start here: a tour of Python's syntax and features{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("library/index") }}">{% trans %}Library reference{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}Standard library and builtins{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("reference/index") }}">{% trans %}Language reference{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}Syntax and language elements{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("using/index") }}">{% trans %}Python setup and usage{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}How to install, configure, and use Python{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("howto/index") }}">{% trans %}Python HOWTOs{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}In-depth topic manuals{% endtrans %}</span></p>
    </td><td width="50%">
      <p class="biglink"><a class="biglink" href="{{ pathto("installing/index") }}">{% trans %}Installing Python modules{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}Third-party modules and PyPI.org{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("distributing/index") }}">{% trans %}Distributing Python modules{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}Publishing modules for use by other people{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("extending/index") }}">{% trans %}Extending and embedding{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}For C/C++ programmers{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("c-api/index") }}">{% trans %}Python's C API{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}C API reference{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("faq/index") }}">{% trans %}FAQs{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}Frequently asked questions (with answers!){% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("deprecations/index") }}">{% trans %}Deprecations{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}Deprecated functionality{% endtrans %}</span></p>
    </td></tr>
  </table>

  <p><strong>{% trans %}Indices, glossary, and search:{% endtrans %}</strong></p>
  <table class="contentstable" align="center"><tr>
    <td width="50%">
      <p class="biglink"><a class="biglink" href="{{ pathto("py-modindex") }}">{% trans %}Global module index{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}All modules and libraries{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("genindex") }}">{% trans %}General index{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}All functions, classes, and terms{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("glossary") }}">{% trans %}Glossary{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}Terms explained{% endtrans %}</span></p>
    </td><td width="50%">
      <p class="biglink"><a class="biglink" href="{{ pathto("search") }}">{% trans %}Search page{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}Search this documentation{% endtrans %}</span></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("contents") }}">{% trans %}Complete table of contents{% endtrans %}</a><br/>
         <span class="linkdescr">{% trans %}Lists all sections and subsections{% endtrans %}</span></p>
    </td></tr>
  </table>

  <p><strong>{% trans %}Project information:{% endtrans %}</strong></p>
  <table class="contentstable" align="center"><tr>
    <td width="50%">
      <p class="biglink"><a class="biglink" href="{{ pathto("bugs") }}">{% trans %}Reporting issues{% endtrans %}</a></p>
      <p class="biglink"><a class="biglink" href="https://devguide.python.org/documentation/help-documenting/">{% trans %}Contributing to Docs{% endtrans %}</a></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("download") }}">{% trans %}Download the documentation{% endtrans %}</a></p>
    </td><td width="50%">
      <p class="biglink"><a class="biglink" href="{{ pathto("license") }}">{% trans %}History and license of Python{% endtrans %}</a></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("copyright") }}">{% trans %}Copyright{% endtrans %}</a></p>
      <p class="biglink"><a class="biglink" href="{{ pathto("about") }}">{% trans %}About the documentation{% endtrans %}</a></p>
    </td></tr>
  </table>
{% endblock %}


================================================
File: /Doc/tools/templates/indexsidebar.html
================================================
<h3>{% trans %}Download{% endtrans %}</h3>
<p><a href="{{ pathto('download') }}">{% trans %}Download these documents{% endtrans %}</a></p>
<h3>{% trans %}Docs by version{% endtrans %}</h3>
<ul>
  <li><a href="https://docs.python.org/">{% trans %}Stable{% endtrans %}</a></li>
  <li><a href="https://docs.python.org/dev/">{% trans %}In development{% endtrans %}</a></li>
  <li><a href="https://www.python.org/doc/versions/">{% trans %}All versions{% endtrans %}</a></li>
</ul>

<h3>{% trans %}Other resources{% endtrans %}</h3>
<ul>
  {# XXX: many of these should probably be merged in the main docs #}
  <li><a href="https://peps.python.org/">{% trans %}PEP Index{% endtrans %}</a></li>
  <li><a href="https://wiki.python.org/moin/BeginnersGuide">{% trans %}Beginner's Guide{% endtrans %}</a></li>
  <li><a href="https://wiki.python.org/moin/PythonBooks">{% trans %}Book List{% endtrans %}</a></li>
  <li><a href="https://www.python.org/doc/av/">{% trans %}Audio/Visual Talks{% endtrans %}</a></li>
  <li><a href="https://devguide.python.org/">{% trans %}Python Developer’s Guide{% endtrans %}</a></li>
</ul>


================================================
File: /Doc/tools/templates/layout.html
================================================
{% extends "!layout.html" %}

{% block header %}
{%- if outdated %}
<div id="outdated-warning" style="padding: .5em; text-align: center; background-color: #FFBABA; color: #6A0E0E;">
    {% trans %}This document is for an old version of Python that is no longer supported.
    You should upgrade, and read the{% endtrans %}
    <a href="/3/{{ pagename }}{{ file_suffix }}">{% trans %}Python documentation for the current stable release{% endtrans %}</a>.
</div>
{%- endif %}

{%- if is_deployment_preview %}
<div id="deployment-preview-warning" style="padding: .5em; text-align: center; background-color: #fff2ba; color: #6a580e;">
  {% trans %}This is a deploy preview created from a <a href="{{ repository_url }}/pull/{{ pr_id }}">pull request</a>.
  For authoritative documentation, see{% endtrans %}
  <a href="https://docs.python.org/3/{{ pagename }}{{ file_suffix }}">{% trans %}the current stable release{% endtrans %}</a>.
</div>
{%- endif %}
{% endblock %}

{% block rootrellink %}
{{ super() }}
    <li id="cpython-language-and-version">
      <a href="{{ pathto('index') }}">{{ shorttitle }}</a>{{ reldelim1 }}
    </li>
{% endblock %}

{% block extrahead %}
    {% if builder == "html" and enable_analytics %}
      <script defer data-domain="docs.python.org" src="https://plausible.io/js/script.js"></script>
    {% endif %}
    <link rel="canonical" href="https://docs.python.org/3/{{pagename}}.html" />
    {% if builder != "htmlhelp" %}
      {% if pagename == 'whatsnew/changelog' and not embedded %}
      <script type="text/javascript" src="{{ pathto('_static/changelog_search.js', 1) }}"></script>{% endif %}
    {% endif %}

    {# custom CSS; used in asyncio docs! #}
    <style>
      @media only screen {{ "{" }}
        table.full-width-table {{ "{" }}
            width: 100%;
        {{ "}" }}
      {{ "}" }}
    </style>
{{ super() }}

{%- if not embedded %}
            <script type="text/javascript" src="{{ pathto('_static/rtd_switcher.js', 1) }}"></script>
            <meta name="readthedocs-addons-api-version" content="1">
{%- endif %}
{% endblock %}


================================================
File: /Doc/tools/templates/opensearch.xml
================================================
{% extends "!opensearch.xml" %}
{% block extra -%}
<Image height="16" width="16" type="image/x-icon">https://www.python.org/images/favicon16x16.ico</Image>
{%- endblock %}


================================================
File: /Doc/tools/templates/search.html
================================================
{% extends "!search.html" %}
{% block extrahead %}
    {{ super() }}
    <meta name="robots" content="noindex">
    <script type="text/javascript" src="{{ pathto('_static/glossary_search.js', resource=True) }}"></script>
{% endblock %}
{% block searchresults %}
<div id="search-results">
  {# For glossary_search.js #}
  <div style="display: none;" class="admonition seealso" id="glossary-result">
    <p class="topic-title">
    <a id="glossary-title" href="#"></a>
    </p>
  <div id="glossary-body"></div>
</div>
</div>
{% endblock %}


================================================
File: /Doc/tutorial/appendix.rst
================================================
.. _tut-appendix:

********
Appendix
********


.. _tut-interac:

Interactive Mode
================

There are two variants of the interactive :term:`REPL`.  The classic
basic interpreter is supported on all platforms with minimal line
control capabilities.

On Windows, or Unix-like systems with :mod:`curses` support,
a new interactive shell is used by default.
This one supports color, multiline editing, history browsing, and
paste mode.  To disable color, see :ref:`using-on-controlling-color` for
details.  Function keys provide some additional functionality.
:kbd:`F1` enters the interactive help browser :mod:`pydoc`.
:kbd:`F2` allows for browsing command-line history with neither output nor the
:term:`>>>` and :term:`...` prompts. :kbd:`F3` enters "paste mode", which
makes pasting larger blocks of code easier. Press :kbd:`F3` to return to
the regular prompt.

When using the new interactive shell, exit the shell by typing :kbd:`exit`
or :kbd:`quit`. Adding call parentheses after those commands is not
required.

If the new interactive shell is not desired, it can be disabled via
the :envvar:`PYTHON_BASIC_REPL` environment variable.

.. _tut-error:

Error Handling
--------------

When an error occurs, the interpreter prints an error message and a stack trace.
In interactive mode, it then returns to the primary prompt; when input came from
a file, it exits with a nonzero exit status after printing the stack trace.
(Exceptions handled by an :keyword:`except` clause in a :keyword:`try` statement
are not errors in this context.)  Some errors are unconditionally fatal and
cause an exit with a nonzero exit status; this applies to internal inconsistencies and
some cases of running out of memory.  All error messages are written to the
standard error stream; normal output from executed commands is written to
standard output.

Typing the interrupt character (usually :kbd:`Control-C` or :kbd:`Delete`) to the primary or
secondary prompt cancels the input and returns to the primary prompt. [#]_
Typing an interrupt while a command is executing raises the
:exc:`KeyboardInterrupt` exception, which may be handled by a :keyword:`try`
statement.


.. _tut-scripts:

Executable Python Scripts
-------------------------

On BSD'ish Unix systems, Python scripts can be made directly executable, like
shell scripts, by putting the line ::

   #!/usr/bin/env python3

(assuming that the interpreter is on the user's :envvar:`PATH`) at the beginning
of the script and giving the file an executable mode.  The ``#!`` must be the
first two characters of the file.  On some platforms, this first line must end
with a Unix-style line ending (``'\n'``), not a Windows (``'\r\n'``) line
ending.  Note that the hash, or pound, character, ``'#'``, is used to start a
comment in Python.

The script can be given an executable mode, or permission, using the
:program:`chmod` command.

.. code-block:: shell-session

   $ chmod +x myscript.py

On Windows systems, there is no notion of an "executable mode".  The Python
installer automatically associates ``.py`` files with ``python.exe`` so that
a double-click on a Python file will run it as a script.  The extension can
also be ``.pyw``, in that case, the console window that normally appears is
suppressed.


.. _tut-startup:

The Interactive Startup File
----------------------------

When you use Python interactively, it is frequently handy to have some standard
commands executed every time the interpreter is started.  You can do this by
setting an environment variable named :envvar:`PYTHONSTARTUP` to the name of a
file containing your start-up commands.  This is similar to the :file:`.profile`
feature of the Unix shells.

This file is only read in interactive sessions, not when Python reads commands
from a script, and not when :file:`/dev/tty` is given as the explicit source of
commands (which otherwise behaves like an interactive session).  It is executed
