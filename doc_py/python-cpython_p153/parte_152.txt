        self.src_dir = os.path.join(args.base_directory, 'src')
        self.src_file = os.path.join(
            self.src_dir, self.src_template.format(version))
        # build directory (removed after install)
        self.build_dir = os.path.join(
            self.src_dir, self.build_template.format(version))
        self.system = args.system

    def __str__(self):
        return "<{0.__class__.__name__} for {0.version}>".format(self)

    def __eq__(self, other):
        if not isinstance(other, AbstractBuilder):
            return NotImplemented
        return (
            self.library == other.library
            and self.version == other.version
        )

    def __hash__(self):
        return hash((self.library, self.version))

    @property
    def short_version(self):
        """Short version for OpenSSL download URL"""
        return None

    @property
    def openssl_cli(self):
        """openssl CLI binary"""
        return os.path.join(self.install_dir, "bin", "openssl")

    @property
    def openssl_version(self):
        """output of 'bin/openssl version'"""
        cmd = [self.openssl_cli, "version"]
        return self._subprocess_output(cmd)

    @property
    def pyssl_version(self):
        """Value of ssl.OPENSSL_VERSION"""
        cmd = [
            sys.executable,
            '-c', 'import ssl; print(ssl.OPENSSL_VERSION)'
        ]
        return self._subprocess_output(cmd)

    @property
    def include_dir(self):
        return os.path.join(self.install_dir, "include")

    @property
    def lib_dir(self):
        return os.path.join(self.install_dir, "lib")

    @property
    def has_openssl(self):
        return os.path.isfile(self.openssl_cli)

    @property
    def has_src(self):
        return os.path.isfile(self.src_file)

    def _subprocess_call(self, cmd, env=None, **kwargs):
        log.debug("Call '{}'".format(" ".join(cmd)))
        return subprocess.check_call(cmd, env=env, **kwargs)

    def _subprocess_output(self, cmd, env=None, **kwargs):
        log.debug("Call '{}'".format(" ".join(cmd)))
        if env is None:
            env = os.environ.copy()
            env["LD_LIBRARY_PATH"] = self.lib_dir
        out = subprocess.check_output(cmd, env=env, **kwargs)
        return out.strip().decode("utf-8")

    def _download_src(self):
        """Download sources"""
        src_dir = os.path.dirname(self.src_file)
        if not os.path.isdir(src_dir):
            os.makedirs(src_dir)
        data = None
        for url_template in self.url_templates:
            url = url_template.format(v=self.version, s=self.short_version)
            log.info("Downloading from {}".format(url))
            try:
                req = urlopen(url)
                # KISS, read all, write all
                data = req.read()
            except HTTPError as e:
                log.error(
                    "Download from {} has from failed: {}".format(url, e)
                )
            else:
                log.info("Successfully downloaded from {}".format(url))
                break
        if data is None:
            raise ValueError("All download URLs have failed")
        log.info("Storing {}".format(self.src_file))
        with open(self.src_file, "wb") as f:
            f.write(data)

    def _unpack_src(self):
        """Unpack tar.gz bundle"""
        # cleanup
        if os.path.isdir(self.build_dir):
            shutil.rmtree(self.build_dir)
        os.makedirs(self.build_dir)

        tf = tarfile.open(self.src_file)
        name = self.build_template.format(self.version)
        base = name + '/'
        # force extraction into build dir
        members = tf.getmembers()
        for member in list(members):
            if member.name == name:
                members.remove(member)
            elif not member.name.startswith(base):
                raise ValueError(member.name, base)
            member.name = member.name[len(base):].lstrip('/')
        log.info("Unpacking files to {}".format(self.build_dir))
        tf.extractall(self.build_dir, members)

    def _build_src(self, config_args=()):
        """Now build openssl"""
        log.info("Running build in {}".format(self.build_dir))
        cwd = self.build_dir
        cmd = [
            "./config", *config_args,
            "shared", "--debug",
            "--prefix={}".format(self.install_dir)
        ]
        # cmd.extend(["no-deprecated", "--api=1.1.0"])
        env = os.environ.copy()
        # set rpath
        env["LD_RUN_PATH"] = self.lib_dir
        if self.system:
            env['SYSTEM'] = self.system
        self._subprocess_call(cmd, cwd=cwd, env=env)
        if self.depend_target:
            self._subprocess_call(
                ["make", "-j1", self.depend_target], cwd=cwd, env=env
            )
        self._subprocess_call(["make", f"-j{self.jobs}"], cwd=cwd, env=env)

    def _make_install(self):
        self._subprocess_call(
            ["make", "-j1", self.install_target],
            cwd=self.build_dir
        )
        self._post_install()
        if not self.args.keep_sources:
            shutil.rmtree(self.build_dir)

    def _post_install(self):
        pass

    def install(self):
        log.info(self.openssl_cli)
        if not self.has_openssl or self.args.force:
            if not self.has_src:
                self._download_src()
            else:
                log.debug("Already has src {}".format(self.src_file))
            self._unpack_src()
            self._build_src()
            self._make_install()
        else:
            log.info("Already has installation {}".format(self.install_dir))
        # validate installation
        version = self.openssl_version
        if self.version not in version:
            raise ValueError(version)

    def recompile_pymods(self):
        log.warning("Using build from {}".format(self.build_dir))
        # force a rebuild of all modules that use OpenSSL APIs
        for fname in self.module_files:
            os.utime(fname, None)
        # remove all build artefacts
        for root, dirs, files in os.walk('build'):
            for filename in files:
                if filename.startswith(self.module_libs):
                    os.unlink(os.path.join(root, filename))

        # overwrite header and library search paths
        env = os.environ.copy()
        env["CPPFLAGS"] = "-I{}".format(self.include_dir)
        env["LDFLAGS"] = "-L{}".format(self.lib_dir)
        # set rpath
        env["LD_RUN_PATH"] = self.lib_dir

        log.info("Rebuilding Python modules")
        cmd = ["make", "sharedmods", "checksharedmods"]
        self._subprocess_call(cmd, env=env)
        self.check_imports()

    def check_imports(self):
        cmd = [sys.executable, "-c", "import _ssl; import _hashlib"]
        self._subprocess_call(cmd)

    def check_pyssl(self):
        version = self.pyssl_version
        if self.version not in version:
            raise ValueError(version)

    def run_python_tests(self, tests, network=True):
        if not tests:
            cmd = [
                sys.executable,
                os.path.join(PYTHONROOT, 'Lib/test/ssltests.py'),
                '-j0'
            ]
        elif sys.version_info < (3, 3):
            cmd = [sys.executable, '-m', 'test.regrtest']
        else:
            cmd = [sys.executable, '-m', 'test', '-j0']
        if network:
            cmd.extend(['-u', 'network', '-u', 'urlfetch'])
        cmd.extend(['-w', '-r'])
        cmd.extend(tests)
        self._subprocess_call(cmd, stdout=None)


class BuildOpenSSL(AbstractBuilder):
    library = "OpenSSL"
    url_templates = (
        "https://github.com/openssl/openssl/releases/download/openssl-{v}/openssl-{v}.tar.gz",
        "https://www.openssl.org/source/openssl-{v}.tar.gz",
        "https://www.openssl.org/source/old/{s}/openssl-{v}.tar.gz"
    )
    src_template = "openssl-{}.tar.gz"
    build_template = "openssl-{}"
    # only install software, skip docs
    install_target = 'install_sw'
    depend_target = 'depend'

    def _post_install(self):
        if self.version.startswith("3."):
            self._post_install_3xx()

    def _build_src(self, config_args=()):
        if self.version.startswith("3."):
            config_args += ("enable-fips",)
        super()._build_src(config_args)

    def _post_install_3xx(self):
        # create ssl/ subdir with example configs
        # Install FIPS module
        self._subprocess_call(
            ["make", "-j1", "install_ssldirs", "install_fips"],
            cwd=self.build_dir
        )
        if not os.path.isdir(self.lib_dir):
            # 3.0.0-beta2 uses lib64 on 64 bit platforms
            lib64 = self.lib_dir + "64"
            os.symlink(lib64, self.lib_dir)

    @property
    def short_version(self):
        """Short version for OpenSSL download URL"""
        mo = re.search(r"^(\d+)\.(\d+)\.(\d+)", self.version)
        parsed = tuple(int(m) for m in mo.groups())
        if parsed < (1, 0, 0):
            return "0.9.x"
        if parsed >= (3, 0, 0):
            # OpenSSL 3.0.0 -> /old/3.0/
            parsed = parsed[:2]
        return ".".join(str(i) for i in parsed)


class BuildLibreSSL(AbstractBuilder):
    library = "LibreSSL"
    url_templates = (
        "https://ftp.openbsd.org/pub/OpenBSD/LibreSSL/libressl-{v}.tar.gz",
    )
    src_template = "libressl-{}.tar.gz"
    build_template = "libressl-{}"


def configure_make():
    if not os.path.isfile('Makefile'):
        log.info('Running ./configure')
        subprocess.check_call([
            './configure', '--config-cache', '--quiet',
            '--with-pydebug'
        ])

    log.info('Running make')
    subprocess.check_call(['make', '--quiet'])


def main():
    args = parser.parse_args()
    if not args.openssl and not args.libressl:
        args.openssl = list(OPENSSL_RECENT_VERSIONS)
        args.libressl = list(LIBRESSL_RECENT_VERSIONS)
        if not args.disable_ancient:
            args.openssl.extend(OPENSSL_OLD_VERSIONS)
            args.libressl.extend(LIBRESSL_OLD_VERSIONS)

    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.INFO,
        format="*** %(levelname)s %(message)s"
    )

    start = datetime.now()

    if args.steps in {'modules', 'tests'}:
        for name in ['Makefile.pre.in', 'Modules/_ssl.c']:
            if not os.path.isfile(os.path.join(PYTHONROOT, name)):
                parser.error(
                    "Must be executed from CPython build dir"
                )
        if not os.path.samefile('python', sys.executable):
            parser.error(
                "Must be executed with ./python from CPython build dir"
            )
        # check for configure and run make
        configure_make()

    # download and register builder
    builds = []

    for version in args.openssl:
        build = BuildOpenSSL(
            version,
            args
        )
        build.install()
        builds.append(build)

    for version in args.libressl:
        build = BuildLibreSSL(
            version,
            args
        )
        build.install()
        builds.append(build)

    if args.steps in {'modules', 'tests'}:
        for build in builds:
            try:
                build.recompile_pymods()
                build.check_pyssl()
                if args.steps == 'tests':
                    build.run_python_tests(
                        tests=args.tests,
                        network=args.network,
                    )
            except Exception as e:
                log.exception("%s failed", build)
                print("{} failed: {}".format(build, e), file=sys.stderr)
                sys.exit(2)

    log.info("\n{} finished in {}".format(
            args.steps.capitalize(),
            datetime.now() - start
        ))
    print('Python: ', sys.version)
    if args.steps == 'tests':
        if args.tests:
            print('Executed Tests:', ' '.join(args.tests))
        else:
            print('Executed all SSL tests.')

    print('OpenSSL / LibreSSL versions:')
    for build in builds:
        print("    * {0.library} {0.version}".format(build))


if __name__ == "__main__":
    main()


================================================
File: /Tools/tsan/suppressions_free_threading.txt
================================================
# This file contains suppressions for the free-threaded build. It contains the
# suppressions for the default build and additional suppressions needed only in
# the free-threaded build.
#
# reference: https://github.com/google/sanitizers/wiki/ThreadSanitizerSuppressions

## Default build suppresssions

race:get_allocator_unlocked
race:set_allocator_unlocked

## Free-threaded suppressions


# These entries are for warnings that trigger in a library function, as called
# by a CPython function.

# https://gist.github.com/swtaarrs/08dfe7883b4c975c31ecb39388987a67
race:free_threadstate


# These warnings trigger directly in a CPython function.

race_top:assign_version_tag
race_top:new_reference
race_top:_multiprocessing_SemLock_acquire_impl
race_top:list_get_item_ref
race_top:_Py_slot_tp_getattr_hook
race_top:add_threadstate
race_top:dump_traceback
race_top:fatal_error
race_top:_multiprocessing_SemLock_release_impl
race_top:_PyFrame_GetCode
race_top:_PyFrame_Initialize
race_top:PyInterpreterState_ThreadHead
race_top:_PyObject_TryGetInstanceAttribute
race_top:PyThreadState_Next
race_top:PyUnstable_InterpreterFrame_GetLine
race_top:tstate_delete_common
race_top:tstate_is_freed
race_top:type_modified_unlocked
race_top:write_thread_id
race_top:PyThreadState_Clear
# Only seen on macOS, sample: https://gist.github.com/aisk/dda53f5d494a4556c35dde1fce03259c
race_top:set_default_allocator_unlocked

# https://gist.github.com/mpage/6962e8870606cfc960e159b407a0cb40
thread:pthread_create


================================================
File: /Tools/tsan/supressions.txt
================================================
# This file contains suppressions for the default (with GIL) build.
# reference: https://github.com/google/sanitizers/wiki/ThreadSanitizerSuppressions
race:get_allocator_unlocked
race:set_allocator_unlocked

# https://gist.github.com/mpage/daaf32b39180c1989572957b943eb665
thread:pthread_create


================================================
File: /Tools/tz/zdump.py
================================================
import sys
import os
import struct
from array import array
from collections import namedtuple
from datetime import datetime

ttinfo = namedtuple('ttinfo', ['tt_gmtoff', 'tt_isdst', 'tt_abbrind'])

class TZInfo:
    def __init__(self, transitions, type_indices, ttis, abbrs):
        self.transitions = transitions
        self.type_indices = type_indices
        self.ttis = ttis
        self.abbrs = abbrs

    @classmethod
    def fromfile(cls, fileobj):
        if fileobj.read(4).decode() != "TZif":
            raise ValueError("not a zoneinfo file")
        fileobj.seek(20)
        header = fileobj.read(24)
        tzh = (tzh_ttisgmtcnt, tzh_ttisstdcnt, tzh_leapcnt,
               tzh_timecnt, tzh_typecnt, tzh_charcnt) = struct.unpack(">6l", header)
        transitions = array('i')
        transitions.fromfile(fileobj, tzh_timecnt)
        if sys.byteorder != 'big':
            transitions.byteswap()

        type_indices = array('B')
        type_indices.fromfile(fileobj, tzh_timecnt)

        ttis = []
        for i in range(tzh_typecnt):
            ttis.append(ttinfo._make(struct.unpack(">lbb", fileobj.read(6))))

        abbrs = fileobj.read(tzh_charcnt)

        self = cls(transitions, type_indices, ttis, abbrs)
        self.tzh = tzh

        return self

    def dump(self, stream, start=None, end=None):
        for j, (trans, i) in enumerate(zip(self.transitions, self.type_indices)):
            utc = datetime.utcfromtimestamp(trans)
            tti = self.ttis[i]
            lmt = datetime.utcfromtimestamp(trans + tti.tt_gmtoff)
            abbrind = tti.tt_abbrind
            abbr = self.abbrs[abbrind:self.abbrs.find(0, abbrind)].decode()
            if j > 0:
                prev_tti = self.ttis[self.type_indices[j - 1]]
                shift = " %+g" % ((tti.tt_gmtoff - prev_tti.tt_gmtoff) / 3600)
            else:
                shift = ''
            print("%s UTC = %s %-5s isdst=%d" % (utc, lmt, abbr, tti[1]) + shift, file=stream)

    @classmethod
    def zonelist(cls, zonedir='/usr/share/zoneinfo'):
        zones = []
        for root, _, files in os.walk(zonedir):
            for f in files:
                p = os.path.join(root, f)
                with open(p, 'rb') as o:
                    magic =  o.read(4)
                if magic == b'TZif':
                    zones.append(p[len(zonedir) + 1:])
        return zones

if __name__ == '__main__':
    if len(sys.argv) < 2:
        zones = TZInfo.zonelist()
        for z in zones:
            print(z)
        sys.exit()
    filepath = sys.argv[1]
    if not filepath.startswith('/'):
        filepath = os.path.join('/usr/share/zoneinfo', filepath)
    with open(filepath, 'rb') as fileobj:
        tzi = TZInfo.fromfile(fileobj)
    tzi.dump(sys.stdout)


================================================
File: /Tools/unicode/Makefile
================================================
#
# Recreate the Python charmap codecs from the Unicode mapping
# files available at ftp://ftp.unicode.org/
#
#(c) Copyright Marc-Andre Lemburg, 2005.
#    Licensed to PSF under a Contributor Agreement.

# Python binary to use
PYTHON = python

# Remove tool to use
RM = /bin/rm

### Generic targets

all:	distclean mappings codecs

codecs:	misc windows iso apple ebcdic custom-mappings cjk

### Mappings

mappings:
	ncftpget -R ftp.unicode.org . Public/MAPPINGS

### Codecs

build/:
	mkdir build

misc:	build/
	$(PYTHON) gencodec.py MAPPINGS/VENDORS/MISC/ build/
	$(RM) build/atarist.*
	$(RM) build/us_ascii_quotes.*
	$(RM) build/ibmgraph.*
	$(RM) build/sgml.*
	$(RM) -f build/readme.*

custom-mappings:	build/
	$(PYTHON) gencodec.py python-mappings/ build/

windows:	build/
	$(PYTHON) gencodec.py MAPPINGS/VENDORS/MICSFT/WINDOWS/ build/
	$(RM) build/cp9*
	$(RM) -f build/readme.*

iso:	build/
	$(PYTHON) gencodec.py MAPPINGS/ISO8859/ build/ iso
	$(RM) -f build/isoreadme.*

apple:	build/
	$(PYTHON) gencodec.py MAPPINGS/VENDORS/APPLE/ build/ mac_
	$(RM) build/mac_dingbats.*
	$(RM) build/mac_japanese.*
	$(RM) build/mac_chin*
	$(RM) build/mac_korean.*
	$(RM) build/mac_symbol.*
	$(RM) build/mac_corpchar.*
	$(RM) build/mac_devanaga.*
	$(RM) build/mac_gaelic.*
	$(RM) build/mac_gurmukhi.*
	$(RM) build/mac_hebrew.*
	$(RM) build/mac_inuit.*
	$(RM) build/mac_thai.*
	$(RM) build/mac_ukraine.*
	$(RM) build/mac_arabic.py
	$(RM) build/mac_celtic.*
	$(RM) build/mac_gujarati.*
	$(RM) build/mac_keyboard.*
	$(RM) -f build/mac_readme.*

ebcdic:	build/
	$(PYTHON) gencodec.py MAPPINGS/VENDORS/MICSFT/EBCDIC/ build/
	$(RM) -f build/readme.*

cjk:	build/
	$(PYTHON) gencjkcodecs.py build/

### Cleanup

clean:
	$(RM) -f build/*

distclean:	clean
	$(RM) -rf MAPPINGS/


================================================
File: /Tools/unicode/comparecodecs.py
================================================
#!/usr/bin/env python3

""" Compare the output of two codecs.

(c) Copyright 2005, Marc-Andre Lemburg (mal@lemburg.com).

    Licensed to PSF under a Contributor Agreement.

"""
import sys

def compare_codecs(encoding1, encoding2):

    print('Comparing encoding/decoding of   %r and   %r' % (encoding1, encoding2))
    mismatch = 0
    # Check encoding
    for i in range(sys.maxunicode+1):
        u = chr(i)
        try:
            c1 = u.encode(encoding1)
        except UnicodeError as reason:
            c1 = '<undefined>'
        try:
            c2 = u.encode(encoding2)
        except UnicodeError as reason:
            c2 = '<undefined>'
        if c1 != c2:
            print(' * encoding mismatch for 0x%04X: %-14r != %r' % \
                  (i, c1, c2))
            mismatch += 1
    # Check decoding
    for i in range(256):
        c = bytes([i])
        try:
            u1 = c.decode(encoding1)
        except UnicodeError:
            u1 = '<undefined>'
        try:
            u2 = c.decode(encoding2)
        except UnicodeError:
            u2 = '<undefined>'
        if u1 != u2:
            print(' * decoding mismatch for 0x%04X: %-14r != %r' % \
                  (i, u1, u2))
            mismatch += 1
    if mismatch:
        print()
        print('Found %i mismatches' % mismatch)
    else:
        print('-> Codecs are identical.')

if __name__ == '__main__':
    compare_codecs(sys.argv[1], sys.argv[2])


================================================
File: /Tools/unicode/dawg.py
================================================
# Original Algorithm:
# By Steve Hanov, 2011. Released to the public domain.
# Please see http://stevehanov.ca/blog/index.php?id=115 for the accompanying article.
#
# Adapted for PyPy/CPython by Carl Friedrich Bolz-Tereick
#
# Based on Daciuk, Jan, et al. "Incremental construction of minimal acyclic finite-state automata."
# Computational linguistics 26.1 (2000): 3-16.
#
# Updated 2014 to use DAWG as a mapping; see
# Kowaltowski, T.; CL. Lucchesi (1993), "Applications of finite automata representing large vocabularies",
# Software-Practice and Experience 1993

from collections import defaultdict
from functools import cached_property


# This class represents a node in the directed acyclic word graph (DAWG). It
# has a list of edges to other nodes. It has functions for testing whether it
# is equivalent to another node. Nodes are equivalent if they have identical
# edges, and each identical edge leads to identical states. The __hash__ and
# __eq__ functions allow it to be used as a key in a python dictionary.


class DawgNode:

    def __init__(self, dawg):
        self.id = dawg.next_id
        dawg.next_id += 1
        self.final = False
        self.edges = {}

        self.linear_edges = None # later: list of (string, next_state)

    def __str__(self):
        if self.final:
            arr = ["1"]
        else:
            arr = ["0"]

        for (label, node) in sorted(self.edges.items()):
            arr.append(label)
            arr.append(str(node.id))

        return "_".join(arr)
    __repr__ = __str__

    def _as_tuple(self):
        edges = sorted(self.edges.items())
        edge_tuple = tuple((label, node.id) for label, node in edges)
        return (self.final, edge_tuple)

    def __hash__(self):
        return hash(self._as_tuple())

    def __eq__(self, other):
        return self._as_tuple() == other._as_tuple()

    @cached_property
    def num_reachable_linear(self):
        # returns the number of different paths to final nodes reachable from
        # this one

        count = 0
        # staying at self counts as a path if self is final
        if self.final:
            count += 1
        for label, node in self.linear_edges:
            count += node.num_reachable_linear

        return count


class Dawg:
    def __init__(self):
        self.previous_word = ""
        self.next_id = 0
        self.root = DawgNode(self)

        # Here is a list of nodes that have not been checked for duplication.
        self.unchecked_nodes = []

        # To deduplicate, maintain a dictionary with
        # minimized_nodes[canonical_node] is canonical_node.
        # Based on __hash__ and __eq__, minimized_nodes[n] is the
        # canonical node equal to n.
        # In other words, self.minimized_nodes[x] == x for all nodes found in
        # the dict.
        self.minimized_nodes = {}

        # word: value mapping
        self.data = {}
        # value: word mapping
        self.inverse = {}

    def insert(self, word, value):
        if not all(0 <= ord(c) < 128 for c in word):
            raise ValueError("Use 7-bit ASCII characters only")
        if word <= self.previous_word:
            raise ValueError("Error: Words must be inserted in alphabetical order.")
        if value in self.inverse:
            raise ValueError(f"value {value} is duplicate, got it for word {self.inverse[value]} and now {word}")

        # find common prefix between word and previous word
        common_prefix = 0
        for i in range(min(len(word), len(self.previous_word))):
            if word[i] != self.previous_word[i]:
                break
            common_prefix += 1

        # Check the unchecked_nodes for redundant nodes, proceeding from last
        # one down to the common prefix size. Then truncate the list at that
        # point.
        self._minimize(common_prefix)

        self.data[word] = value
        self.inverse[value] = word

        # add the suffix, starting from the correct node mid-way through the
        # graph
        if len(self.unchecked_nodes) == 0:
            node = self.root
        else:
            node = self.unchecked_nodes[-1][2]

        for letter in word[common_prefix:]:
            next_node = DawgNode(self)
            node.edges[letter] = next_node
            self.unchecked_nodes.append((node, letter, next_node))
            node = next_node

        node.final = True
        self.previous_word = word

    def finish(self):
        if not self.data:
            raise ValueError("need at least one word in the dawg")
        # minimize all unchecked_nodes
        self._minimize(0)

        self._linearize_edges()

        topoorder, linear_data, inverse = self._topological_order()
        return self.compute_packed(topoorder), linear_data, inverse

    def _minimize(self, down_to):
        # proceed from the leaf up to a certain point
        for i in range(len(self.unchecked_nodes) - 1, down_to - 1, -1):
            (parent, letter, child) = self.unchecked_nodes[i]
            if child in self.minimized_nodes:
                # replace the child with the previously encountered one
                parent.edges[letter] = self.minimized_nodes[child]
            else:
                # add the state to the minimized nodes.
                self.minimized_nodes[child] = child
            self.unchecked_nodes.pop()

    def _lookup(self, word):
        """ Return an integer 0 <= k < number of strings in dawg
        where word is the kth successful traversal of the dawg. """
        node = self.root
        skipped = 0  # keep track of number of final nodes that we skipped
        index = 0
        while index < len(word):
            for label, child in node.linear_edges:
                if word[index] == label[0]:
                    if word[index:index + len(label)] == label:
                        if node.final:
                            skipped += 1
                        index += len(label)
                        node = child
                        break
                    else:
                        return None
                skipped += child.num_reachable_linear
            else:
                return None
        return skipped

    def enum_all_nodes(self):
        stack = [self.root]
        done = set()
        while stack:
            node = stack.pop()
            if node.id in done:
                continue
            yield node
            done.add(node.id)
            for label, child in sorted(node.edges.items()):
                stack.append(child)

    def prettyprint(self):
        for node in sorted(self.enum_all_nodes(), key=lambda e: e.id):
            s_final = " final" if node.final else ""
            print(f"{node.id}: ({node}) {s_final}")
            for label, child in sorted(node.edges.items()):
                print(f"    {label} goto {child.id}")

    def _inverse_lookup(self, number):
        assert 0, "not working in the current form, but keep it as the pure python version of compact lookup"
        result = []
        node = self.root
        while 1:
            if node.final:
                if pos == 0:
                    return "".join(result)
                pos -= 1
            for label, child in sorted(node.edges.items()):
                nextpos = pos - child.num_reachable_linear
                if nextpos < 0:
                    result.append(label)
                    node = child
                    break
                else:
                    pos = nextpos
            else:
                assert 0

    def _linearize_edges(self):
        # compute "linear" edges. the idea is that long chains of edges without
        # any of the intermediate states being final or any extra incoming or
        # outgoing edges can be represented by having removing them, and
        # instead using longer strings as edge labels (instead of single
        # characters)
        incoming = defaultdict(list)
        nodes = sorted(self.enum_all_nodes(), key=lambda e: e.id)
        for node in nodes:
            for label, child in sorted(node.edges.items()):
                incoming[child].append(node)
        for node in nodes:
            node.linear_edges = []
            for label, child in sorted(node.edges.items()):
                s = [label]
                while len(child.edges) == 1 and len(incoming[child]) == 1 and not child.final:
                    (c, child), = child.edges.items()
                    s.append(c)
                node.linear_edges.append((''.join(s), child))

    def _topological_order(self):
        # compute reachable linear nodes, and the set of incoming edges for each node
        order = []
        stack = [self.root]
        seen = set()
        while stack:
            # depth first traversal
            node = stack.pop()
            if node.id in seen:
                continue
            seen.add(node.id)
            order.append(node)
            for label, child in node.linear_edges:
                stack.append(child)

        # do a (slightly bad) topological sort
        incoming = defaultdict(set)
        for node in order:
            for label, child in node.linear_edges:
                incoming[child].add((label, node))
        no_incoming = [order[0]]
        topoorder = []
        positions = {}
        while no_incoming:
            node = no_incoming.pop()
            topoorder.append(node)
            positions[node] = len(topoorder)
            # use "reversed" to make sure that the linear_edges get reorderd
            # from their alphabetical order as little as necessary (no_incoming
            # is LIFO)
            for label, child in reversed(node.linear_edges):
                incoming[child].discard((label, node))
                if not incoming[child]:
                    no_incoming.append(child)
                    del incoming[child]
        # check result
        assert set(topoorder) == set(order)
        assert len(set(topoorder)) == len(topoorder)

        for node in order:
            node.linear_edges.sort(key=lambda element: positions[element[1]])

        for node in order:
            for label, child in node.linear_edges:
                assert positions[child] > positions[node]
        # number the nodes. afterwards every input string in the set has a
        # unique number in the 0 <= number < len(data). We then put the data in
        # self.data into a linear list using these numbers as indexes.
        topoorder[0].num_reachable_linear
        linear_data = [None] * len(self.data)
        inverse = {} # maps value back to index
        for word, value in self.data.items():
            index = self._lookup(word)
            linear_data[index] = value
            inverse[value] = index

        return topoorder, linear_data, inverse

    def compute_packed(self, order):
        def compute_chunk(node, offsets):
            """ compute the packed node/edge data for a node. result is a
            list of bytes as long as order. the jump distance calculations use
            the offsets dictionary to know where in the final big output
            bytestring the individual nodes will end up. """
            result = bytearray()
            offset = offsets[node]
            encode_varint_unsigned(number_add_bits(node.num_reachable_linear, node.final), result)
            if len(node.linear_edges) == 0:
                assert node.final
                encode_varint_unsigned(0, result) # add a 0 saying "done"
            prev_child_offset = offset + len(result)
            for edgeindex, (label, targetnode) in enumerate(node.linear_edges):
                label = label.encode('ascii')
                child_offset = offsets[targetnode]
                child_offset_difference = child_offset - prev_child_offset

                info = number_add_bits(child_offset_difference, len(label) == 1, edgeindex == len(node.linear_edges) - 1)
                if edgeindex == 0:
                    assert info != 0
                encode_varint_unsigned(info, result)
                prev_child_offset = child_offset
                if len(label) > 1:
                    encode_varint_unsigned(len(label), result)
                result.extend(label)
            return result

        def compute_new_offsets(chunks, offsets):
            """ Given a list of chunks, compute the new offsets (by adding the
            chunk lengths together). Also check if we cannot shrink the output
            further because none of the node offsets are smaller now. if that's
            the case return None. """
            new_offsets = {}
            curr_offset = 0
            should_continue = False
            for node, result in zip(order, chunks):
                if curr_offset < offsets[node]:
                    # the new offset is below the current assumption, this
                    # means we can shrink the output more
                    should_continue = True
                new_offsets[node] = curr_offset
                curr_offset += len(result)
            if not should_continue:
                return None
            return new_offsets

        # assign initial offsets to every node
        offsets = {}
        for i, node in enumerate(order):
            # we don't know position of the edge yet, just use something big as
            # the starting position. we'll have to do further iterations anyway,
            # but the size is at least a lower limit then
            offsets[node] = i * 2 ** 30


        # due to the variable integer width encoding of edge targets we need to
        # run this to fixpoint. in the process we shrink the output more and
        # more until we can't any more. at any point we can stop and use the
        # output, but we might need padding zero bytes when joining the chunks
        # to have the correct jump distances
        last_offsets = None
        while 1:
            chunks = [compute_chunk(node, offsets) for node in order]
            last_offsets = offsets
            offsets = compute_new_offsets(chunks, offsets)
            if offsets is None: # couldn't shrink
                break

        # build the final packed string
        total_result = bytearray()
        for node, result in zip(order, chunks):
            node_offset = last_offsets[node]
            if node_offset > len(total_result):
                # need to pad to get the offsets correct
                padding = b"\x00" * (node_offset - len(total_result))
                total_result.extend(padding)
            assert node_offset == len(total_result)
            total_result.extend(result)
        return bytes(total_result)


# ______________________________________________________________________
# the following functions operate on the packed representation

def number_add_bits(x, *bits):
    for bit in bits:
        assert bit == 0 or bit == 1
        x = (x << 1) | bit
    return x

def encode_varint_unsigned(i, res):
    # https://en.wikipedia.org/wiki/LEB128 unsigned variant
    more = True
    startlen = len(res)
    if i < 0:
        raise ValueError("only positive numbers supported", i)
    while more:
        lowest7bits = i & 0b1111111
        i >>= 7
        if i == 0:
            more = False
        else:
            lowest7bits |= 0b10000000
        res.append(lowest7bits)
    return len(res) - startlen

def number_split_bits(x, n, acc=()):
    if n == 1:
        return x >> 1, x & 1
    if n == 2:
        return x >> 2, (x >> 1) & 1, x & 1
    assert 0, "implement me!"

def decode_varint_unsigned(b, index=0):
    res = 0
    shift = 0
    while True:
        byte = b[index]
        res = res | ((byte & 0b1111111) << shift)
        index += 1
        shift += 7
        if not (byte & 0b10000000):
            return res, index

def decode_node(packed, node):
    x, node = decode_varint_unsigned(packed, node)
    node_count, final = number_split_bits(x, 1)
    return node_count, final, node

def decode_edge(packed, edgeindex, prev_child_offset, offset):
    x, offset = decode_varint_unsigned(packed, offset)
    if x == 0 and edgeindex == 0:
        raise KeyError # trying to decode past a final node
    child_offset_difference, len1, last_edge = number_split_bits(x, 2)
    child_offset = prev_child_offset + child_offset_difference
    if len1:
        size = 1
    else:
        size, offset = decode_varint_unsigned(packed, offset)
    return child_offset, last_edge, size, offset

def _match_edge(packed, s, size, node_offset, stringpos):
    if size > 1 and stringpos + size > len(s):
        # past the end of the string, can't match
        return False
    for i in range(size):
        if packed[node_offset + i] != s[stringpos + i]:
            # if a subsequent char of an edge doesn't match, the word isn't in
            # the dawg
            if i > 0:
                raise KeyError
            return False
    return True

def lookup(packed, data, s):
    return data[_lookup(packed, s)]

def _lookup(packed, s):
    stringpos = 0
    node_offset = 0
    skipped = 0  # keep track of number of final nodes that we skipped
    false = False
    while stringpos < len(s):
        #print(f"{node_offset=} {stringpos=}")
        _, final, edge_offset = decode_node(packed, node_offset)
        prev_child_offset = edge_offset
        edgeindex = 0
        while 1:
            child_offset, last_edge, size, edgelabel_chars_offset = decode_edge(packed, edgeindex, prev_child_offset, edge_offset)
            #print(f"    {edge_offset=} {child_offset=} {last_edge=} {size=} {edgelabel_chars_offset=}")
            edgeindex += 1
            prev_child_offset = child_offset
            if _match_edge(packed, s, size, edgelabel_chars_offset, stringpos):
                # match
                if final:
                    skipped += 1
                stringpos += size
                node_offset = child_offset
                break
            if last_edge:
                raise KeyError
            descendant_count, _, _ = decode_node(packed, child_offset)
            skipped += descendant_count
            edge_offset = edgelabel_chars_offset + size
    _, final, _ = decode_node(packed, node_offset)
    if final:
        return skipped
    raise KeyError

def inverse_lookup(packed, inverse, x):
    pos = inverse[x]
    return _inverse_lookup(packed, pos)

def _inverse_lookup(packed, pos):
    result = bytearray()
    node_offset = 0
    while 1:
        node_count, final, edge_offset = decode_node(packed, node_offset)
        if final:
            if pos == 0:
                return bytes(result)
            pos -= 1
        prev_child_offset = edge_offset
        edgeindex = 0
        while 1:
            child_offset, last_edge, size, edgelabel_chars_offset = decode_edge(packed, edgeindex, prev_child_offset, edge_offset)
            edgeindex += 1
            prev_child_offset = child_offset
            descendant_count, _, _ = decode_node(packed, child_offset)
            nextpos = pos - descendant_count
            if nextpos < 0:
                assert edgelabel_chars_offset >= 0
                result.extend(packed[edgelabel_chars_offset: edgelabel_chars_offset + size])
                node_offset = child_offset
                break
            elif not last_edge:
                pos = nextpos
                edge_offset = edgelabel_chars_offset + size
            else:
                raise KeyError
        else:
            raise KeyError


def build_compression_dawg(ucdata):
    d = Dawg()
    ucdata.sort()
    for name, value in ucdata:
        d.insert(name, value)
    packed, pos_to_code, reversedict = d.finish()
    print("size of dawg [KiB]", round(len(packed) / 1024, 2))
    # check that lookup and inverse_lookup work correctly on the input data
    for name, value in ucdata:
        assert lookup(packed, pos_to_code, name.encode('ascii')) == value
        assert inverse_lookup(packed, reversedict, value) == name.encode('ascii')
    return packed, pos_to_code


================================================
File: /Tools/unicode/gencjkcodecs.py
================================================
import os, string

codecs = {
    'cn': ('gb2312', 'gbk', 'gb18030', 'hz'),
    'tw': ('big5', 'cp950'),
    'hk': ('big5hkscs',),
    'jp': ('cp932', 'shift_jis', 'euc_jp', 'euc_jisx0213', 'shift_jisx0213',
           'euc_jis_2004', 'shift_jis_2004'),
    'kr': ('cp949', 'euc_kr', 'johab'),
    'iso2022': ('iso2022_jp', 'iso2022_jp_1', 'iso2022_jp_2',
                'iso2022_jp_2004', 'iso2022_jp_3', 'iso2022_jp_ext',
                'iso2022_kr'),
}

TEMPLATE = string.Template("""\
#
# $encoding.py: Python Unicode Codec for $ENCODING
#
# Written by Hye-Shik Chang <perky@FreeBSD.org>
#

import _codecs_$owner, codecs
import _multibytecodec as mbc

codec = _codecs_$owner.getcodec('$encoding')

class Codec(codecs.Codec):
    encode = codec.encode
    decode = codec.decode

class IncrementalEncoder(mbc.MultibyteIncrementalEncoder,
                         codecs.IncrementalEncoder):
    codec = codec

class IncrementalDecoder(mbc.MultibyteIncrementalDecoder,
                         codecs.IncrementalDecoder):
    codec = codec

class StreamReader(Codec, mbc.MultibyteStreamReader, codecs.StreamReader):
    codec = codec

class StreamWriter(Codec, mbc.MultibyteStreamWriter, codecs.StreamWriter):
    codec = codec

def getregentry():
    return codecs.CodecInfo(
        name='$encoding',
        encode=Codec().encode,
        decode=Codec().decode,
        incrementalencoder=IncrementalEncoder,
        incrementaldecoder=IncrementalDecoder,
        streamreader=StreamReader,
        streamwriter=StreamWriter,
    )
""")

def gencodecs(prefix):
    for loc, encodings in codecs.items():
        for enc in encodings:
            code = TEMPLATE.substitute(ENCODING=enc.upper(),
                                       encoding=enc.lower(),
                                       owner=loc)
            codecpath = os.path.join(prefix, enc + '.py')
            with open(codecpath, 'w') as f:
                f.write(code)

if __name__ == '__main__':
    import sys
    gencodecs(sys.argv[1])


================================================
File: /Tools/unicode/gencodec.py
================================================
""" Unicode Mapping Parser and Codec Generator.

This script parses Unicode mapping files as available from the Unicode
site (ftp://ftp.unicode.org/Public/MAPPINGS/) and creates Python codec
modules from them. The codecs use the standard character mapping codec
to actually apply the mapping.

Synopsis: gencodec.py dir codec_prefix

All files in dir are scanned and those producing non-empty mappings
will be written to <codec_prefix><mapname>.py with <mapname> being the
first part of the map's filename ('a' in a.b.c.txt) converted to
lowercase with hyphens replaced by underscores.

The tool also writes marshalled versions of the mapping tables to the
same location (with .mapping extension).

Written by Marc-Andre Lemburg (mal@lemburg.com).

(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.
(c) Copyright Guido van Rossum, 2000.

Table generation:
(c) Copyright Marc-Andre Lemburg, 2005.
    Licensed to PSF under a Contributor Agreement.

"""#"

import re, os, marshal, codecs

# Maximum allowed size of charmap tables
MAX_TABLE_SIZE = 8192

# Standard undefined Unicode code point
UNI_UNDEFINED = chr(0xFFFE)

# Placeholder for a missing code point
MISSING_CODE = -1

mapRE = re.compile(r'((?:0x[0-9a-fA-F]+\+?)+)'
                   r'\s+'
                   r'((?:(?:0x[0-9a-fA-Z]+|<[A-Za-z]+>)\+?)*)'
                   r'\s*'
                   r'(#.+)?')

def parsecodes(codes, len=len, range=range):

    """ Converts code combinations to either a single code integer
        or a tuple of integers.

        meta-codes (in angular brackets, e.g. <LR> and <RL>) are
        ignored.

        Empty codes or illegal ones are returned as None.

    """
    if not codes:
        return MISSING_CODE
    l = codes.split('+')
    if len(l) == 1:
        return int(l[0],16)
    for i in range(len(l)):
        try:
            l[i] = int(l[i],16)
        except ValueError:
            l[i] = MISSING_CODE
    l = [x for x in l if x != MISSING_CODE]
    if len(l) == 1:
        return l[0]
    else:
        return tuple(l)

def readmap(filename):

    with open(filename) as f:
        lines = f.readlines()
    enc2uni = {}
    identity = []
    unmapped = list(range(256))

    # UTC mapping tables per convention don't include the identity
    # mappings for code points 0x00 - 0x1F and 0x7F, unless these are
    # explicitly mapped to different characters or undefined
    for i in list(range(32)) + [127]:
        identity.append(i)
        unmapped.remove(i)
        enc2uni[i] = (i, 'CONTROL CHARACTER')

    for line in lines:
        line = line.strip()
        if not line or line[0] == '#':
            continue
        m = mapRE.match(line)
        if not m:
            #print '* not matched: %s' % repr(line)
            continue
        enc,uni,comment = m.groups()
        enc = parsecodes(enc)
        uni = parsecodes(uni)
        if comment is None:
            comment = ''
        else:
            comment = comment[1:].strip()
        if not isinstance(enc, tuple) and enc < 256:
            if enc in unmapped:
                unmapped.remove(enc)
            if enc == uni:
                identity.append(enc)
            enc2uni[enc] = (uni,comment)
        else:
            enc2uni[enc] = (uni,comment)

    # If there are more identity-mapped entries than unmapped entries,
    # it pays to generate an identity dictionary first, and add explicit
    # mappings to None for the rest
    if len(identity) >= len(unmapped):
        for enc in unmapped:
            enc2uni[enc] = (MISSING_CODE, "")
        enc2uni['IDENTITY'] = 256

    return enc2uni

def hexrepr(t, precision=4):

    if t is None:
        return 'None'
    try:
        len(t)
    except TypeError:
        return '0x%0*X' % (precision, t)
    try:
        return '(' + ', '.join(['0x%0*X' % (precision, item)
                                for item in t]) + ')'
    except TypeError as why:
        print('* failed to convert %r: %s' % (t, why))
        raise

def python_mapdef_code(varname, map, comments=1, precisions=(2, 4)):

    l = []
    append = l.append
    if "IDENTITY" in map:
        append("%s = codecs.make_identity_dict(range(%d))" %
               (varname, map["IDENTITY"]))
        append("%s.update({" % varname)
        splits = 1
        del map["IDENTITY"]
        identity = 1
    else:
        append("%s = {" % varname)
        splits = 0
        identity = 0

    mappings = sorted(map.items())
    i = 0
    key_precision, value_precision = precisions
    for mapkey, mapvalue in mappings:
        mapcomment = ''
        if isinstance(mapkey, tuple):
            (mapkey, mapcomment) = mapkey
        if isinstance(mapvalue, tuple):
            (mapvalue, mapcomment) = mapvalue
        if mapkey is None:
            continue
        if (identity and
            mapkey == mapvalue and
            mapkey < 256):
            # No need to include identity mappings, since these
            # are already set for the first 256 code points.
            continue
        key = hexrepr(mapkey, key_precision)
        value = hexrepr(mapvalue, value_precision)
        if mapcomment and comments:
            append('    %s: %s,\t#  %s' % (key, value, mapcomment))
        else:
            append('    %s: %s,' % (key, value))
        i += 1
        if i == 4096:
            # Split the definition into parts to that the Python
            # parser doesn't dump core
            if splits == 0:
                append('}')
            else:
                append('})')
            append('%s.update({' % varname)
            i = 0
            splits = splits + 1
    if splits == 0:
        append('}')
    else:
        append('})')

    return l

def python_tabledef_code(varname, map, comments=1, key_precision=2):

    l = []
    append = l.append
    append('%s = (' % varname)

    # Analyze map and create table dict
    mappings = sorted(map.items())
    table = {}
    maxkey = 255
    if 'IDENTITY' in map:
        for key in range(256):
            table[key] = (key, '')
        del map['IDENTITY']
    for mapkey, mapvalue in mappings:
        mapcomment = ''
        if isinstance(mapkey, tuple):
            (mapkey, mapcomment) = mapkey
        if isinstance(mapvalue, tuple):
            (mapvalue, mapcomment) = mapvalue
        if mapkey == MISSING_CODE:
            continue
        table[mapkey] = (mapvalue, mapcomment)
        if mapkey > maxkey:
            maxkey = mapkey
    if maxkey > MAX_TABLE_SIZE:
        # Table too large
        return None

    # Create table code
    maxchar = 0
    for key in range(maxkey + 1):
        if key not in table:
            mapvalue = MISSING_CODE
            mapcomment = 'UNDEFINED'
        else:
            mapvalue, mapcomment = table[key]
        if mapvalue == MISSING_CODE:
            mapchar = UNI_UNDEFINED
        else:
            if isinstance(mapvalue, tuple):
                # 1-n mappings not supported
                return None
            else:
                mapchar = chr(mapvalue)
        maxchar = max(maxchar, ord(mapchar))
        if mapcomment and comments:
            append('    %a \t#  %s -> %s' % (mapchar,
                                            hexrepr(key, key_precision),
                                            mapcomment))
        else:
            append('    %a' % mapchar)

    if maxchar < 256:
        append('    %a \t## Widen to UCS2 for optimization' % UNI_UNDEFINED)
    append(')')
    return l

def codegen(name, map, encodingname, comments=1):

    """ Returns Python source for the given map.

        Comments are included in the source, if comments is true (default).

    """
    # Generate code
    decoding_map_code = python_mapdef_code(
        'decoding_map',
        map,
        comments=comments)
    decoding_table_code = python_tabledef_code(
        'decoding_table',
        map,
        comments=comments)
    encoding_map_code = python_mapdef_code(
        'encoding_map',
        codecs.make_encoding_map(map),
        comments=comments,
        precisions=(4, 2))

    if decoding_table_code:
        suffix = 'table'
    else:
        suffix = 'map'

    l = [
        '''\
""" Python Character Mapping Codec %s generated from '%s' with gencodec.py.

"""#"

import codecs

### Codec APIs

class Codec(codecs.Codec):

    def encode(self, input, errors='strict'):
        return codecs.charmap_encode(input, errors, encoding_%s)

    def decode(self, input, errors='strict'):
        return codecs.charmap_decode(input, errors, decoding_%s)
''' % (encodingname, name, suffix, suffix)]
    l.append('''\
class IncrementalEncoder(codecs.IncrementalEncoder):
    def encode(self, input, final=False):
        return codecs.charmap_encode(input, self.errors, encoding_%s)[0]

class IncrementalDecoder(codecs.IncrementalDecoder):
    def decode(self, input, final=False):
        return codecs.charmap_decode(input, self.errors, decoding_%s)[0]''' %
        (suffix, suffix))

    l.append('''
class StreamWriter(Codec, codecs.StreamWriter):
    pass

class StreamReader(Codec, codecs.StreamReader):
    pass

### encodings module API

def getregentry():
    return codecs.CodecInfo(
        name=%r,
        encode=Codec().encode,
        decode=Codec().decode,
        incrementalencoder=IncrementalEncoder,
        incrementaldecoder=IncrementalDecoder,
        streamreader=StreamReader,
        streamwriter=StreamWriter,
    )
''' % encodingname.replace('_', '-'))

    # Add decoding table or map (with preference to the table)
    if not decoding_table_code:
        l.append('''
### Decoding Map
''')
        l.extend(decoding_map_code)
    else:
        l.append('''
### Decoding Table
''')
        l.extend(decoding_table_code)

    # Add encoding map
    if decoding_table_code:
        l.append('''
### Encoding table
encoding_table = codecs.charmap_build(decoding_table)
''')
    else:
        l.append('''
### Encoding Map
''')
        l.extend(encoding_map_code)

    # Final new-line
    l.append('')

    return '\n'.join(l).expandtabs()

def pymap(name,map,pyfile,encodingname,comments=1):

    code = codegen(name,map,encodingname,comments)
    with open(pyfile,'w') as f:
        f.write(code)

def marshalmap(name,map,marshalfile):

    d = {}
    for e,(u,c) in map.items():
        d[e] = (u,c)
    with open(marshalfile,'wb') as f:
        marshal.dump(d,f)

def convertdir(dir, dirprefix='', nameprefix='', comments=1):

    mapnames = os.listdir(dir)
    for mapname in mapnames:
        mappathname = os.path.join(dir, mapname)
        if not os.path.isfile(mappathname):
            continue
        name = os.path.split(mapname)[1]
        name = name.replace('-','_')
        name = name.split('.')[0]
        name = name.lower()
        name = nameprefix + name
        codefile = name + '.py'
        marshalfile = name + '.mapping'
        print('converting %s to %s and %s' % (mapname,
                                              dirprefix + codefile,
                                              dirprefix + marshalfile))
        try:
            map = readmap(os.path.join(dir,mapname))
            if not map:
                print('* map is empty; skipping')
            else:
                pymap(mappathname, map, dirprefix + codefile,name,comments)
                marshalmap(mappathname, map, dirprefix + marshalfile)
        except ValueError as why:
            print('* conversion failed: %s' % why)
            raise

def rewritepythondir(dir, dirprefix='', comments=1):

    mapnames = os.listdir(dir)
    for mapname in mapnames:
        if not mapname.endswith('.mapping'):
            continue
        name = mapname[:-len('.mapping')]
        codefile = name + '.py'
        print('converting %s to %s' % (mapname,
                                       dirprefix + codefile))
        try:
            with open(os.path.join(dir, mapname), 'rb') as f:
                map = marshal.load(f)
            if not map:
                print('* map is empty; skipping')
            else:
                pymap(mapname, map, dirprefix + codefile,name,comments)
        except ValueError as why:
            print('* conversion failed: %s' % why)

if __name__ == '__main__':

    import sys
    if 1:
        convertdir(*sys.argv[1:])
    else:
        rewritepythondir(*sys.argv[1:])


================================================
File: /Tools/unicode/genmap_japanese.py
================================================
#
# genmap_ja_codecs.py: Japanese Codecs Map Generator
#
# Original Author:  Hye-Shik Chang <perky@FreeBSD.org>
# Modified Author:  Donghee Na <donghee.na92@gmail.com>
#
import os

from genmap_support import *

JISX0208_C1 = (0x21, 0x74)
JISX0208_C2 = (0x21, 0x7e)
JISX0212_C1 = (0x22, 0x6d)
JISX0212_C2 = (0x21, 0x7e)
JISX0213_C1 = (0x21, 0x7e)
JISX0213_C2 = (0x21, 0x7e)
CP932P0_C1  = (0x81, 0x81) # patches between shift-jis and cp932
CP932P0_C2  = (0x5f, 0xca)
CP932P1_C1  = (0x87, 0x87) # CP932 P1
CP932P1_C2  = (0x40, 0x9c)
CP932P2_C1  = (0xed, 0xfc) # CP932 P2
CP932P2_C2  = (0x40, 0xfc)

MAPPINGS_JIS0208 = 'http://www.unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/JIS/JIS0208.TXT'
MAPPINGS_JIS0212 = 'http://www.unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/JIS/JIS0212.TXT'
MAPPINGS_CP932 = 'http://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/CP932.TXT'
MAPPINGS_JISX0213_2004 = 'http://wakaba-web.hp.infoseek.co.jp/table/jisx0213-2004-std.txt'


def loadmap_jisx0213(fo):
    decmap3, decmap4 = {}, {} # maps to BMP for level 3 and 4
    decmap3_2, decmap4_2 = {}, {} # maps to U+2xxxx for level 3 and 4
    decmap3_pair = {} # maps to BMP-pair for level 3
    for line in fo:
        line = line.split('#', 1)[0].strip()
        if not line or len(line.split()) < 2:
            continue

        row = line.split()
        loc = eval('0x' + row[0][2:])
        level = eval(row[0][0])
        m = None
        if len(row[1].split('+')) == 2: # single unicode
            uni = eval('0x' + row[1][2:])
            if level == 3:
                if uni < 0x10000:
                    m = decmap3
                elif 0x20000 <= uni < 0x30000:
                    uni -= 0x20000
                    m = decmap3_2
            elif level == 4:
                if uni < 0x10000:
                    m = decmap4
                elif 0x20000 <= uni < 0x30000:
                    uni -= 0x20000
                    m = decmap4_2
            m.setdefault((loc >> 8), {})
            m[(loc >> 8)][(loc & 0xff)] = uni
        else: # pair
            uniprefix = eval('0x' + row[1][2:6]) # body
            uni = eval('0x' + row[1][7:11]) # modifier
            if level != 3:
                raise ValueError("invalid map")
            decmap3_pair.setdefault(uniprefix, {})
            m = decmap3_pair[uniprefix]

        if m is None:
            raise ValueError("invalid map")
        m.setdefault((loc >> 8), {})
        m[(loc >> 8)][(loc & 0xff)] = uni

    return decmap3, decmap4, decmap3_2, decmap4_2, decmap3_pair


def main():
    jisx0208file = open_mapping_file('python-mappings/JIS0208.TXT', MAPPINGS_JIS0208)
    jisx0212file = open_mapping_file('python-mappings/JIS0212.TXT', MAPPINGS_JIS0212)
    cp932file = open_mapping_file('python-mappings/CP932.TXT', MAPPINGS_CP932)
    jisx0213file = open_mapping_file('python-mappings/jisx0213-2004-std.txt', MAPPINGS_JISX0213_2004)

    print("Loading Mapping File...")

    sjisdecmap = loadmap(jisx0208file, natcol=0, unicol=2)
    jisx0208decmap = loadmap(jisx0208file, natcol=1, unicol=2)
    jisx0212decmap = loadmap(jisx0212file)
    cp932decmap = loadmap(cp932file)
    jis3decmap, jis4decmap, jis3_2_decmap, jis4_2_decmap, jis3_pairdecmap = loadmap_jisx0213(jisx0213file)

    if jis3decmap[0x21][0x24] != 0xff0c:
        raise SystemExit('Please adjust your JIS X 0213 map using jisx0213-2000-std.txt.diff')

    sjisencmap, cp932encmap = {}, {}
    jisx0208_0212encmap = {}
    for c1, m in sjisdecmap.items():
        for c2, code in m.items():
            sjisencmap.setdefault(code >> 8, {})
            sjisencmap[code >> 8][code & 0xff] = c1 << 8 | c2
    for c1, m in cp932decmap.items():
        for c2, code in m.items():
            cp932encmap.setdefault(code >> 8, {})
            if (code & 0xff) not in cp932encmap[code >> 8]:
                cp932encmap[code >> 8][code & 0xff] = c1 << 8 | c2
    for c1, m in cp932encmap.copy().items():
        for c2, code in m.copy().items():
            if c1 in sjisencmap and c2 in sjisencmap[c1] and sjisencmap[c1][c2] == code:
                del cp932encmap[c1][c2]
                if not cp932encmap[c1]:
                    del cp932encmap[c1]

    jisx0213pairdecmap = {}
    jisx0213pairencmap = []
    for unibody, m1 in jis3_pairdecmap.items():
        for c1, m2 in m1.items():
            for c2, modifier in m2.items():
                jisx0213pairencmap.append((unibody, modifier, c1 << 8 | c2))
                jisx0213pairdecmap.setdefault(c1, {})
                jisx0213pairdecmap[c1][c2] = unibody << 16 | modifier

    # Twinmap for both of JIS X 0208 (MSB unset) and JIS X 0212 (MSB set)
    for c1, m in jisx0208decmap.items():
        for c2, code in m.items():
            jisx0208_0212encmap.setdefault(code >> 8, {})
            jisx0208_0212encmap[code >> 8][code & 0xff] = c1 << 8 | c2

    for c1, m in jisx0212decmap.items():
        for c2, code in m.items():
            jisx0208_0212encmap.setdefault(code >> 8, {})
            if (code & 0xff) in jisx0208_0212encmap[code >> 8]:
                print("OOPS!!!", (code))
            jisx0208_0212encmap[code >> 8][code & 0xff] = 0x8000 | c1 << 8 | c2

    jisx0213bmpencmap = {}
    for c1, m in jis3decmap.copy().items():
        for c2, code in m.copy().items():
            if c1 in jisx0208decmap and c2 in jisx0208decmap[c1]:
                if code in jis3_pairdecmap:
                    jisx0213bmpencmap[code >> 8][code & 0xff] = (0,) # pair
                    jisx0213pairencmap.append((code, 0, c1 << 8 | c2))
                elif jisx0208decmap[c1][c2] == code:
                    del jis3decmap[c1][c2]
                    if not jis3decmap[c1]:
                        del jis3decmap[c1]
                else:
                    raise ValueError("Difference between JIS X 0208 and JIS X 0213 Plane 1 is found.")
            else:
                jisx0213bmpencmap.setdefault(code >> 8, {})
                if code not in jis3_pairdecmap:
                    jisx0213bmpencmap[code >> 8][code & 0xff] = c1 << 8 | c2
                else:
                    jisx0213bmpencmap[code >> 8][code & 0xff] = (0,) # pair
                    jisx0213pairencmap.append((code, 0, c1 << 8 | c2))

    for c1, m in jis4decmap.items():
        for c2, code in m.items():
            jisx0213bmpencmap.setdefault(code >> 8, {})
            jisx0213bmpencmap[code >> 8][code & 0xff] = 0x8000 | c1 << 8 | c2

    jisx0213empencmap = {}
    for c1, m in jis3_2_decmap.items():
        for c2, code in m.items():
            jisx0213empencmap.setdefault(code >> 8, {})
            jisx0213empencmap[code >> 8][code & 0xff] = c1 << 8 | c2
    for c1, m in jis4_2_decmap.items():
        for c2, code in m.items():
            jisx0213empencmap.setdefault(code >> 8, {})
            jisx0213empencmap[code >> 8][code & 0xff] = 0x8000 | c1 << 8 | c2

    with open("mappings_jp.h", "w") as fp:
        print_autogen(fp, os.path.basename(__file__))
        print("Generating JIS X 0208 decode map...")
        writer = DecodeMapWriter(fp, "jisx0208", jisx0208decmap)
        writer.update_decode_map(JISX0208_C1, JISX0208_C2)
        writer.generate()

        print("Generating JIS X 0212 decode map...")
        writer = DecodeMapWriter(fp, "jisx0212", jisx0212decmap)
        writer.update_decode_map(JISX0212_C1, JISX0212_C2)
        writer.generate()

        print("Generating JIS X 0208 && JIS X 0212 encode map...")
        writer = EncodeMapWriter(fp, "jisxcommon", jisx0208_0212encmap)
        writer.generate()

        print("Generating CP932 Extension decode map...")
        writer = DecodeMapWriter(fp, "cp932ext", cp932decmap)
        writer.update_decode_map(CP932P0_C1, CP932P0_C2)
        writer.update_decode_map(CP932P1_C1, CP932P1_C2)
        writer.update_decode_map(CP932P2_C1, CP932P2_C2)
        writer.generate()

        print("Generating CP932 Extension encode map...")
        writer = EncodeMapWriter(fp, "cp932ext", cp932encmap)
        writer.generate()

        print("Generating JIS X 0213 Plane 1 BMP decode map...")
        writer = DecodeMapWriter(fp, "jisx0213_1_bmp", jis3decmap)
        writer.update_decode_map(JISX0213_C1, JISX0213_C2)
        writer.generate()

        print("Generating JIS X 0213 Plane 2 BMP decode map...")
        writer = DecodeMapWriter(fp, "jisx0213_2_bmp", jis4decmap)
        writer.update_decode_map(JISX0213_C1, JISX0213_C2)
        writer.generate()

        print("Generating JIS X 0213 BMP encode map...")
        writer = EncodeMapWriter(fp, "jisx0213_bmp", jisx0213bmpencmap)
        writer.generate()

        print("Generating JIS X 0213 Plane 1 EMP decode map...")
        writer = DecodeMapWriter(fp, "jisx0213_1_emp", jis3_2_decmap)
        writer.update_decode_map(JISX0213_C1, JISX0213_C2)
        writer.generate()

        print("Generating JIS X 0213 Plane 2 EMP decode map...")
        writer = DecodeMapWriter(fp, "jisx0213_2_emp", jis4_2_decmap)
        writer.update_decode_map(JISX0213_C1, JISX0213_C2)
        writer.generate()

        print("Generating JIS X 0213 EMP encode map...")
        writer = EncodeMapWriter(fp, "jisx0213_emp", jisx0213empencmap)
        writer.generate()

    with open('mappings_jisx0213_pair.h', 'w') as fp:
        print_autogen(fp, os.path.basename(__file__))
        fp.write(f"#define JISX0213_ENCPAIRS {len(jisx0213pairencmap)}\n")
        fp.write("""\
#ifdef EXTERN_JISX0213_PAIR
static const struct widedbcs_index *jisx0213_pair_decmap;
static const struct pair_encodemap *jisx0213_pair_encmap;
#else
""")

        print("Generating JIS X 0213 unicode-pair decode map...")
        writer = DecodeMapWriter(fp, "jisx0213_pair", jisx0213pairdecmap)
        writer.update_decode_map(JISX0213_C1, JISX0213_C2)
        writer.generate(wide=True)

        print("Generating JIS X 0213 unicode-pair encode map...")
        jisx0213pairencmap.sort()
        fp.write("static const struct pair_encodemap jisx0213_pair_encmap[JISX0213_ENCPAIRS] = {\n")
        filler = BufferedFiller()
        for body, modifier, jis in jisx0213pairencmap:
            filler.write('{', '0x%04x%04x,' % (body, modifier), '0x%04x' % jis, '},')
        filler.printout(fp)
        fp.write("};\n")
        fp.write("#endif\n")

    print("Done!")

if __name__ == '__main__':
    main()


================================================
File: /Tools/unicode/genmap_korean.py
================================================
#
# genmap_korean.py: Korean Codecs Map Generator
#
# Original Author:  Hye-Shik Chang <perky@FreeBSD.org>
# Modified Author:  Donghee Na <donghee.na92@gmail.com>
#
import os

from genmap_support import *


KSX1001_C1 = (0x21, 0x7e)
KSX1001_C2 = (0x21, 0x7e)
UHCL1_C1 = (0x81, 0xa0)
UHCL1_C2 = (0x41, 0xfe)
UHCL2_C1 = (0xa1, 0xfe)
UHCL2_C2 = (0x41, 0xa0)
MAPPINGS_CP949 = 'http://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/CP949.TXT'


def main():
    mapfile = open_mapping_file('python-mappings/CP949.TXT', MAPPINGS_CP949)
    print("Loading Mapping File...")
    decmap = loadmap(mapfile)
    uhcdecmap, ksx1001decmap, cp949encmap = {}, {}, {}
    for c1, c2map in decmap.items():
        for c2, code in c2map.items():
            if c1 >= 0xa1 and c2 >= 0xa1:
                ksx1001decmap.setdefault(c1 & 0x7f, {})
                ksx1001decmap[c1 & 0x7f][c2 & 0x7f] = c2map[c2]
                cp949encmap.setdefault(code >> 8, {})
                cp949encmap[code >> 8][code & 0xFF] = (c1 << 8 | c2) & 0x7f7f
            else:
                # uhc
                uhcdecmap.setdefault(c1, {})
                uhcdecmap[c1][c2] = c2map[c2]
                cp949encmap.setdefault(code >> 8, {})  # MSB set
                cp949encmap[code >> 8][code & 0xFF] = (c1 << 8 | c2)

    with open('mappings_kr.h', 'w') as fp:
        print_autogen(fp, os.path.basename(__file__))

        print("Generating KS X 1001 decode map...")
        writer = DecodeMapWriter(fp, "ksx1001", ksx1001decmap)
        writer.update_decode_map(KSX1001_C1, KSX1001_C2)
        writer.generate()

        print("Generating UHC decode map...")
        writer = DecodeMapWriter(fp, "cp949ext", uhcdecmap)
        writer.update_decode_map(UHCL1_C1, UHCL1_C2)
        writer.update_decode_map(UHCL2_C1, UHCL2_C2)
        writer.generate()

        print("Generating CP949 (includes KS X 1001) encode map...")
        writer = EncodeMapWriter(fp, "cp949", cp949encmap)
        writer.generate()

    print("Done!")


if __name__ == '__main__':
    main()


================================================
File: /Tools/unicode/genmap_schinese.py
================================================
#
# genmap_schinese.py: Simplified Chinese Codecs Map Generator
#
# Original Author:  Hye-Shik Chang <perky@FreeBSD.org>
# Modified Author:  Donghee Na <donghee.na92@gmail.com>
#
import os
import re

from genmap_support import *


GB2312_C1   = (0x21, 0x7e)
GB2312_C2   = (0x21, 0x7e)
GBKL1_C1    = (0x81, 0xa8)
GBKL1_C2    = (0x40, 0xfe)
GBKL2_C1    = (0xa9, 0xfe)
GBKL2_C2    = (0x40, 0xa0)
GB18030EXTP1_C1 = (0xa1, 0xa9)
GB18030EXTP1_C2 = (0x40, 0xfe)
GB18030EXTP2_C1 = (0xaa, 0xaf)
GB18030EXTP2_C2 = (0xa1, 0xfe)
GB18030EXTP3_C1 = (0xd7, 0xd7)
GB18030EXTP3_C2 = (0xfa, 0xfe)
GB18030EXTP4_C1 = (0xf8, 0xfd)
GB18030EXTP4_C2 = (0xa1, 0xfe)
GB18030EXTP5_C1 = (0xfe, 0xfe)
GB18030EXTP5_C2 = (0x50, 0xfe)

MAPPINGS_GB2312 = 'http://people.freebsd.org/~perky/i18n/GB2312.TXT'
MAPPINGS_CP936 = 'http://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/CP936.TXT'
MAPPINGS_GB18030 = 'http://oss.software.ibm.com/cvs/icu/~checkout~/charset/data/xml/gb-18030-2000.xml'

re_gb18030ass = re.compile('<a u="([A-F0-9]{4})" b="([0-9A-F ]+)"/>')


def parse_gb18030map(fo):
    m, gbuni = {}, {}
    for i in range(65536):
        if i < 0xd800 or i > 0xdfff: # exclude unicode surrogate area
            gbuni[i] = None
    for uni, native in re_gb18030ass.findall(fo.read()):
        uni = eval('0x'+uni)
        native = [eval('0x'+u) for u in native.split()]
        if len(native) <= 2:
            del gbuni[uni]
        if len(native) == 2: # we can decode algorithmically for 1 or 4 bytes
            m.setdefault(native[0], {})
            m[native[0]][native[1]] = uni
    gbuni = [k for k in gbuni.keys()]
    gbuni.sort()
    return m, gbuni

def main():
    print("Loading Mapping File...")
    gb2312map = open_mapping_file('python-mappings/GB2312.TXT', MAPPINGS_GB2312)
    cp936map = open_mapping_file('python-mappings/CP936.TXT', MAPPINGS_CP936)
    gb18030map = open_mapping_file('python-mappings/gb-18030-2000.xml', MAPPINGS_GB18030)

    gb18030decmap, gb18030unilinear = parse_gb18030map(gb18030map)
    gbkdecmap = loadmap(cp936map)
    gb2312decmap = loadmap(gb2312map)
    difmap = {}
    for c1, m in gbkdecmap.items():
        for c2, code in m.items():
            del gb18030decmap[c1][c2]
            if not gb18030decmap[c1]:
                del gb18030decmap[c1]
    for c1, m in gb2312decmap.items():
        for c2, code in m.items():
            gbkc1, gbkc2 = c1 | 0x80, c2 | 0x80
            if gbkdecmap[gbkc1][gbkc2] == code:
                del gbkdecmap[gbkc1][gbkc2]
                if not gbkdecmap[gbkc1]:
                    del gbkdecmap[gbkc1]

    gb2312_gbkencmap, gb18030encmap = {}, {}
    for c1, m in gbkdecmap.items():
        for c2, code in m.items():
            gb2312_gbkencmap.setdefault(code >> 8, {})
            gb2312_gbkencmap[code >> 8][code & 0xff] = c1 << 8 | c2 # MSB set
    for c1, m in gb2312decmap.items():
        for c2, code in m.items():
            gb2312_gbkencmap.setdefault(code >> 8, {})
            gb2312_gbkencmap[code >> 8][code & 0xff] = c1 << 8 | c2 # MSB unset
    for c1, m in gb18030decmap.items():
        for c2, code in m.items():
            gb18030encmap.setdefault(code >> 8, {})
            gb18030encmap[code >> 8][code & 0xff] = c1 << 8 | c2

    with open('mappings_cn.h', 'w') as fp:
        print_autogen(fp, os.path.basename(__file__))

        print("Generating GB2312 decode map...")
        writer = DecodeMapWriter(fp, "gb2312", gb2312decmap)
        writer.update_decode_map(GB2312_C1, GB2312_C2)
        writer.generate()

        print("Generating GBK decode map...")
        writer = DecodeMapWriter(fp, "gbkext", gbkdecmap)
        writer.update_decode_map(GBKL1_C1, GBKL1_C2)
        writer.update_decode_map(GBKL2_C1, GBKL2_C2)
        writer.generate()

        print("Generating GB2312 && GBK encode map...")
        writer = EncodeMapWriter(fp, "gbcommon", gb2312_gbkencmap)
        writer.generate()

        print("Generating GB18030 extension decode map...")
        writer = DecodeMapWriter(fp, "gb18030ext", gb18030decmap)
        for i in range(1, 6):
            writer.update_decode_map(eval("GB18030EXTP%d_C1" % i), eval("GB18030EXTP%d_C2" % i))

        writer.generate()

        print("Generating GB18030 extension encode map...")
        writer = EncodeMapWriter(fp, "gb18030ext", gb18030encmap)
        writer.generate()

        print("Generating GB18030 Unicode BMP Mapping Ranges...")
        ranges = [[-1, -1, -1]]
        gblinnum = 0
        fp.write("""
static const struct _gb18030_to_unibmp_ranges {
    Py_UCS4   first, last;
    DBCHAR       base;
} gb18030_to_unibmp_ranges[] = {
""")

        for uni in gb18030unilinear:
            if uni == ranges[-1][1] + 1:
                ranges[-1][1] = uni
            else:
                ranges.append([uni, uni, gblinnum])
            gblinnum += 1

        filler = BufferedFiller()
        for first, last, base in ranges[1:]:
            filler.write('{', str(first), ',', str(last), ',', str(base), '},')

        filler.write('{', '0,', '0,', str(
            ranges[-1][2] + ranges[-1][1] - ranges[-1][0] + 1), '}', '};')
        filler.printout(fp)

    print("Done!")


if __name__ == '__main__':
    main()


================================================
File: /Tools/unicode/genmap_support.py
================================================
#
# genmap_support.py: Multibyte Codec Map Generator
#
# Original Author:  Hye-Shik Chang <perky@FreeBSD.org>
# Modified Author:  Donghee Na <donghee.na92@gmail.com>
#


class BufferedFiller:
    def __init__(self, column=78):
        self.column = column
        self.buffered = []
        self.cline = []
        self.clen = 0
        self.count = 0

    def write(self, *data):
        for s in data:
            if len(s) > self.column:
                raise ValueError("token is too long")
            if len(s) + self.clen > self.column:
                self.flush()
            self.clen += len(s)
            self.cline.append(s)
            self.count += 1

    def flush(self):
        if not self.cline:
            return
        self.buffered.append(''.join(self.cline))
        self.clen = 0
        del self.cline[:]

    def printout(self, fp):
        self.flush()
        for l in self.buffered:
            fp.write(f'{l}\n')
        del self.buffered[:]

    def __len__(self):
        return self.count


class DecodeMapWriter:
    filler_class = BufferedFiller

    def __init__(self, fp, prefix, decode_map):
        self.fp = fp
        self.prefix = prefix
        self.decode_map = decode_map
        self.filler = self.filler_class()

    def update_decode_map(self, c1range, c2range, onlymask=(), wide=0):
        c2values = range(c2range[0], c2range[1] + 1)

        for c1 in range(c1range[0], c1range[1] + 1):
            if c1 not in self.decode_map or (onlymask and c1 not in onlymask):
                continue
            c2map = self.decode_map[c1]
            rc2values = [n for n in c2values if n in c2map]
            if not rc2values:
                continue

            c2map[self.prefix] = True
            c2map['min'] = rc2values[0]
            c2map['max'] = rc2values[-1]
            c2map['midx'] = len(self.filler)

            for v in range(rc2values[0], rc2values[-1] + 1):
                if v in c2map:
                    self.filler.write('%d,' % c2map[v])
                else:
                    self.filler.write('U,')

    def generate(self, wide=False):
        if not wide:
            self.fp.write(f"static const ucs2_t __{self.prefix}_decmap[{len(self.filler)}] = {{\n")
        else:
            self.fp.write(f"static const Py_UCS4 __{self.prefix}_decmap[{len(self.filler)}] = {{\n")

        self.filler.printout(self.fp)
        self.fp.write("};\n\n")

        if not wide:
            self.fp.write(f"static const struct dbcs_index {self.prefix}_decmap[256] = {{\n")
        else:
            self.fp.write(f"static const struct widedbcs_index {self.prefix}_decmap[256] = {{\n")

        for i in range(256):
            if i in self.decode_map and self.prefix in self.decode_map[i]:
                m = self.decode_map
                prefix = self.prefix
            else:
                self.filler.write("{", "0,", "0,", "0", "},")
                continue

            self.filler.write("{", "__%s_decmap" % prefix, "+", "%d" % m[i]['midx'],
                              ",", "%d," % m[i]['min'], "%d" % m[i]['max'], "},")
        self.filler.printout(self.fp)
        self.fp.write("};\n\n")


class EncodeMapWriter:
    filler_class = BufferedFiller
    elemtype = 'DBCHAR'
    indextype = 'struct unim_index'

    def __init__(self, fp, prefix, encode_map):
        self.fp = fp
        self.prefix = prefix
        self.encode_map = encode_map
        self.filler = self.filler_class()

    def generate(self):
        self.buildmap()
        self.printmap()

    def buildmap(self):
        for c1 in range(0, 256):
            if c1 not in self.encode_map:
                continue
            c2map = self.encode_map[c1]
            rc2values = [k for k in c2map.keys()]
            rc2values.sort()
            if not rc2values:
                continue

            c2map[self.prefix] = True
            c2map['min'] = rc2values[0]
            c2map['max'] = rc2values[-1]
            c2map['midx'] = len(self.filler)

            for v in range(rc2values[0], rc2values[-1] + 1):
                if v not in c2map:
                    self.write_nochar()
                elif isinstance(c2map[v], int):
                    self.write_char(c2map[v])
                elif isinstance(c2map[v], tuple):
                    self.write_multic(c2map[v])
                else:
                    raise ValueError

    def write_nochar(self):
        self.filler.write('N,')

    def write_multic(self, point):
        self.filler.write('M,')

    def write_char(self, point):
        self.filler.write(str(point) + ',')

    def printmap(self):
        self.fp.write(f"static const {self.elemtype} __{self.prefix}_encmap[{len(self.filler)}] = {{\n")
        self.filler.printout(self.fp)
        self.fp.write("};\n\n")
        self.fp.write(f"static const {self.indextype} {self.prefix}_encmap[256] = {{\n")

        for i in range(256):
            if i in self.encode_map and self.prefix in self.encode_map[i]:
                self.filler.write("{", "__%s_encmap" % self.prefix, "+",
                                  "%d" % self.encode_map[i]['midx'], ",",
                                  "%d," % self.encode_map[i]['min'],
                                  "%d" % self.encode_map[i]['max'], "},")
            else:
                self.filler.write("{", "0,", "0,", "0", "},")
                continue
        self.filler.printout(self.fp)
        self.fp.write("};\n\n")


def open_mapping_file(path, source):
    try:
        f = open(path)
    except IOError:
        raise SystemExit(f'{source} is needed')
    return f


def print_autogen(fo, source):
    fo.write(f'// AUTO-GENERATED FILE FROM {source}: DO NOT EDIT\n')


def loadmap(fo, natcol=0, unicol=1, sbcs=0):
    print("Loading from", fo)
    fo.seek(0, 0)
    decmap = {}
    for line in fo:
        line = line.split('#', 1)[0].strip()
        if not line or len(line.split()) < 2:
            continue

        row = [eval(e) for e in line.split()]
        loc, uni = row[natcol], row[unicol]
        if loc >= 0x100 or sbcs:
            decmap.setdefault((loc >> 8), {})
            decmap[(loc >> 8)][(loc & 0xff)] = uni

    return decmap


================================================
File: /Tools/unicode/genmap_tchinese.py
================================================
#
# genmap_tchinese.py: Traditional Chinese Codecs Map Generator
#
# Original Author:  Hye-Shik Chang <perky@FreeBSD.org>
#
import os

from genmap_support import *


# ranges for (lead byte, follower byte)
BIG5_C1 = (0xa1, 0xfe)
BIG5_C2 = (0x40, 0xfe)
BIG5HKSCS_C1 = (0x87, 0xfe)
BIG5HKSCS_C2 = (0x40, 0xfe)

MAPPINGS_BIG5 = 'https://unicode.org/Public/MAPPINGS/OBSOLETE/EASTASIA/OTHER/BIG5.TXT'
MAPPINGS_CP950 = 'https://www.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/WINDOWS/CP950.TXT'

HKSCS_VERSION = '2004'
# The files for HKSCS mappings are available under a restrictive license.
# Users of the script need to download the files from the HKSARG CCLI website:
MAPPINGS_HKSCS = f'https://www.ccli.gov.hk/en/archive/terms_hkscs-{HKSCS_VERSION}-big5-iso.html'


def bh2s(code):
    return ((code >> 8) - 0x87) * (0xfe - 0x40 + 1) + ((code & 0xff) - 0x40)


def split_bytes(code):
    """Split 0xABCD into 0xAB, 0xCD"""
    return code >> 8, code & 0xff


def parse_hkscs_map(fo):
    fo.seek(0, 0)
    table = []
    for line in fo:
        line = line.split('#', 1)[0].strip()
        # We expect 4 columns in supported HKSCS files:
        # [1999]: unsupported
        # [2001]: unsupported
        # [2004]: Big-5; iso10646-1:1993; iso10646-1:2000; iso10646:2003+amd1
        # [2008]: Big-5; iso10646-1:1993; iso10646-1:2000; iso10646:2003+amd6
        # [2016]: not supported here--uses a json file instead
        #
        # In both supported cases, we only need the first and last column:
        #  * Big-5 is a hex string (always 4 digits)
        #  * iso10646:2003 is either a hex string (4 or 5 digits) or a sequence
        #    of hex strings like: `<code_point1,code_point2>`
        try:
            hkscs_col, _, _, uni_col = line.split()
            hkscs = int(hkscs_col, 16)
            seq = tuple(int(cp, 16) for cp in uni_col.strip('<>').split(','))
        except ValueError:
            continue
        table.append((hkscs, seq))
    return table


def make_hkscs_map(table):
    decode_map = {}
    encode_map_bmp, encode_map_notbmp = {}, {}
    is_bmp_map = {}
    sequences = []
    beginnings = {}
    single_cp_table = []
    # Determine multi-codepoint sequences, and sequence beginnings that encode
    # multiple multibyte (i.e. Big-5) codes.
    for mbcode, cp_seq in table:
        cp, *_ = cp_seq
        if len(cp_seq) == 1:
            single_cp_table.append((mbcode, cp))
        else:
            sequences.append((mbcode, cp_seq))
        beginnings.setdefault(cp, []).append(mbcode)
    # Decode table only cares about single code points (no sequences) currently
    for mbcode, cp in single_cp_table:
        b1, b2 = split_bytes(mbcode)
        decode_map.setdefault(b1, {})
        decode_map[b1][b2] = cp & 0xffff
    # Encode table needs to mark code points beginning a sequence as tuples.
    for cp, mbcodes in beginnings.items():
        plane = cp >> 16
        if plane == 0:
            encode_map = encode_map_bmp
        elif plane == 2:
            encode_map = encode_map_notbmp
            is_bmp_map[bh2s(mbcodes[0])] = 1
        else:
            assert False, 'only plane 0 (BMP) and plane 2 (SIP) allowed'
        if len(mbcodes) == 1:
            encode_value = mbcodes[0]
        else:
            encode_value = tuple(mbcodes)
        uni_b1, uni_b2 = split_bytes(cp & 0xffff)
        encode_map.setdefault(uni_b1, {})
        encode_map[uni_b1][uni_b2] = encode_value

    return decode_map, encode_map_bmp, encode_map_notbmp, is_bmp_map


def load_big5_map():
    mapfile = open_mapping_file('python-mappings/BIG5.txt', MAPPINGS_BIG5)
    with mapfile:
        big5decmap = loadmap(mapfile)
    # big5 mapping fix: use the cp950 mapping for these characters as the file
    # provided by unicode.org doesn't define a mapping. See notes in BIG5.txt.
    # Since U+5341, U+5345, U+FF0F, U+FF3C already have a big5 mapping, no
    # roundtrip compatibility is guaranteed for those.
    for m in """\
    0xA15A      0x2574
    0xA1C3      0xFFE3
    0xA1C5      0x02CD
    0xA1FE      0xFF0F
    0xA240      0xFF3C
    0xA2CC      0x5341
    0xA2CE      0x5345""".splitlines():
        bcode, ucode = list(map(eval, m.split()))
        big5decmap[bcode >> 8][bcode & 0xff] = ucode
    # encoding map
    big5encmap = {}
    for c1, m in list(big5decmap.items()):
        for c2, code in list(m.items()):
            big5encmap.setdefault(code >> 8, {})
            if code & 0xff not in big5encmap[code >> 8]:
                big5encmap[code >> 8][code & 0xff] = c1 << 8 | c2
    # fix unicode->big5 priority for the above-mentioned duplicate characters
    big5encmap[0xFF][0x0F] = 0xA241
    big5encmap[0xFF][0x3C] = 0xA242
    big5encmap[0x53][0x41] = 0xA451
    big5encmap[0x53][0x45] = 0xA4CA

    return big5decmap, big5encmap


def load_cp950_map():
    mapfile = open_mapping_file('python-mappings/CP950.TXT', MAPPINGS_CP950)
    with mapfile:
        cp950decmap = loadmap(mapfile)
    cp950encmap = {}
    for c1, m in list(cp950decmap.items()):
        for c2, code in list(m.items()):
            cp950encmap.setdefault(code >> 8, {})
            if code & 0xff not in cp950encmap[code >> 8]:
                cp950encmap[code >> 8][code & 0xff] = c1 << 8 | c2
    # fix unicode->big5 duplicated mapping priority
    cp950encmap[0x53][0x41] = 0xA451
    cp950encmap[0x53][0x45] = 0xA4CA
    return cp950decmap, cp950encmap


def main_tw():
    big5decmap, big5encmap = load_big5_map()
    cp950decmap, cp950encmap = load_cp950_map()

    # CP950 extends Big5, and the codec can use the Big5 lookup tables
    # for most entries. So the CP950 tables should only include entries
    # that are not in Big5:
    for c1, m in list(cp950encmap.items()):
        for c2, code in list(m.items()):
            if (c1 in big5encmap and c2 in big5encmap[c1]
                    and big5encmap[c1][c2] == code):
                del cp950encmap[c1][c2]
    for c1, m in list(cp950decmap.items()):
        for c2, code in list(m.items()):
            if (c1 in big5decmap and c2 in big5decmap[c1]
                    and big5decmap[c1][c2] == code):
                del cp950decmap[c1][c2]

    with open('mappings_tw.h', 'w') as fp:
        print_autogen(fp, os.path.basename(__file__))
        write_big5_maps(fp, 'BIG5', 'big5', big5decmap, big5encmap)
        write_big5_maps(fp, 'CP950', 'cp950ext', cp950decmap, cp950encmap)


def write_big5_maps(fp, display_name, table_name, decode_map, encode_map):
    print(f'Generating {display_name} decode map...')
    writer = DecodeMapWriter(fp, table_name, decode_map)
    writer.update_decode_map(BIG5_C1, BIG5_C2)
    writer.generate()
    print(f'Generating {display_name} encode map...')
    writer = EncodeMapWriter(fp, table_name, encode_map)
    writer.generate()


class HintsWriter:
    filler_class = BufferedFiller

    def __init__(self, fp, prefix, isbmpmap):
        self.fp = fp
        self.prefix = prefix
        self.isbmpmap = isbmpmap
        self.filler = self.filler_class()

    def fillhints(self, hintfrom, hintto):
        name = f'{self.prefix}_phint_{hintfrom}'
        self.fp.write(f'static const unsigned char {name}[] = {{\n')
        for msbcode in range(hintfrom, hintto+1, 8):
            v = 0
            for c in range(msbcode, msbcode+8):
                v |= self.isbmpmap.get(c, 0) << (c - msbcode)
            self.filler.write('%d,' % v)
        self.filler.printout(self.fp)
        self.fp.write('};\n\n')


def main_hkscs():
    filename = f'python-mappings/hkscs-{HKSCS_VERSION}-big5-iso.txt'
    with open_mapping_file(filename, MAPPINGS_HKSCS) as f:
        table = parse_hkscs_map(f)
    hkscsdecmap, hkscsencmap_bmp, hkscsencmap_nonbmp, isbmpmap = (
        make_hkscs_map(table)
    )
    with open('mappings_hk.h', 'w') as fp:
        print('Generating BIG5HKSCS decode map...')
        print_autogen(fp, os.path.basename(__file__))
        writer = DecodeMapWriter(fp, 'big5hkscs', hkscsdecmap)
        writer.update_decode_map(BIG5HKSCS_C1, BIG5HKSCS_C2)
        writer.generate()

        print('Generating BIG5HKSCS decode map Unicode plane hints...')
        writer = HintsWriter(fp, 'big5hkscs', isbmpmap)
        writer.fillhints(bh2s(0x8740), bh2s(0xa0fe))
        writer.fillhints(bh2s(0xc6a1), bh2s(0xc8fe))
        writer.fillhints(bh2s(0xf9d6), bh2s(0xfefe))

        print('Generating BIG5HKSCS encode map (BMP)...')
        writer = EncodeMapWriter(fp, 'big5hkscs_bmp', hkscsencmap_bmp)
        writer.generate()

        print('Generating BIG5HKSCS encode map (non-BMP)...')
        writer = EncodeMapWriter(fp, 'big5hkscs_nonbmp', hkscsencmap_nonbmp)
        writer.generate()


if __name__ == '__main__':
    main_tw()
    main_hkscs()


================================================
File: /Tools/unicode/genwincodec.py
================================================
"""This script generates a Python codec module from a Windows Code Page.

It uses the function MultiByteToWideChar to generate a decoding table.
"""

import ctypes
from ctypes import wintypes
from gencodec import codegen
import unicodedata

def genwinmap(codepage):
    MultiByteToWideChar = ctypes.windll.kernel32.MultiByteToWideChar
    MultiByteToWideChar.argtypes = [wintypes.UINT, wintypes.DWORD,
                                    wintypes.LPCSTR, ctypes.c_int,
                                    wintypes.LPWSTR, ctypes.c_int]
    MultiByteToWideChar.restype = ctypes.c_int

    enc2uni = {}

    for i in list(range(32)) + [127]:
        enc2uni[i] = (i, 'CONTROL CHARACTER')

    for i in range(256):
        buf = ctypes.create_unicode_buffer(2)
        ret = MultiByteToWideChar(
            codepage, 0,
            bytes([i]), 1,
            buf, 2)
        assert ret == 1, "invalid code page"
        assert buf[1] == '\x00'
        try:
            name = unicodedata.name(buf[0])
        except ValueError:
            try:
                name = enc2uni[i][1]
            except KeyError:
                name = ''

        enc2uni[i] = (ord(buf[0]), name)

    return enc2uni

def genwincodec(codepage):
    import platform
    map = genwinmap(codepage)
    encodingname = 'cp%d' % codepage
    code = codegen("", map, encodingname)
    # Replace first lines with our own docstring
    code = '''\
"""Python Character Mapping Codec %s generated on Windows:
%s with the command:
  python Tools/unicode/genwincodec.py %s
"""#"
''' % (encodingname, ' '.join(platform.win32_ver()), codepage
      ) + code.split('"""#"', 1)[1]

    print(code)

if __name__ == '__main__':
    import sys
    genwincodec(int(sys.argv[1]))


================================================
File: /Tools/unicode/genwincodecs.bat
================================================
@rem Recreate some python charmap codecs from the Windows function
@rem MultiByteToWideChar.

@cd /d %~dp0
@mkdir build
@rem Arabic DOS code page
c:\python30\python genwincodec.py 720 > build/cp720.py


================================================
File: /Tools/unicode/listcodecs.py
================================================
""" List all available codec modules.

(c) Copyright 2005, Marc-Andre Lemburg (mal@lemburg.com).

    Licensed to PSF under a Contributor Agreement.

"""

import os, codecs, encodings

_debug = 0

def listcodecs(dir):
    names = []
    for filename in os.listdir(dir):
        if filename[-3:] != '.py':
            continue
        name = filename[:-3]
        # Check whether we've found a true codec
        try:
            codecs.lookup(name)
        except LookupError:
            # Codec not found
            continue
        except Exception as reason:
            # Probably an error from importing the codec; still it's
            # a valid code name
            if _debug:
                print('* problem importing codec %r: %s' % \
                      (name, reason))
        names.append(name)
    return names


if __name__ == '__main__':
    names = listcodecs(encodings.__path__[0])
    names.sort()
    print('all_codecs = [')
    for name in names:
        print('    %r,' % name)
    print(']')


================================================
File: /Tools/unicode/makeunicodedata.py
================================================
#
# (re)generate unicode property and type databases
#
# This script converts Unicode database files to Modules/unicodedata_db.h,
# Modules/unicodename_db.h, and Objects/unicodetype_db.h
#
# history:
# 2000-09-24 fl   created (based on bits and pieces from unidb)
# 2000-09-25 fl   merged tim's splitbin fixes, separate decomposition table
# 2000-09-25 fl   added character type table
# 2000-09-26 fl   added LINEBREAK, DECIMAL, and DIGIT flags/fields (2.0)
# 2000-11-03 fl   expand first/last ranges
# 2001-01-19 fl   added character name tables (2.1)
# 2001-01-21 fl   added decomp compression; dynamic phrasebook threshold
# 2002-09-11 wd   use string methods
# 2002-10-18 mvl  update to Unicode 3.2
# 2002-10-22 mvl  generate NFC tables
# 2002-11-24 mvl  expand all ranges, sort names version-independently
# 2002-11-25 mvl  add UNIDATA_VERSION
# 2004-05-29 perky add east asian width information
# 2006-03-10 mvl  update to Unicode 4.1; add UCD 3.2 delta
# 2008-06-11 gb   add PRINTABLE_MASK for Atsuo Ishimoto's ascii() patch
# 2011-10-21 ezio add support for name aliases and named sequences
# 2012-01    benjamin add full case mappings
#
# written by Fredrik Lundh (fredrik@pythonware.com)
#

import dataclasses
import os
import sys
import zipfile

from functools import partial
from textwrap import dedent
from typing import Iterator, List, Optional, Set, Tuple

SCRIPT = os.path.normpath(sys.argv[0])
VERSION = "3.3"

# The Unicode Database
# --------------------
# When changing UCD version please update
#   * Doc/library/stdtypes.rst, and
#   * Doc/library/unicodedata.rst
#   * Doc/reference/lexical_analysis.rst (two occurrences)
UNIDATA_VERSION = "16.0.0"
UNICODE_DATA = "UnicodeData%s.txt"
COMPOSITION_EXCLUSIONS = "CompositionExclusions%s.txt"
EASTASIAN_WIDTH = "EastAsianWidth%s.txt"
UNIHAN = "Unihan%s.zip"
DERIVED_CORE_PROPERTIES = "DerivedCoreProperties%s.txt"
DERIVEDNORMALIZATION_PROPS = "DerivedNormalizationProps%s.txt"
LINE_BREAK = "LineBreak%s.txt"
NAME_ALIASES = "NameAliases%s.txt"
NAMED_SEQUENCES = "NamedSequences%s.txt"
SPECIAL_CASING = "SpecialCasing%s.txt"
CASE_FOLDING = "CaseFolding%s.txt"

# Private Use Areas -- in planes 1, 15, 16
PUA_1 = range(0xE000, 0xF900)
PUA_15 = range(0xF0000, 0xFFFFE)
PUA_16 = range(0x100000, 0x10FFFE)

# we use this ranges of PUA_15 to store name aliases and named sequences
NAME_ALIASES_START = 0xF0000
NAMED_SEQUENCES_START = 0xF0200

old_versions = ["3.2.0"]

CATEGORY_NAMES = [ "Cn", "Lu", "Ll", "Lt", "Mn", "Mc", "Me", "Nd",
    "Nl", "No", "Zs", "Zl", "Zp", "Cc", "Cf", "Cs", "Co", "Cn", "Lm",
    "Lo", "Pc", "Pd", "Ps", "Pe", "Pi", "Pf", "Po", "Sm", "Sc", "Sk",
    "So" ]

BIDIRECTIONAL_NAMES = [ "", "L", "LRE", "LRO", "R", "AL", "RLE", "RLO",
    "PDF", "EN", "ES", "ET", "AN", "CS", "NSM", "BN", "B", "S", "WS",
    "ON", "LRI", "RLI", "FSI", "PDI" ]

# "N" needs to be the first entry, see the comment in makeunicodedata
EASTASIANWIDTH_NAMES = [ "N", "H", "W", "Na", "A", "F" ]

MANDATORY_LINE_BREAKS = [ "BK", "CR", "LF", "NL" ]

# note: should match definitions in Objects/unicodectype.c
ALPHA_MASK = 0x01
DECIMAL_MASK = 0x02
DIGIT_MASK = 0x04
LOWER_MASK = 0x08
LINEBREAK_MASK = 0x10
SPACE_MASK = 0x20
TITLE_MASK = 0x40
UPPER_MASK = 0x80
XID_START_MASK = 0x100
XID_CONTINUE_MASK = 0x200
PRINTABLE_MASK = 0x400
NUMERIC_MASK = 0x800
CASE_IGNORABLE_MASK = 0x1000
CASED_MASK = 0x2000
EXTENDED_CASE_MASK = 0x4000

# these ranges need to match unicodedata.c:is_unified_ideograph
cjk_ranges = [
    ('3400', '4DBF'),    # CJK Ideograph Extension A CJK
    ('4E00', '9FFF'),    # CJK Ideograph
    ('20000', '2A6DF'),  # CJK Ideograph Extension B
    ('2A700', '2B739'),  # CJK Ideograph Extension C
    ('2B740', '2B81D'),  # CJK Ideograph Extension D
    ('2B820', '2CEA1'),  # CJK Ideograph Extension E
    ('2CEB0', '2EBE0'),  # CJK Ideograph Extension F
    ('2EBF0', '2EE5D'),  # CJK Ideograph Extension I
    ('30000', '3134A'),  # CJK Ideograph Extension G
    ('31350', '323AF'),  # CJK Ideograph Extension H
]


def maketables(trace=0):

    print("--- Reading", UNICODE_DATA % "", "...")

    unicode = UnicodeData(UNIDATA_VERSION)

    print(len(list(filter(None, unicode.table))), "characters")

    for version in old_versions:
        print("--- Reading", UNICODE_DATA % ("-"+version), "...")
        old_unicode = UnicodeData(version, cjk_check=False)
        print(len(list(filter(None, old_unicode.table))), "characters")
        merge_old_version(version, unicode, old_unicode)

    makeunicodename(unicode, trace)
    makeunicodedata(unicode, trace)
    makeunicodetype(unicode, trace)


# --------------------------------------------------------------------
# unicode character properties

def makeunicodedata(unicode, trace):

    # the default value of east_asian_width is "N", for unassigned code points
    # not mentioned in EastAsianWidth.txt
    # in addition there are some reserved but unassigned code points in CJK
    # ranges that are classified as "W". code points in private use areas
    # have a width of "A". both of these have entries in
    # EastAsianWidth.txt
    # see https://unicode.org/reports/tr11/#Unassigned
    assert EASTASIANWIDTH_NAMES[0] == "N"
    dummy = (0, 0, 0, 0, 0, 0)
    table = [dummy]
    cache = {0: dummy}
    index = [0] * len(unicode.chars)

    FILE = "Modules/unicodedata_db.h"

    print("--- Preparing", FILE, "...")

    # 1) database properties

    for char in unicode.chars:
        record = unicode.table[char]
        if record:
            # extract database properties
            category = CATEGORY_NAMES.index(record.general_category)
            combining = int(record.canonical_combining_class)
            bidirectional = BIDIRECTIONAL_NAMES.index(record.bidi_class)
            mirrored = record.bidi_mirrored == "Y"
            eastasianwidth = EASTASIANWIDTH_NAMES.index(record.east_asian_width)
            normalizationquickcheck = record.quick_check
            item = (
                category, combining, bidirectional, mirrored, eastasianwidth,
                normalizationquickcheck
                )
        elif unicode.widths[char] is not None:
            # an unassigned but reserved character, with a known
            # east_asian_width
            eastasianwidth = EASTASIANWIDTH_NAMES.index(unicode.widths[char])
            item = (0, 0, 0, 0, eastasianwidth, 0)
        else:
            continue

        # add entry to index and item tables
        i = cache.get(item)
        if i is None:
            cache[item] = i = len(table)
            table.append(item)
        index[char] = i

    # 2) decomposition data

    decomp_data_cache = {}
    decomp_data = [0]
    decomp_prefix = [""]
    decomp_index = [0] * len(unicode.chars)
    decomp_size = 0

    comp_pairs = []
    comp_first = [None] * len(unicode.chars)
    comp_last = [None] * len(unicode.chars)

    for char in unicode.chars:
        record = unicode.table[char]
        if record:
            if record.decomposition_type:
                decomp = record.decomposition_type.split()
                if len(decomp) > 19:
                    raise Exception("character %x has a decomposition too large for nfd_nfkd" % char)
                # prefix
                if decomp[0][0] == "<":
                    prefix = decomp.pop(0)
                else:
                    prefix = ""
                try:
                    i = decomp_prefix.index(prefix)
                except ValueError:
                    i = len(decomp_prefix)
                    decomp_prefix.append(prefix)
                prefix = i
                assert prefix < 256
                # content
                decomp = [prefix + (len(decomp)<<8)] + [int(s, 16) for s in decomp]
                # Collect NFC pairs
                if not prefix and len(decomp) == 3 and \
                   char not in unicode.exclusions and \
                   unicode.table[decomp[1]].canonical_combining_class == "0":
                    p, l, r = decomp
                    comp_first[l] = 1
                    comp_last[r] = 1
                    comp_pairs.append((l,r,char))
                key = tuple(decomp)
                i = decomp_data_cache.get(key, -1)
                if i == -1:
                    i = len(decomp_data)
                    decomp_data.extend(decomp)
                    decomp_size = decomp_size + len(decomp) * 2
                    decomp_data_cache[key] = i
                else:
                    assert decomp_data[i:i+len(decomp)] == decomp
            else:
                i = 0
            decomp_index[char] = i

    f = l = 0
    comp_first_ranges = []
    comp_last_ranges = []
    prev_f = prev_l = None
    for i in unicode.chars:
        if comp_first[i] is not None:
            comp_first[i] = f
            f += 1
            if prev_f is None:
                prev_f = (i,i)
            elif prev_f[1]+1 == i:
                prev_f = prev_f[0],i
            else:
                comp_first_ranges.append(prev_f)
                prev_f = (i,i)
        if comp_last[i] is not None:
            comp_last[i] = l
            l += 1
            if prev_l is None:
                prev_l = (i,i)
            elif prev_l[1]+1 == i:
                prev_l = prev_l[0],i
            else:
                comp_last_ranges.append(prev_l)
                prev_l = (i,i)
    comp_first_ranges.append(prev_f)
    comp_last_ranges.append(prev_l)
    total_first = f
    total_last = l

    comp_data = [0]*(total_first*total_last)
    for f,l,char in comp_pairs:
        f = comp_first[f]
        l = comp_last[l]
        comp_data[f*total_last+l] = char

    print(len(table), "unique properties")
    print(len(decomp_prefix), "unique decomposition prefixes")
    print(len(decomp_data), "unique decomposition entries:", end=' ')
    print(decomp_size, "bytes")
    print(total_first, "first characters in NFC")
    print(total_last, "last characters in NFC")
    print(len(comp_pairs), "NFC pairs")

    print("--- Writing", FILE, "...")

    with open(FILE, "w") as fp:
        fprint = partial(print, file=fp)

        fprint("/* this file was generated by %s %s */" % (SCRIPT, VERSION))
        fprint()
        fprint('#define UNIDATA_VERSION "%s"' % UNIDATA_VERSION)
        fprint("/* a list of unique database records */")
        fprint("const _PyUnicode_DatabaseRecord _PyUnicode_Database_Records[] = {")
        for item in table:
            fprint("    {%d, %d, %d, %d, %d, %d}," % item)
        fprint("};")
        fprint()

        fprint("/* Reindexing of NFC first characters. */")
        fprint("#define TOTAL_FIRST",total_first)
        fprint("#define TOTAL_LAST",total_last)
        fprint("struct reindex{int start;short count,index;};")
        fprint("static struct reindex nfc_first[] = {")
        for start,end in comp_first_ranges:
            fprint("    { %d, %d, %d}," % (start,end-start,comp_first[start]))
        fprint("    {0,0,0}")
        fprint("};\n")
        fprint("static struct reindex nfc_last[] = {")
        for start,end in comp_last_ranges:
            fprint("  { %d, %d, %d}," % (start,end-start,comp_last[start]))
        fprint("  {0,0,0}")
        fprint("};\n")

        # FIXME: <fl> the following tables could be made static, and
        # the support code moved into unicodedatabase.c

        fprint("/* string literals */")
        fprint("const char *_PyUnicode_CategoryNames[] = {")
        for name in CATEGORY_NAMES:
            fprint("    \"%s\"," % name)
        fprint("    NULL")
        fprint("};")

        fprint("const char *_PyUnicode_BidirectionalNames[] = {")
        for name in BIDIRECTIONAL_NAMES:
            fprint("    \"%s\"," % name)
        fprint("    NULL")
        fprint("};")

        fprint("const char *_PyUnicode_EastAsianWidthNames[] = {")
        for name in EASTASIANWIDTH_NAMES:
            fprint("    \"%s\"," % name)
        fprint("    NULL")
        fprint("};")

        fprint("static const char *decomp_prefix[] = {")
        for name in decomp_prefix:
            fprint("    \"%s\"," % name)
        fprint("    NULL")
        fprint("};")

        # split record index table
        index1, index2, shift = splitbins(index, trace)

        fprint("/* index tables for the database records */")
        fprint("#define SHIFT", shift)
        Array("index1", index1).dump(fp, trace)
        Array("index2", index2).dump(fp, trace)

        # split decomposition index table
        index1, index2, shift = splitbins(decomp_index, trace)

        fprint("/* decomposition data */")
        Array("decomp_data", decomp_data).dump(fp, trace)

        fprint("/* index tables for the decomposition data */")
        fprint("#define DECOMP_SHIFT", shift)
        Array("decomp_index1", index1).dump(fp, trace)
        Array("decomp_index2", index2).dump(fp, trace)

        index, index2, shift = splitbins(comp_data, trace)
        fprint("/* NFC pairs */")
        fprint("#define COMP_SHIFT", shift)
        Array("comp_index", index).dump(fp, trace)
        Array("comp_data", index2).dump(fp, trace)

        # Generate delta tables for old versions
        for version, table, normalization in unicode.changed:
            cversion = version.replace(".","_")
            records = [table[0]]
            cache = {table[0]:0}
            index = [0] * len(table)
            for i, record in enumerate(table):
                try:
                    index[i] = cache[record]
                except KeyError:
                    index[i] = cache[record] = len(records)
                    records.append(record)
            index1, index2, shift = splitbins(index, trace)
            fprint("static const change_record change_records_%s[] = {" % cversion)
            for record in records:
                fprint("    { %s }," % ", ".join(map(str,record)))
            fprint("};")
            Array("changes_%s_index" % cversion, index1).dump(fp, trace)
            Array("changes_%s_data" % cversion, index2).dump(fp, trace)
            fprint("static const change_record* get_change_%s(Py_UCS4 n)" % cversion)
            fprint("{")
            fprint("    int index;")
            fprint("    if (n >= 0x110000) index = 0;")
            fprint("    else {")
            fprint("        index = changes_%s_index[n>>%d];" % (cversion, shift))
            fprint("        index = changes_%s_data[(index<<%d)+(n & %d)];" % \
                   (cversion, shift, ((1<<shift)-1)))
            fprint("    }")
            fprint("    return change_records_%s+index;" % cversion)
            fprint("}\n")
            fprint("static Py_UCS4 normalization_%s(Py_UCS4 n)" % cversion)
            fprint("{")
            fprint("    switch(n) {")
            for k, v in normalization:
                fprint("    case %s: return 0x%s;" % (hex(k), v))
            fprint("    default: return 0;")
            fprint("    }\n}\n")


# --------------------------------------------------------------------
# unicode character type tables

def makeunicodetype(unicode, trace):

    FILE = "Objects/unicodetype_db.h"

    print("--- Preparing", FILE, "...")

    # extract unicode types
    dummy = (0, 0, 0, 0, 0, 0)
    table = [dummy]
    cache = {dummy: 0}
    index = [0] * len(unicode.chars)
    numeric = {}
    spaces = []
    linebreaks = []
    extra_casing = []

    for char in unicode.chars:
        record = unicode.table[char]
        if record:
            # extract database properties
            category = record.general_category
            bidirectional = record.bidi_class
            properties = record.binary_properties
            flags = 0
            if category in ["Lm", "Lt", "Lu", "Ll", "Lo"]:
                flags |= ALPHA_MASK
            if "Lowercase" in properties:
                flags |= LOWER_MASK
            if 'Line_Break' in properties or bidirectional == "B":
                flags |= LINEBREAK_MASK
                linebreaks.append(char)
            if category == "Zs" or bidirectional in ("WS", "B", "S"):
                flags |= SPACE_MASK
                spaces.append(char)
            if category == "Lt":
                flags |= TITLE_MASK
            if "Uppercase" in properties:
                flags |= UPPER_MASK
            if char == ord(" ") or category[0] not in ("C", "Z"):
                flags |= PRINTABLE_MASK
            if "XID_Start" in properties:
                flags |= XID_START_MASK
            if "XID_Continue" in properties:
                flags |= XID_CONTINUE_MASK
            if "Cased" in properties:
                flags |= CASED_MASK
            if "Case_Ignorable" in properties:
                flags |= CASE_IGNORABLE_MASK
            sc = unicode.special_casing.get(char)
            cf = unicode.case_folding.get(char, [char])
            if record.simple_uppercase_mapping:
                upper = int(record.simple_uppercase_mapping, 16)
            else:
                upper = char
            if record.simple_lowercase_mapping:
                lower = int(record.simple_lowercase_mapping, 16)
            else:
                lower = char
            if record.simple_titlecase_mapping:
                title = int(record.simple_titlecase_mapping, 16)
            else:
                title = upper
            if sc is None and cf != [lower]:
                sc = ([lower], [title], [upper])
            if sc is None:
                if upper == lower == title:
                    upper = lower = title = 0
                else:
                    upper = upper - char
                    lower = lower - char
                    title = title - char
                    assert (abs(upper) <= 2147483647 and
                            abs(lower) <= 2147483647 and
                            abs(title) <= 2147483647)
            else:
                # This happens either when some character maps to more than one
                # character in uppercase, lowercase, or titlecase or the
                # casefolded version of the character is different from the
                # lowercase. The extra characters are stored in a different
                # array.
                flags |= EXTENDED_CASE_MASK
                lower = len(extra_casing) | (len(sc[0]) << 24)
                extra_casing.extend(sc[0])
                if cf != sc[0]:
                    lower |= len(cf) << 20
                    extra_casing.extend(cf)
                upper = len(extra_casing) | (len(sc[2]) << 24)
                extra_casing.extend(sc[2])
                # Title is probably equal to upper.
                if sc[1] == sc[2]:
                    title = upper
                else:
                    title = len(extra_casing) | (len(sc[1]) << 24)
                    extra_casing.extend(sc[1])
            # decimal digit, integer digit
            decimal = 0
            if record.decomposition_mapping:
                flags |= DECIMAL_MASK
                decimal = int(record.decomposition_mapping)
            digit = 0
            if record.numeric_type:
                flags |= DIGIT_MASK
                digit = int(record.numeric_type)
            if record.numeric_value:
                flags |= NUMERIC_MASK
                numeric.setdefault(record.numeric_value, []).append(char)
            item = (
                upper, lower, title, decimal, digit, flags
                )
            # add entry to index and item tables
            i = cache.get(item)
            if i is None:
                cache[item] = i = len(table)
                table.append(item)
            index[char] = i

    print(len(table), "unique character type entries")
    print(sum(map(len, numeric.values())), "numeric code points")
    print(len(spaces), "whitespace code points")
    print(len(linebreaks), "linebreak code points")
    print(len(extra_casing), "extended case array")

    print("--- Writing", FILE, "...")

    with open(FILE, "w") as fp:
        fprint = partial(print, file=fp)

        fprint("/* this file was generated by %s %s */" % (SCRIPT, VERSION))
        fprint()
        fprint("/* a list of unique character type descriptors */")
        fprint("const _PyUnicode_TypeRecord _PyUnicode_TypeRecords[] = {")
        for item in table:
            fprint("    {%d, %d, %d, %d, %d, %d}," % item)
        fprint("};")
        fprint()

        fprint("/* extended case mappings */")
        fprint()
        fprint("const Py_UCS4 _PyUnicode_ExtendedCase[] = {")
        for c in extra_casing:
            fprint("    %d," % c)
        fprint("};")
        fprint()

        # split decomposition index table
        index1, index2, shift = splitbins(index, trace)

        fprint("/* type indexes */")
        fprint("#define SHIFT", shift)
        Array("index1", index1).dump(fp, trace)
        Array("index2", index2).dump(fp, trace)

        # Generate code for _PyUnicode_ToNumeric()
        numeric_items = sorted(numeric.items())
        fprint('/* Returns the numeric value as double for Unicode characters')
        fprint(' * having this property, -1.0 otherwise.')
        fprint(' */')
        fprint('double _PyUnicode_ToNumeric(Py_UCS4 ch)')
        fprint('{')
        fprint('    switch (ch) {')
        for value, codepoints in numeric_items:
            # Turn text into float literals
            parts = value.split('/')
            parts = [repr(float(part)) for part in parts]
            value = '/'.join(parts)

            codepoints.sort()
            for codepoint in codepoints:
                fprint('    case 0x%04X:' % (codepoint,))
            fprint('        return (double) %s;' % (value,))
        fprint('    }')
        fprint('    return -1.0;')
        fprint('}')
        fprint()

        # Generate code for _PyUnicode_IsWhitespace()
        fprint("/* Returns 1 for Unicode characters having the bidirectional")
        fprint(" * type 'WS', 'B' or 'S' or the category 'Zs', 0 otherwise.")
        fprint(" */")
        fprint('int _PyUnicode_IsWhitespace(const Py_UCS4 ch)')
        fprint('{')
        fprint('    switch (ch) {')

        for codepoint in sorted(spaces):
            fprint('    case 0x%04X:' % (codepoint,))
        fprint('        return 1;')

        fprint('    }')
        fprint('    return 0;')
        fprint('}')
        fprint()

        # Generate code for _PyUnicode_IsLinebreak()
        fprint("/* Returns 1 for Unicode characters having the line break")
        fprint(" * property 'BK', 'CR', 'LF' or 'NL' or having bidirectional")
        fprint(" * type 'B', 0 otherwise.")
        fprint(" */")
        fprint('int _PyUnicode_IsLinebreak(const Py_UCS4 ch)')
        fprint('{')
        fprint('    switch (ch) {')
        for codepoint in sorted(linebreaks):
            fprint('    case 0x%04X:' % (codepoint,))
        fprint('        return 1;')

        fprint('    }')
        fprint('    return 0;')
        fprint('}')
        fprint()


# --------------------------------------------------------------------
# unicode name database

def makeunicodename(unicode, trace):
    from dawg import build_compression_dawg

    FILE = "Modules/unicodename_db.h"

    print("--- Preparing", FILE, "...")

    # unicode name hash table

    # extract names
    data = []
    for char in unicode.chars:
        record = unicode.table[char]
        if record:
            name = record.name.strip()
            if name and name[0] != "<":
                data.append((name, char))

    print("--- Writing", FILE, "...")

    with open(FILE, "w") as fp:
        fprint = partial(print, file=fp)

        fprint("/* this file was generated by %s %s */" % (SCRIPT, VERSION))
        fprint()
        fprint("#define NAME_MAXLEN", 256)
        assert max(len(x) for x in data) < 256
        fprint()

        fprint("/* name->code dictionary */")
        packed_dawg, pos_to_codepoint = build_compression_dawg(data)
        notfound = len(pos_to_codepoint)
        inverse_list = [notfound] * len(unicode.chars)
        for pos, codepoint in enumerate(pos_to_codepoint):
            inverse_list[codepoint] = pos
        Array("packed_name_dawg", list(packed_dawg)).dump(fp, trace)
        Array("dawg_pos_to_codepoint", pos_to_codepoint).dump(fp, trace)
        index1, index2, shift = splitbins(inverse_list, trace)
        fprint("#define DAWG_CODEPOINT_TO_POS_SHIFT", shift)
        fprint("#define DAWG_CODEPOINT_TO_POS_NOTFOUND", notfound)
        Array("dawg_codepoint_to_pos_index1", index1).dump(fp, trace)
        Array("dawg_codepoint_to_pos_index2", index2).dump(fp, trace)

        fprint()
        fprint('static const unsigned int aliases_start = %#x;' %
               NAME_ALIASES_START)
        fprint('static const unsigned int aliases_end = %#x;' %
               (NAME_ALIASES_START + len(unicode.aliases)))

        fprint('static const unsigned int name_aliases[] = {')
        for name, codepoint in unicode.aliases:
            fprint('    0x%04X,' % codepoint)
        fprint('};')

        # In Unicode 6.0.0, the sequences contain at most 4 BMP chars,
        # so we are using Py_UCS2 seq[4].  This needs to be updated if longer
        # sequences or sequences with non-BMP chars are added.
        # unicodedata_lookup should be adapted too.
        fprint(dedent("""
            typedef struct NamedSequence {
                int seqlen;
                Py_UCS2 seq[4];
            } named_sequence;
            """))

        fprint('static const unsigned int named_sequences_start = %#x;' %
               NAMED_SEQUENCES_START)
        fprint('static const unsigned int named_sequences_end = %#x;' %
               (NAMED_SEQUENCES_START + len(unicode.named_sequences)))

        fprint('static const named_sequence named_sequences[] = {')
        for name, sequence in unicode.named_sequences:
            seq_str = ', '.join('0x%04X' % cp for cp in sequence)
            fprint('    {%d, {%s}},' % (len(sequence), seq_str))
        fprint('};')


def merge_old_version(version, new, old):
    # Changes to exclusion file not implemented yet
    if old.exclusions != new.exclusions:
        raise NotImplementedError("exclusions differ")

    # In these change records, 0xFF means "no change"
    bidir_changes = [0xFF]*0x110000
    category_changes = [0xFF]*0x110000
    decimal_changes = [0xFF]*0x110000
    mirrored_changes = [0xFF]*0x110000
    east_asian_width_changes = [0xFF]*0x110000
    # In numeric data, 0 means "no change",
    # -1 means "did not have a numeric value
    numeric_changes = [0] * 0x110000
    # normalization_changes is a list of key-value pairs
    normalization_changes = []
    for i in range(0x110000):
        if new.table[i] is None:
            # Characters unassigned in the new version ought to
            # be unassigned in the old one
            assert old.table[i] is None
            continue
        # check characters unassigned in the old version
        if old.table[i] is None:
            # category 0 is "unassigned"
            category_changes[i] = 0
            continue
        # check characters that differ
        if old.table[i] != new.table[i]:
            for k, field in enumerate(dataclasses.fields(UcdRecord)):
                value = getattr(old.table[i], field.name)
                new_value = getattr(new.table[i], field.name)
                if value != new_value:
                    if k == 1 and i in PUA_15:
                        # the name is not set in the old.table, but in the
                        # new.table we are using it for aliases and named seq
                        assert value == ''
                    elif k == 2:
                        category_changes[i] = CATEGORY_NAMES.index(value)
                    elif k == 4:
                        bidir_changes[i] = BIDIRECTIONAL_NAMES.index(value)
                    elif k == 5:
                        # We assume that all normalization changes are in 1:1 mappings
                        assert " " not in value
                        normalization_changes.append((i, value))
                    elif k == 6:
                        # we only support changes where the old value is a single digit
                        assert value in "0123456789"
                        decimal_changes[i] = int(value)
                    elif k == 8:
                        # Since 0 encodes "no change", the old value is better not 0
                        if not value:
                            numeric_changes[i] = -1
                        else:
                            numeric_changes[i] = float(value)
                            assert numeric_changes[i] not in (0, -1)
                    elif k == 9:
                        if value == 'Y':
                            mirrored_changes[i] = '1'
                        else:
                            mirrored_changes[i] = '0'
                    elif k == 11:
                        # change to ISO comment, ignore
                        pass
                    elif k == 12:
                        # change to simple uppercase mapping; ignore
                        pass
                    elif k == 13:
                        # change to simple lowercase mapping; ignore
                        pass
                    elif k == 14:
                        # change to simple titlecase mapping; ignore
                        pass
                    elif k == 15:
                        # change to east asian width
                        east_asian_width_changes[i] = EASTASIANWIDTH_NAMES.index(value)
                    elif k == 16:
                        # derived property changes; not yet
                        pass
                    elif k == 17:
                        # normalization quickchecks are not performed
                        # for older versions
                        pass
                    else:
                        class Difference(Exception):pass
                        raise Difference(hex(i), k, old.table[i], new.table[i])
    new.changed.append((version, list(zip(bidir_changes, category_changes,
                                          decimal_changes, mirrored_changes,
                                          east_asian_width_changes,
                                          numeric_changes)),
                        normalization_changes))


DATA_DIR = os.path.join('Tools', 'unicode', 'data')

def open_data(template, version):
    local = os.path.join(DATA_DIR, template % ('-'+version,))
    if not os.path.exists(local):
        import urllib.request
        if version == '3.2.0':
            # irregular url structure
            url = ('https://www.unicode.org/Public/3.2-Update/'+template) % ('-'+version,)
        else:
            url = ('https://www.unicode.org/Public/%s/ucd/'+template) % (version, '')
        os.makedirs(DATA_DIR, exist_ok=True)
        urllib.request.urlretrieve(url, filename=local)
    if local.endswith('.txt'):
        return open(local, encoding='utf-8')
    else:
        # Unihan.zip
        return open(local, 'rb')


def expand_range(char_range: str) -> Iterator[int]:
    '''
    Parses ranges of code points, as described in UAX #44:
      https://www.unicode.org/reports/tr44/#Code_Point_Ranges
    '''
    if '..' in char_range:
        first, last = [int(c, 16) for c in char_range.split('..')]
    else:
        first = last = int(char_range, 16)
    for char in range(first, last+1):
        yield char


class UcdFile:
    '''
    A file in the standard format of the UCD.

    See: https://www.unicode.org/reports/tr44/#Format_Conventions

    Note that, as described there, the Unihan data files have their
    own separate format.
    '''

    def __init__(self, template: str, version: str) -> None:
        self.template = template
        self.version = version

    def records(self) -> Iterator[List[str]]:
        with open_data(self.template, self.version) as file:
            for line in file:
                line = line.split('#', 1)[0].strip()
                if not line:
                    continue
                yield [field.strip() for field in line.split(';')]

    def __iter__(self) -> Iterator[List[str]]:
        return self.records()

    def expanded(self) -> Iterator[Tuple[int, List[str]]]:
        for record in self.records():
            char_range, rest = record[0], record[1:]
            for char in expand_range(char_range):
                yield char, rest


@dataclasses.dataclass
class UcdRecord:
    # 15 fields from UnicodeData.txt .  See:
    #   https://www.unicode.org/reports/tr44/#UnicodeData.txt
    codepoint: str
    name: str
    general_category: str
    canonical_combining_class: str
    bidi_class: str
    decomposition_type: str
    decomposition_mapping: str
    numeric_type: str
    numeric_value: str
    bidi_mirrored: str
    unicode_1_name: str  # obsolete
    iso_comment: str  # obsolete
    simple_uppercase_mapping: str
    simple_lowercase_mapping: str
    simple_titlecase_mapping: str

    # https://www.unicode.org/reports/tr44/#EastAsianWidth.txt
    east_asian_width: Optional[str]

    # Binary properties, as a set of those that are true.
    # Taken from multiple files:
    #   https://www.unicode.org/reports/tr44/#DerivedCoreProperties.txt
    #   https://www.unicode.org/reports/tr44/#LineBreak.txt
    binary_properties: Set[str]

    # The Quick_Check properties related to normalization:
    #   https://www.unicode.org/reports/tr44/#Decompositions_and_Normalization
    # We store them as a bitmask.
    quick_check: int


def from_row(row: List[str]) -> UcdRecord:
    return UcdRecord(*row, None, set(), 0)


# --------------------------------------------------------------------
# the following support code is taken from the unidb utilities
# Copyright (c) 1999-2000 by Secret Labs AB

# load a unicode-data file from disk

class UnicodeData:
    # table: List[Optional[UcdRecord]]  # index is codepoint; None means unassigned

    def __init__(self, version, cjk_check=True):
        self.changed = []
        table = [None] * 0x110000
        for s in UcdFile(UNICODE_DATA, version):
            char = int(s[0], 16)
            table[char] = from_row(s)

        cjk_ranges_found = []

        # expand first-last ranges
        field = None
        for i in range(0, 0x110000):
            # The file UnicodeData.txt has its own distinct way of
            # expressing ranges.  See:
            #   https://www.unicode.org/reports/tr44/#Code_Point_Ranges
            s = table[i]
            if s:
                if s.name[-6:] == "First>":
                    s.name = ""
                    field = dataclasses.astuple(s)[:15]
                elif s.name[-5:] == "Last>":
                    if s.name.startswith("<CJK Ideograph"):
                        cjk_ranges_found.append((field[0],
                                                 s.codepoint))
                    s.name = ""
                    field = None
            elif field:
                table[i] = from_row(('%X' % i,) + field[1:])
        if cjk_check and cjk_ranges != cjk_ranges_found:
            raise ValueError("CJK ranges deviate: have %r" % cjk_ranges_found)

        # public attributes
        self.filename = UNICODE_DATA % ''
        self.table = table
        self.chars = list(range(0x110000)) # unicode 3.2

        # check for name aliases and named sequences, see #12753
        # aliases and named sequences are not in 3.2.0
        if version != '3.2.0':
            self.aliases = []
            # store aliases in the Private Use Area 15, in range U+F0000..U+F00FF,
            # in order to take advantage of the compression and lookup
            # algorithms used for the other characters
            pua_index = NAME_ALIASES_START
            for char, name, abbrev in UcdFile(NAME_ALIASES, version):
                char = int(char, 16)
                self.aliases.append((name, char))
                # also store the name in the PUA 1
                self.table[pua_index].name = name
                pua_index += 1
            assert pua_index - NAME_ALIASES_START == len(self.aliases)

            self.named_sequences = []
            # store named sequences in the PUA 1, in range U+F0100..,
            # in order to take advantage of the compression and lookup
            # algorithms used for the other characters.

            assert pua_index < NAMED_SEQUENCES_START
            pua_index = NAMED_SEQUENCES_START
            for name, chars in UcdFile(NAMED_SEQUENCES, version):
                chars = tuple(int(char, 16) for char in chars.split())
                # check that the structure defined in makeunicodename is OK
                assert 2 <= len(chars) <= 4, "change the Py_UCS2 array size"
                assert all(c <= 0xFFFF for c in chars), ("use Py_UCS4 in "
                    "the NamedSequence struct and in unicodedata_lookup")
                self.named_sequences.append((name, chars))
                # also store these in the PUA 1
                self.table[pua_index].name = name
                pua_index += 1
            assert pua_index - NAMED_SEQUENCES_START == len(self.named_sequences)

        self.exclusions = {}
        for char, in UcdFile(COMPOSITION_EXCLUSIONS, version):
            char = int(char, 16)
            self.exclusions[char] = 1

        widths = [None] * 0x110000
        for char, (width,) in UcdFile(EASTASIAN_WIDTH, version).expanded():
            widths[char] = width

        for i in range(0, 0x110000):
            if table[i] is not None:
                table[i].east_asian_width = widths[i]
        self.widths = widths

        for char, (propname, *propinfo) in UcdFile(DERIVED_CORE_PROPERTIES, version).expanded():
            if propinfo:
                # this is not a binary property, ignore it
                continue

            if table[char]:
                # Some properties (e.g. Default_Ignorable_Code_Point)
                # apply to unassigned code points; ignore them
                table[char].binary_properties.add(propname)

        for char_range, value in UcdFile(LINE_BREAK, version):
            if value not in MANDATORY_LINE_BREAKS:
                continue
            for char in expand_range(char_range):
                table[char].binary_properties.add('Line_Break')

        # We only want the quickcheck properties
        # Format: NF?_QC; Y(es)/N(o)/M(aybe)
        # Yes is the default, hence only N and M occur
        # In 3.2.0, the format was different (NF?_NO)
        # The parsing will incorrectly determine these as
        # "yes", however, unicodedata.c will not perform quickchecks
        # for older versions, and no delta records will be created.
        quickchecks = [0] * 0x110000
        qc_order = 'NFD_QC NFKD_QC NFC_QC NFKC_QC'.split()
        for s in UcdFile(DERIVEDNORMALIZATION_PROPS, version):
            if len(s) < 2 or s[1] not in qc_order:
                continue
            quickcheck = 'MN'.index(s[2]) + 1 # Maybe or No
            quickcheck_shift = qc_order.index(s[1])*2
            quickcheck <<= quickcheck_shift
            for char in expand_range(s[0]):
                assert not (quickchecks[char]>>quickcheck_shift)&3
                quickchecks[char] |= quickcheck
        for i in range(0, 0x110000):
            if table[i] is not None:
                table[i].quick_check = quickchecks[i]

        with open_data(UNIHAN, version) as file:
            zip = zipfile.ZipFile(file)
            if version == '3.2.0':
                data = zip.open('Unihan-3.2.0.txt').read()
            else:
                data = zip.open('Unihan_NumericValues.txt').read()
        for line in data.decode("utf-8").splitlines():
            if not line.startswith('U+'):
                continue
            code, tag, value = line.split(None, 3)[:3]
            if tag not in ('kAccountingNumeric', 'kPrimaryNumeric',
                           'kOtherNumeric'):
                continue
            value = value.strip().replace(',', '')
            i = int(code[2:], 16)
            # Patch the numeric field
            if table[i] is not None:
                table[i].numeric_value = value

        sc = self.special_casing = {}
        for data in UcdFile(SPECIAL_CASING, version):
            if data[4]:
                # We ignore all conditionals (since they depend on
                # languages) except for one, which is hardcoded. See
                # handle_capital_sigma in unicodeobject.c.
                continue
            c = int(data[0], 16)
            lower = [int(char, 16) for char in data[1].split()]
            title = [int(char, 16) for char in data[2].split()]
            upper = [int(char, 16) for char in data[3].split()]
            sc[c] = (lower, title, upper)

        cf = self.case_folding = {}
        if version != '3.2.0':
            for data in UcdFile(CASE_FOLDING, version):
                if data[1] in "CF":
                    c = int(data[0], 16)
                    cf[c] = [int(char, 16) for char in data[2].split()]

    def uselatin1(self):
        # restrict character range to ISO Latin 1
        self.chars = list(range(256))



# stuff to deal with arrays of unsigned integers

class Array:

    def __init__(self, name, data):
        self.name = name
        self.data = data

    def dump(self, file, trace=0):
        # write data to file, as a C array
        size = getsize(self.data)
        if trace:
            print(self.name+":", size*len(self.data), "bytes", file=sys.stderr)
        file.write("static const ")
        if size == 1:
            file.write("unsigned char")
        elif size == 2:
            file.write("unsigned short")
        else:
            file.write("unsigned int")
        file.write(" " + self.name + "[] = {\n")
        if self.data:
            s = "    "
            for item in self.data:
                i = str(item) + ", "
                if len(s) + len(i) > 78:
                    file.write(s.rstrip() + "\n")
                    s = "    " + i
                else:
                    s = s + i
            if s.strip():
                file.write(s.rstrip() + "\n")
        file.write("};\n\n")


def getsize(data):
    # return smallest possible integer size for the given array
    maxdata = max(data)
    if maxdata < 256:
        return 1
    elif maxdata < 65536:
        return 2
    else:
        return 4


def splitbins(t, trace=0):
    """t, trace=0 -> (t1, t2, shift).  Split a table to save space.

    t is a sequence of ints.  This function can be useful to save space if
    many of the ints are the same.  t1 and t2 are lists of ints, and shift
    is an int, chosen to minimize the combined size of t1 and t2 (in C
    code), and where for each i in range(len(t)),
        t[i] == t2[(t1[i >> shift] << shift) + (i & mask)]
    where mask is a bitmask isolating the last "shift" bits.

    If optional arg trace is non-zero (default zero), progress info
    is printed to sys.stderr.  The higher the value, the more info
    you'll get.
    """

    if trace:
        def dump(t1, t2, shift, bytes):
            print("%d+%d bins at shift %d; %d bytes" % (
                len(t1), len(t2), shift, bytes), file=sys.stderr)
        print("Size of original table:", len(t)*getsize(t), "bytes",
              file=sys.stderr)
    n = len(t)-1    # last valid index
    maxshift = 0    # the most we can shift n and still have something left
    if n > 0:
        while n >> 1:
            n >>= 1
            maxshift += 1
    del n
    bytes = sys.maxsize  # smallest total size so far
    t = tuple(t)    # so slices can be dict keys
    for shift in range(maxshift + 1):
        t1 = []
        t2 = []
        size = 2**shift
        bincache = {}
        for i in range(0, len(t), size):
            bin = t[i:i+size]
            index = bincache.get(bin)
            if index is None:
                index = len(t2)
                bincache[bin] = index
                t2.extend(bin)
            t1.append(index >> shift)
        # determine memory size
        b = len(t1)*getsize(t1) + len(t2)*getsize(t2)
        if trace > 1:
            dump(t1, t2, shift, b)
        if b < bytes:
            best = t1, t2, shift
            bytes = b
    t1, t2, shift = best
    if trace:
        print("Best:", end=' ', file=sys.stderr)
        dump(t1, t2, shift, bytes)
    if __debug__:
        # exhaustively verify that the decomposition is correct
        mask = ~((~0) << shift) # i.e., low-bit mask of shift bits
        for i in range(len(t)):
            assert t[i] == t2[(t1[i >> shift] << shift) + (i & mask)]
    return best


if __name__ == "__main__":
    maketables(1)


================================================
File: /Tools/unicode/mkstringprep.py
================================================
import re
from unicodedata import ucd_3_2_0 as unicodedata

def gen_category(cats):
    for i in range(0, 0x110000):
        if unicodedata.category(chr(i)) in cats:
            yield(i)

def gen_bidirectional(cats):
    for i in range(0, 0x110000):
        if unicodedata.bidirectional(chr(i)) in cats:
            yield(i)

def compact_set(l):
    single = []
    tuple = []
    prev = None
    span = 0
    for e in l:
        if prev is None:
            prev = e
            span = 0
            continue
        if prev+span+1 != e:
            if span > 2:
                tuple.append((prev,prev+span+1))
            else:
                for i in range(prev, prev+span+1):
                    single.append(i)
            prev = e
            span = 0
        else:
            span += 1
    if span:
        tuple.append((prev,prev+span+1))
    else:
        single.append(prev)
    if not single and len(tuple) == 1:
        tuple = "range(%d,%d)" % tuple[0]
    else:
        tuple = " + ".join("list(range(%d,%d))" % t for t in tuple)
    if not single:
        return "set(%s)" % tuple
    if not tuple:
        return "set(%r)" % (single,)
    return "set(%r + %s)" % (single, tuple)

############## Read the tables in the RFC #######################

with open("rfc3454.txt") as f:
    data = f.readlines()

tables = []
curname = None
for l in data:
    l = l.strip()
    if not l:
        continue
    # Skip RFC page breaks
    if l.startswith(("Hoffman & Blanchet", "RFC 3454")):
        continue
    # Find start/end lines
    m = re.match("----- (Start|End) Table ([A-Z](.[0-9])+) -----", l)
    if m:
        if m.group(1) == "Start":
            if curname:
                raise RuntimeError("Double Start", (curname, l))
            curname = m.group(2)
            table = {}
            tables.append((curname, table))
            continue
        else:
            if not curname:
                raise RuntimeError("End without start", l)
            if curname != m.group(2):
                raise RuntimeError("Unexpected end", l)
            curname = None
            continue
    if not curname:
        continue
    # Now we are in a table
    fields = l.split(";")
    if len(fields) > 1:
        # Drop comment field
        fields = fields[:-1]
    if len(fields) == 1:
        fields = fields[0].split("-")
        if len(fields) > 1:
            # range
            try:
                start, end = fields
            except ValueError:
                raise RuntimeError("Unpacking problem", l)
        else:
            start = end = fields[0]
        start = int(start, 16)
        end = int(end, 16)
        for i in range(start, end+1):
            table[i] = i
    else:
        code, value = fields
        value = value.strip()
        if value:
            value = [int(v, 16) for v in value.split(" ")]
        else:
            # table B.1
            value = None
        table[int(code, 16)] = value

########### Generate compact Python versions of the tables #############

print("""# This file is generated by mkstringprep.py. DO NOT EDIT.
\"\"\"Library that exposes various tables found in the StringPrep RFC 3454.

There are two kinds of tables: sets, for which a member test is provided,
and mappings, for which a mapping function is provided.
\"\"\"

from unicodedata import ucd_3_2_0 as unicodedata
""")

print("assert unicodedata.unidata_version == %r" % (unicodedata.unidata_version,))

# A.1 is the table of unassigned characters
# XXX Plane 15 PUA is listed as unassigned in Python.
name, table = tables[0]
del tables[0]
assert name == "A.1"
table = set(table.keys())
Cn = set(gen_category(["Cn"]))

# FDD0..FDEF are process internal codes
Cn -= set(range(0xFDD0, 0xFDF0))
# not a character
Cn -= set(range(0xFFFE, 0x110000, 0x10000))
Cn -= set(range(0xFFFF, 0x110000, 0x10000))

# assert table == Cn

print("""
def in_table_a1(code):
    if unicodedata.category(code) != 'Cn': return False
    c = ord(code)
    if 0xFDD0 <= c < 0xFDF0: return False
    return (c & 0xFFFF) not in (0xFFFE, 0xFFFF)
""")

# B.1 cannot easily be derived
name, table = tables[0]
del tables[0]
assert name == "B.1"
table = sorted(table.keys())
print("""
b1_set = """ + compact_set(table) + """
def in_table_b1(code):
    return ord(code) in b1_set
""")

# B.2 and B.3 is case folding.
# It takes CaseFolding.txt into account, which is
# not available in the Python database. Since
# B.2 is derived from B.3, we process B.3 first.
# B.3 supposedly *is* CaseFolding-3.2.0.txt.

name, table_b2 = tables[0]
del tables[0]
assert name == "B.2"

name, table_b3 = tables[0]
del tables[0]
assert name == "B.3"

# B.3 is mostly Python's .lower, except for a number
# of special cases, e.g. considering canonical forms.

b3_exceptions = {}

for k,v in table_b2.items():
    if list(map(ord, chr(k).lower())) != v:
        b3_exceptions[k] = "".join(map(chr,v))

b3 = sorted(b3_exceptions.items())

print("""
b3_exceptions = {""")
for i, kv in enumerate(b3):
    print("0x%x:%a," % kv, end=' ')
    if i % 4 == 3:
        print()
print("}")

print("""
def map_table_b3(code):
    r = b3_exceptions.get(ord(code))
    if r is not None: return r
    return code.lower()
""")

def map_table_b3(code):
    r = b3_exceptions.get(ord(code))
    if r is not None: return r
    return code.lower()

# B.2 is case folding for NFKC. This is the same as B.3,
# except where NormalizeWithKC(Fold(a)) !=
# NormalizeWithKC(Fold(NormalizeWithKC(Fold(a))))

def map_table_b2(a):
    al = map_table_b3(a)
    b = unicodedata.normalize("NFKC", al)
    bl = "".join([map_table_b3(ch) for ch in b])
    c = unicodedata.normalize("NFKC", bl)
    if b != c:
        return c
    else:
        return al

specials = {}
for k,v in table_b2.items():
    if list(map(ord, map_table_b2(chr(k)))) != v:
        specials[k] = v

# B.3 should not add any additional special cases
assert specials == {}

print("""
def map_table_b2(a):
    al = map_table_b3(a)
    b = unicodedata.normalize("NFKC", al)
    bl = "".join([map_table_b3(ch) for ch in b])
    c = unicodedata.normalize("NFKC", bl)
    if b != c:
        return c
    else:
        return al
""")

# C.1.1 is a table with a single character
name, table = tables[0]
del tables[0]
assert name == "C.1.1"
assert table == {0x20:0x20}

print("""
def in_table_c11(code):
    return code == " "
""")

# C.1.2 is the rest of all space characters
name, table = tables[0]
del tables[0]
assert name == "C.1.2"

# table = set(table.keys())
# Zs = set(gen_category(["Zs"])) - {0x20}
# assert Zs == table

print("""
def in_table_c12(code):
    return unicodedata.category(code) == "Zs" and code != " "

def in_table_c11_c12(code):
    return unicodedata.category(code) == "Zs"
""")

# C.2.1 ASCII control characters
name, table_c21 = tables[0]
del tables[0]
assert name == "C.2.1"

Cc = set(gen_category(["Cc"]))
Cc_ascii = Cc & set(range(128))
table_c21 = set(table_c21.keys())
assert Cc_ascii == table_c21

print("""
def in_table_c21(code):
    return ord(code) < 128 and unicodedata.category(code) == "Cc"
""")

# C.2.2 Non-ASCII control characters. It also includes
# a number of characters in category Cf.
name, table_c22 = tables[0]
del tables[0]
assert name == "C.2.2"

Cc_nonascii = Cc - Cc_ascii
table_c22 = set(table_c22.keys())
assert len(Cc_nonascii - table_c22) == 0

specials = list(table_c22 - Cc_nonascii)
specials.sort()

print("""c22_specials = """ + compact_set(specials) + """
def in_table_c22(code):
    c = ord(code)
    if c < 128: return False
    if unicodedata.category(code) == "Cc": return True
    return c in c22_specials

def in_table_c21_c22(code):
    return unicodedata.category(code) == "Cc" or \\
           ord(code) in c22_specials
""")

# C.3 Private use
name, table = tables[0]
del tables[0]
assert name == "C.3"

Co = set(gen_category(["Co"]))
assert set(table.keys()) == Co

print("""
def in_table_c3(code):
    return unicodedata.category(code) == "Co"
""")

# C.4 Non-character code points, xFFFE, xFFFF
# plus process internal codes
name, table = tables[0]
del tables[0]
assert name == "C.4"

nonchar = set(range(0xFDD0,0xFDF0))
nonchar.update(range(0xFFFE,0x110000,0x10000))
nonchar.update(range(0xFFFF,0x110000,0x10000))
table = set(table.keys())
assert table == nonchar

print("""
def in_table_c4(code):
    c = ord(code)
    if c < 0xFDD0: return False
    if c < 0xFDF0: return True
    return (ord(code) & 0xFFFF) in (0xFFFE, 0xFFFF)
""")

# C.5 Surrogate codes
name, table = tables[0]
del tables[0]
assert name == "C.5"

Cs = set(gen_category(["Cs"]))
assert set(table.keys()) == Cs

print("""
def in_table_c5(code):
    return unicodedata.category(code) == "Cs"
""")

# C.6 Inappropriate for plain text
name, table = tables[0]
del tables[0]
assert name == "C.6"

table = sorted(table.keys())

print("""
c6_set = """ + compact_set(table) + """
def in_table_c6(code):
    return ord(code) in c6_set
""")

# C.7 Inappropriate for canonical representation
name, table = tables[0]
del tables[0]
assert name == "C.7"

table = sorted(table.keys())

print("""
c7_set = """ + compact_set(table) + """
def in_table_c7(code):
    return ord(code) in c7_set
""")

# C.8 Change display properties or are deprecated
name, table = tables[0]
del tables[0]
assert name == "C.8"

table = sorted(table.keys())

print("""
c8_set = """ + compact_set(table) + """
def in_table_c8(code):
    return ord(code) in c8_set
""")

# C.9 Tagging characters
name, table = tables[0]
del tables[0]
assert name == "C.9"

table = sorted(table.keys())

print("""
c9_set = """ + compact_set(table) + """
def in_table_c9(code):
    return ord(code) in c9_set
""")

# D.1 Characters with bidirectional property "R" or "AL"
name, table = tables[0]
del tables[0]
assert name == "D.1"

RandAL = set(gen_bidirectional(["R","AL"]))
assert set(table.keys()) == RandAL

print("""
def in_table_d1(code):
    return unicodedata.bidirectional(code) in ("R","AL")
""")

# D.2 Characters with bidirectional property "L"
name, table = tables[0]
del tables[0]
assert name == "D.2"

L = set(gen_bidirectional(["L"]))
assert set(table.keys()) == L

print("""
def in_table_d2(code):
    return unicodedata.bidirectional(code) == "L"
""")


================================================
File: /Tools/unicode/python-mappings/CP1140.TXT
================================================
#
#       Name:             CP1140
#	Unicode version:  3.2
#	Table version:    1.0
#	Table format:     Format A
#	Date:             2005-10-25
#	Authors:          Marc-Andre Lemburg <mal@egenix.com>
#
#       This encoding is a modified CP037 encoding (with added Euro
#       currency sign).
#
#       (c) Copyright Marc-Andre Lemburg, 2005.
#           Licensed to PSF under a Contributor Agreement.
#
#       Based on the file 
#       ftp://ftp.unicode.org/Public/MAPPINGS/VENDORS/MICSFT/EBCDIC/CP037.TXT
#       which is:
#
#	Copyright (c) 2002 Unicode, Inc.  All Rights reserved.
#
#	This file is provided as-is by Unicode, Inc. (The Unicode Consortium).
#	No claims are made as to fitness for any particular purpose.  No
#	warranties of any kind are expressed or implied.  The recipient
#	agrees to determine applicability of information provided.  If this
#	file has been provided on optical media by Unicode, Inc., the sole
#	remedy for any claim will be exchange of defective media within 90
#	days of receipt.
#
#	Unicode, Inc. hereby grants the right to freely use the information
#	supplied in this file in the creation of products supporting the
#	Unicode Standard, and to make copies of this file in any form for
#	internal or external distribution as long as this notice remains
#	attached.
#
0x00	0x0000	#NULL
0x01	0x0001	#START OF HEADING
0x02	0x0002	#START OF TEXT
0x03	0x0003	#END OF TEXT
0x04	0x009C	#CONTROL
0x05	0x0009	#HORIZONTAL TABULATION
0x06	0x0086	#CONTROL
0x07	0x007F	#DELETE
0x08	0x0097	#CONTROL
0x09	0x008D	#CONTROL
0x0A	0x008E	#CONTROL
0x0B	0x000B	#VERTICAL TABULATION
0x0C	0x000C	#FORM FEED
0x0D	0x000D	#CARRIAGE RETURN
0x0E	0x000E	#SHIFT OUT
0x0F	0x000F	#SHIFT IN
0x10	0x0010	#DATA LINK ESCAPE
0x11	0x0011	#DEVICE CONTROL ONE
0x12	0x0012	#DEVICE CONTROL TWO
0x13	0x0013	#DEVICE CONTROL THREE
0x14	0x009D	#CONTROL
0x15	0x0085	#CONTROL
0x16	0x0008	#BACKSPACE
0x17	0x0087	#CONTROL
0x18	0x0018	#CANCEL
0x19	0x0019	#END OF MEDIUM
0x1A	0x0092	#CONTROL
0x1B	0x008F	#CONTROL
0x1C	0x001C	#FILE SEPARATOR
0x1D	0x001D	#GROUP SEPARATOR
0x1E	0x001E	#RECORD SEPARATOR
0x1F	0x001F	#UNIT SEPARATOR
0x20	0x0080	#CONTROL
0x21	0x0081	#CONTROL
0x22	0x0082	#CONTROL
0x23	0x0083	#CONTROL
0x24	0x0084	#CONTROL
0x25	0x000A	#LINE FEED
0x26	0x0017	#END OF TRANSMISSION BLOCK
0x27	0x001B	#ESCAPE
0x28	0x0088	#CONTROL
0x29	0x0089	#CONTROL
0x2A	0x008A	#CONTROL
0x2B	0x008B	#CONTROL
0x2C	0x008C	#CONTROL
0x2D	0x0005	#ENQUIRY
0x2E	0x0006	#ACKNOWLEDGE
0x2F	0x0007	#BELL
0x30	0x0090	#CONTROL
0x31	0x0091	#CONTROL
0x32	0x0016	#SYNCHRONOUS IDLE
0x33	0x0093	#CONTROL
0x34	0x0094	#CONTROL
0x35	0x0095	#CONTROL
0x36	0x0096	#CONTROL
0x37	0x0004	#END OF TRANSMISSION
0x38	0x0098	#CONTROL
0x39	0x0099	#CONTROL
0x3A	0x009A	#CONTROL
0x3B	0x009B	#CONTROL
0x3C	0x0014	#DEVICE CONTROL FOUR
0x3D	0x0015	#NEGATIVE ACKNOWLEDGE
0x3E	0x009E	#CONTROL
0x3F	0x001A	#SUBSTITUTE
0x40	0x0020	#SPACE
0x41	0x00A0	#NO-BREAK SPACE
0x42	0x00E2	#LATIN SMALL LETTER A WITH CIRCUMFLEX
0x43	0x00E4	#LATIN SMALL LETTER A WITH DIAERESIS
0x44	0x00E0	#LATIN SMALL LETTER A WITH GRAVE
0x45	0x00E1	#LATIN SMALL LETTER A WITH ACUTE
0x46	0x00E3	#LATIN SMALL LETTER A WITH TILDE
0x47	0x00E5	#LATIN SMALL LETTER A WITH RING ABOVE
0x48	0x00E7	#LATIN SMALL LETTER C WITH CEDILLA
0x49	0x00F1	#LATIN SMALL LETTER N WITH TILDE
0x4A	0x00A2	#CENT SIGN
0x4B	0x002E	#FULL STOP
0x4C	0x003C	#LESS-THAN SIGN
0x4D	0x0028	#LEFT PARENTHESIS
0x4E	0x002B	#PLUS SIGN
0x4F	0x007C	#VERTICAL LINE
0x50	0x0026	#AMPERSAND
0x51	0x00E9	#LATIN SMALL LETTER E WITH ACUTE
0x52	0x00EA	#LATIN SMALL LETTER E WITH CIRCUMFLEX
0x53	0x00EB	#LATIN SMALL LETTER E WITH DIAERESIS
0x54	0x00E8	#LATIN SMALL LETTER E WITH GRAVE
0x55	0x00ED	#LATIN SMALL LETTER I WITH ACUTE
0x56	0x00EE	#LATIN SMALL LETTER I WITH CIRCUMFLEX
0x57	0x00EF	#LATIN SMALL LETTER I WITH DIAERESIS
0x58	0x00EC	#LATIN SMALL LETTER I WITH GRAVE
0x59	0x00DF	#LATIN SMALL LETTER SHARP S (GERMAN)
0x5A	0x0021	#EXCLAMATION MARK
0x5B	0x0024	#DOLLAR SIGN
0x5C	0x002A	#ASTERISK
0x5D	0x0029	#RIGHT PARENTHESIS
0x5E	0x003B	#SEMICOLON
0x5F	0x00AC	#NOT SIGN
0x60	0x002D	#HYPHEN-MINUS
0x61	0x002F	#SOLIDUS
0x62	0x00C2	#LATIN CAPITAL LETTER A WITH CIRCUMFLEX
0x63	0x00C4	#LATIN CAPITAL LETTER A WITH DIAERESIS
0x64	0x00C0	#LATIN CAPITAL LETTER A WITH GRAVE
0x65	0x00C1	#LATIN CAPITAL LETTER A WITH ACUTE
0x66	0x00C3	#LATIN CAPITAL LETTER A WITH TILDE
0x67	0x00C5	#LATIN CAPITAL LETTER A WITH RING ABOVE
0x68	0x00C7	#LATIN CAPITAL LETTER C WITH CEDILLA
0x69	0x00D1	#LATIN CAPITAL LETTER N WITH TILDE
0x6A	0x00A6	#BROKEN BAR
0x6B	0x002C	#COMMA
0x6C	0x0025	#PERCENT SIGN
0x6D	0x005F	#LOW LINE
0x6E	0x003E	#GREATER-THAN SIGN
0x6F	0x003F	#QUESTION MARK
0x70	0x00F8	#LATIN SMALL LETTER O WITH STROKE
0x71	0x00C9	#LATIN CAPITAL LETTER E WITH ACUTE
0x72	0x00CA	#LATIN CAPITAL LETTER E WITH CIRCUMFLEX
0x73	0x00CB	#LATIN CAPITAL LETTER E WITH DIAERESIS
0x74	0x00C8	#LATIN CAPITAL LETTER E WITH GRAVE
0x75	0x00CD	#LATIN CAPITAL LETTER I WITH ACUTE
0x76	0x00CE	#LATIN CAPITAL LETTER I WITH CIRCUMFLEX
0x77	0x00CF	#LATIN CAPITAL LETTER I WITH DIAERESIS
0x78	0x00CC	#LATIN CAPITAL LETTER I WITH GRAVE
0x79	0x0060	#GRAVE ACCENT
0x7A	0x003A	#COLON
0x7B	0x0023	#NUMBER SIGN
0x7C	0x0040	#COMMERCIAL AT
0x7D	0x0027	#APOSTROPHE
0x7E	0x003D	#EQUALS SIGN
0x7F	0x0022	#QUOTATION MARK
0x80	0x00D8	#LATIN CAPITAL LETTER O WITH STROKE
0x81	0x0061	#LATIN SMALL LETTER A
0x82	0x0062	#LATIN SMALL LETTER B
0x83	0x0063	#LATIN SMALL LETTER C
0x84	0x0064	#LATIN SMALL LETTER D
0x85	0x0065	#LATIN SMALL LETTER E
0x86	0x0066	#LATIN SMALL LETTER F
0x87	0x0067	#LATIN SMALL LETTER G
0x88	0x0068	#LATIN SMALL LETTER H
0x89	0x0069	#LATIN SMALL LETTER I
0x8A	0x00AB	#LEFT-POINTING DOUBLE ANGLE QUOTATION MARK
0x8B	0x00BB	#RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK
0x8C	0x00F0	#LATIN SMALL LETTER ETH (ICELANDIC)
0x8D	0x00FD	#LATIN SMALL LETTER Y WITH ACUTE
0x8E	0x00FE	#LATIN SMALL LETTER THORN (ICELANDIC)
0x8F	0x00B1	#PLUS-MINUS SIGN
0x90	0x00B0	#DEGREE SIGN
0x91	0x006A	#LATIN SMALL LETTER J
0x92	0x006B	#LATIN SMALL LETTER K
0x93	0x006C	#LATIN SMALL LETTER L
0x94	0x006D	#LATIN SMALL LETTER M
0x95	0x006E	#LATIN SMALL LETTER N
0x96	0x006F	#LATIN SMALL LETTER O
0x97	0x0070	#LATIN SMALL LETTER P
0x98	0x0071	#LATIN SMALL LETTER Q
0x99	0x0072	#LATIN SMALL LETTER R
0x9A	0x00AA	#FEMININE ORDINAL INDICATOR
0x9B	0x00BA	#MASCULINE ORDINAL INDICATOR
0x9C	0x00E6	#LATIN SMALL LIGATURE AE
0x9D	0x00B8	#CEDILLA
0x9E	0x00C6	#LATIN CAPITAL LIGATURE AE
#0x9F	0x00A4	#CURRENCY SIGN
0x9F    0x20AC   # EURO SIGN
0xA0	0x00B5	#MICRO SIGN
0xA1	0x007E	#TILDE
0xA2	0x0073	#LATIN SMALL LETTER S
0xA3	0x0074	#LATIN SMALL LETTER T
0xA4	0x0075	#LATIN SMALL LETTER U
0xA5	0x0076	#LATIN SMALL LETTER V
0xA6	0x0077	#LATIN SMALL LETTER W
0xA7	0x0078	#LATIN SMALL LETTER X
0xA8	0x0079	#LATIN SMALL LETTER Y
0xA9	0x007A	#LATIN SMALL LETTER Z
0xAA	0x00A1	#INVERTED EXCLAMATION MARK
0xAB	0x00BF	#INVERTED QUESTION MARK
0xAC	0x00D0	#LATIN CAPITAL LETTER ETH (ICELANDIC)
0xAD	0x00DD	#LATIN CAPITAL LETTER Y WITH ACUTE
0xAE	0x00DE	#LATIN CAPITAL LETTER THORN (ICELANDIC)
0xAF	0x00AE	#REGISTERED SIGN
0xB0	0x005E	#CIRCUMFLEX ACCENT
0xB1	0x00A3	#POUND SIGN
0xB2	0x00A5	#YEN SIGN
0xB3	0x00B7	#MIDDLE DOT
0xB4	0x00A9	#COPYRIGHT SIGN
0xB5	0x00A7	#SECTION SIGN
0xB6	0x00B6	#PILCROW SIGN
0xB7	0x00BC	#VULGAR FRACTION ONE QUARTER
0xB8	0x00BD	#VULGAR FRACTION ONE HALF
0xB9	0x00BE	#VULGAR FRACTION THREE QUARTERS
0xBA	0x005B	#LEFT SQUARE BRACKET
0xBB	0x005D	#RIGHT SQUARE BRACKET
0xBC	0x00AF	#MACRON
0xBD	0x00A8	#DIAERESIS
0xBE	0x00B4	#ACUTE ACCENT
0xBF	0x00D7	#MULTIPLICATION SIGN
0xC0	0x007B	#LEFT CURLY BRACKET
0xC1	0x0041	#LATIN CAPITAL LETTER A
0xC2	0x0042	#LATIN CAPITAL LETTER B
0xC3	0x0043	#LATIN CAPITAL LETTER C
0xC4	0x0044	#LATIN CAPITAL LETTER D
0xC5	0x0045	#LATIN CAPITAL LETTER E
0xC6	0x0046	#LATIN CAPITAL LETTER F
0xC7	0x0047	#LATIN CAPITAL LETTER G
0xC8	0x0048	#LATIN CAPITAL LETTER H
0xC9	0x0049	#LATIN CAPITAL LETTER I
0xCA	0x00AD	#SOFT HYPHEN
0xCB	0x00F4	#LATIN SMALL LETTER O WITH CIRCUMFLEX
0xCC	0x00F6	#LATIN SMALL LETTER O WITH DIAERESIS
0xCD	0x00F2	#LATIN SMALL LETTER O WITH GRAVE
0xCE	0x00F3	#LATIN SMALL LETTER O WITH ACUTE
0xCF	0x00F5	#LATIN SMALL LETTER O WITH TILDE
0xD0	0x007D	#RIGHT CURLY BRACKET
0xD1	0x004A	#LATIN CAPITAL LETTER J
0xD2	0x004B	#LATIN CAPITAL LETTER K
0xD3	0x004C	#LATIN CAPITAL LETTER L
0xD4	0x004D	#LATIN CAPITAL LETTER M
0xD5	0x004E	#LATIN CAPITAL LETTER N
0xD6	0x004F	#LATIN CAPITAL LETTER O
0xD7	0x0050	#LATIN CAPITAL LETTER P
0xD8	0x0051	#LATIN CAPITAL LETTER Q
0xD9	0x0052	#LATIN CAPITAL LETTER R
0xDA	0x00B9	#SUPERSCRIPT ONE
0xDB	0x00FB	#LATIN SMALL LETTER U WITH CIRCUMFLEX
0xDC	0x00FC	#LATIN SMALL LETTER U WITH DIAERESIS
0xDD	0x00F9	#LATIN SMALL LETTER U WITH GRAVE
0xDE	0x00FA	#LATIN SMALL LETTER U WITH ACUTE
0xDF	0x00FF	#LATIN SMALL LETTER Y WITH DIAERESIS
0xE0	0x005C	#REVERSE SOLIDUS
0xE1	0x00F7	#DIVISION SIGN
0xE2	0x0053	#LATIN CAPITAL LETTER S
0xE3	0x0054	#LATIN CAPITAL LETTER T
0xE4	0x0055	#LATIN CAPITAL LETTER U
0xE5	0x0056	#LATIN CAPITAL LETTER V
0xE6	0x0057	#LATIN CAPITAL LETTER W
0xE7	0x0058	#LATIN CAPITAL LETTER X
0xE8	0x0059	#LATIN CAPITAL LETTER Y
0xE9	0x005A	#LATIN CAPITAL LETTER Z
0xEA	0x00B2	#SUPERSCRIPT TWO
0xEB	0x00D4	#LATIN CAPITAL LETTER O WITH CIRCUMFLEX
0xEC	0x00D6	#LATIN CAPITAL LETTER O WITH DIAERESIS
0xED	0x00D2	#LATIN CAPITAL LETTER O WITH GRAVE
0xEE	0x00D3	#LATIN CAPITAL LETTER O WITH ACUTE
0xEF	0x00D5	#LATIN CAPITAL LETTER O WITH TILDE
0xF0	0x0030	#DIGIT ZERO
0xF1	0x0031	#DIGIT ONE
0xF2	0x0032	#DIGIT TWO
0xF3	0x0033	#DIGIT THREE
0xF4	0x0034	#DIGIT FOUR
0xF5	0x0035	#DIGIT FIVE
0xF6	0x0036	#DIGIT SIX
0xF7	0x0037	#DIGIT SEVEN
0xF8	0x0038	#DIGIT EIGHT
0xF9	0x0039	#DIGIT NINE
0xFA	0x00B3	#SUPERSCRIPT THREE
0xFB	0x00DB	#LATIN CAPITAL LETTER U WITH CIRCUMFLEX
0xFC	0x00DC	#LATIN CAPITAL LETTER U WITH DIAERESIS
0xFD	0x00D9	#LATIN CAPITAL LETTER U WITH GRAVE
0xFE	0x00DA	#LATIN CAPITAL LETTER U WITH ACUTE
0xFF	0x009F	#CONTROL


================================================
File: /Tools/unicode/python-mappings/CP273.TXT
================================================
0x00	0x0000	#NULL (NUL)
0x01	0x0001	#START OF HEADING (SOH)
0x02	0x0002	#START OF TEXT (STX)
0x03	0x0003	#END OF TEXT (ETX)
0x04	0x009C	#STRING TERMINATOR (ST)
0x05	0x0009	#CHARACTER TABULATION (HT)
0x06	0x0086	#START OF SELECTED AREA (SSA)
0x07	0x007F	#DELETE (DEL)
0x08	0x0097	#END OF GUARDED AREA (EPA)
0x09	0x008D	#REVERSE LINE FEED (RI)
0x0A	0x008E	#SINGLE-SHIFT TWO (SS2)
0x0B	0x000B	#LINE TABULATION (VT)
0x0C	0x000C	#FORM FEED (FF)
0x0D	0x000D	#CARRIAGE RETURN (CR)
0x0E	0x000E	#SHIFT OUT (SO)
0x0F	0x000F	#SHIFT IN (SI)
0x10	0x0010	#DATALINK ESCAPE (DLE)
0x11	0x0011	#DEVICE CONTROL ONE (DC1)
0x12	0x0012	#DEVICE CONTROL TWO (DC2)
0x13	0x0013	#DEVICE CONTROL THREE (DC3)
0x14	0x009D	#OPERATING SYSTEM COMMAND (OSC)
0x15	0x0085	#NEXT LINE (NEL)
0x16	0x0008	#BACKSPACE (BS)
0x17	0x0087	#END OF SELECTED AREA (ESA)
0x18	0x0018	#CANCEL (CAN)
0x19	0x0019	#END OF MEDIUM (EM)
0x1A	0x0092	#PRIVATE USE TWO (PU2)
0x1B	0x008F	#SINGLE-SHIFT THREE (SS3)
0x1C	0x001C	#FILE SEPARATOR (IS4)
0x1D	0x001D	#GROUP SEPARATOR (IS3)
0x1E	0x001E	#RECORD SEPARATOR (IS2)
0x1F	0x001F	#UNIT SEPARATOR (IS1)
0x20	0x0080	#PADDING CHARACTER (PAD)
0x21	0x0081	#HIGH OCTET PRESET (HOP)
0x22	0x0082	#BREAK PERMITTED HERE (BPH)
0x23	0x0083	#NO BREAK HERE (NBH)
0x24	0x0084	#INDEX (IND)
0x25	0x000A	#LINE FEED (LF)
0x26	0x0017	#END OF TRANSMISSION BLOCK (ETB)
0x27	0x001B	#ESCAPE (ESC)
0x28	0x0088	#CHARACTER TABULATION SET (HTS)
0x29	0x0089	#CHARACTER TABULATION WITH JUSTIFICATION (HTJ)
0x2A	0x008A	#LINE TABULATION SET (VTS)
0x2B	0x008B	#PARTIAL LINE FORWARD (PLD)
0x2C	0x008C	#PARTIAL LINE BACKWARD (PLU)
0x2D	0x0005	#ENQUIRY (ENQ)
0x2E	0x0006	#ACKNOWLEDGE (ACK)
0x2F	0x0007	#BELL (BEL)
0x30	0x0090	#DEVICE CONTROL STRING (DCS)
0x31	0x0091	#PRIVATE USE ONE (PU1)
0x32	0x0016	#SYNCHRONOUS IDLE (SYN)
0x33	0x0093	#SET TRANSMIT STATE (STS)
0x34	0x0094	#CANCEL CHARACTER (CCH)
0x35	0x0095	#MESSAGE WAITING (MW)
0x36	0x0096	#START OF GUARDED AREA (SPA)
0x37	0x0004	#END OF TRANSMISSION (EOT)
0x38	0x0098	#START OF STRING (SOS)
0x39	0x0099	#SINGLE GRAPHIC CHARACTER INTRODUCER (SGCI)
0x3A	0x009A	#SINGLE CHARACTER INTRODUCER (SCI)
0x3B	0x009B	#CONTROL SEQUENCE INTRODUCER (CSI)
0x3C	0x0014	#DEVICE CONTROL FOUR (DC4)
0x3D	0x0015	#NEGATIVE ACKNOWLEDGE (NAK)
0x3E	0x009E	#PRIVACY MESSAGE (PM)
0x3F	0x001A	#SUBSTITUTE (SUB)
0x40	0x0020	#SPACE
0x41	0x00A0	#NO-BREAK SPACE
0x42	0x00E2	#LATIN SMALL LETTER A WITH CIRCUMFLEX
0x43	0x007B	#LEFT CURLY BRACKET
0x44	0x00E0	#LATIN SMALL LETTER A WITH GRAVE
0x45	0x00E1	#LATIN SMALL LETTER A WITH ACUTE
0x46	0x00E3	#LATIN SMALL LETTER A WITH TILDE
0x47	0x00E5	#LATIN SMALL LETTER A WITH RING ABOVE
0x48	0x00E7	#LATIN SMALL LETTER C WITH CEDILLA
0x49	0x00F1	#LATIN SMALL LETTER N WITH TILDE
0x4A	0x00C4	#LATIN CAPITAL LETTER A WITH DIAERESIS
0x4B	0x002E	#FULL STOP
0x4C	0x003C	#LESS-THAN SIGN
0x4D	0x0028	#LEFT PARENTHESIS
0x4E	0x002B	#PLUS SIGN
0x4F	0x0021	#EXCLAMATION MARK
0x50	0x0026	#AMPERSAND
0x51	0x00E9	#LATIN SMALL LETTER E WITH ACUTE
0x52	0x00EA	#LATIN SMALL LETTER E WITH CIRCUMFLEX
0x53	0x00EB	#LATIN SMALL LETTER E WITH DIAERESIS
0x54	0x00E8	#LATIN SMALL LETTER E WITH GRAVE
0x55	0x00ED	#LATIN SMALL LETTER I WITH ACUTE
0x56	0x00EE	#LATIN SMALL LETTER I WITH CIRCUMFLEX
0x57	0x00EF	#LATIN SMALL LETTER I WITH DIAERESIS
0x58	0x00EC	#LATIN SMALL LETTER I WITH GRAVE
0x59	0x007E	#TILDE
0x5A	0x00DC	#LATIN CAPITAL LETTER U WITH DIAERESIS
0x5B	0x0024	#DOLLAR SIGN
0x5C	0x002A	#ASTERISK
0x5D	0x0029	#RIGHT PARENTHESIS
0x5E	0x003B	#SEMICOLON
0x5F	0x005E	#CIRCUMFLEX ACCENT
0x60	0x002D	#HYPHEN-MINUS
0x61	0x002F	#SOLIDUS
0x62	0x00C2	#LATIN CAPITAL LETTER A WITH CIRCUMFLEX
0x63	0x005B	#LEFT SQUARE BRACKET
0x64	0x00C0	#LATIN CAPITAL LETTER A WITH GRAVE
0x65	0x00C1	#LATIN CAPITAL LETTER A WITH ACUTE
0x66	0x00C3	#LATIN CAPITAL LETTER A WITH TILDE
0x67	0x00C5	#LATIN CAPITAL LETTER A WITH RING ABOVE
0x68	0x00C7	#LATIN CAPITAL LETTER C WITH CEDILLA
0x69	0x00D1	#LATIN CAPITAL LETTER N WITH TILDE
0x6A	0x00F6	#LATIN SMALL LETTER O WITH DIAERESIS
0x6B	0x002C	#COMMA
0x6C	0x0025	#PERCENT SIGN
0x6D	0x005F	#LOW LINE
0x6E	0x003E	#GREATER-THAN SIGN
0x6F	0x003F	#QUESTION MARK
0x70	0x00F8	#LATIN SMALL LETTER O WITH STROKE
0x71	0x00C9	#LATIN CAPITAL LETTER E WITH ACUTE
0x72	0x00CA	#LATIN CAPITAL LETTER E WITH CIRCUMFLEX
0x73	0x00CB	#LATIN CAPITAL LETTER E WITH DIAERESIS
0x74	0x00C8	#LATIN CAPITAL LETTER E WITH GRAVE
0x75	0x00CD	#LATIN CAPITAL LETTER I WITH ACUTE
0x76	0x00CE	#LATIN CAPITAL LETTER I WITH CIRCUMFLEX
0x77	0x00CF	#LATIN CAPITAL LETTER I WITH DIAERESIS
0x78	0x00CC	#LATIN CAPITAL LETTER I WITH GRAVE
0x79	0x0060	#GRAVE ACCENT
0x7A	0x003A	#COLON
0x7B	0x0023	#NUMBER SIGN
0x7C	0x00A7	#SECTION SIGN
0x7D	0x0027	#APOSTROPHE
0x7E	0x003D	#EQUALS SIGN
0x7F	0x0022	#QUOTATION MARK
0x80	0x00D8	#LATIN CAPITAL LETTER O WITH STROKE
0x81	0x0061	#LATIN SMALL LETTER A
0x82	0x0062	#LATIN SMALL LETTER B
0x83	0x0063	#LATIN SMALL LETTER C
0x84	0x0064	#LATIN SMALL LETTER D
0x85	0x0065	#LATIN SMALL LETTER E
0x86	0x0066	#LATIN SMALL LETTER F
0x87	0x0067	#LATIN SMALL LETTER G
0x88	0x0068	#LATIN SMALL LETTER H
0x89	0x0069	#LATIN SMALL LETTER I
0x8A	0x00AB	#LEFT-POINTING DOUBLE ANGLE QUOTATION MARK
0x8B	0x00BB	#RIGHT-POINTING DOUBLE ANGLE QUOTATION MARK
0x8C	0x00F0	#LATIN SMALL LETTER ETH (Icelandic)
0x8D	0x00FD	#LATIN SMALL LETTER Y WITH ACUTE
0x8E	0x00FE	#LATIN SMALL LETTER THORN (Icelandic)
0x8F	0x00B1	#PLUS-MINUS SIGN
0x90	0x00B0	#DEGREE SIGN
0x91	0x006A	#LATIN SMALL LETTER J
0x92	0x006B	#LATIN SMALL LETTER K
0x93	0x006C	#LATIN SMALL LETTER L
0x94	0x006D	#LATIN SMALL LETTER M
0x95	0x006E	#LATIN SMALL LETTER N
0x96	0x006F	#LATIN SMALL LETTER O
0x97	0x0070	#LATIN SMALL LETTER P
0x98	0x0071	#LATIN SMALL LETTER Q
0x99	0x0072	#LATIN SMALL LETTER R
0x9A	0x00AA	#FEMININE ORDINAL INDICATOR
0x9B	0x00BA	#MASCULINE ORDINAL INDICATOR
0x9C	0x00E6	#LATIN SMALL LETTER AE
0x9D	0x00B8	#CEDILLA
0x9E	0x00C6	#LATIN CAPITAL LETTER AE
0x9F	0x00A4	#CURRENCY SIGN
0xA0	0x00B5	#MICRO SIGN
0xA1	0x00DF	#LATIN SMALL LETTER SHARP S (German)
0xA2	0x0073	#LATIN SMALL LETTER S
0xA3	0x0074	#LATIN SMALL LETTER T
0xA4	0x0075	#LATIN SMALL LETTER U
0xA5	0x0076	#LATIN SMALL LETTER V
0xA6	0x0077	#LATIN SMALL LETTER W
0xA7	0x0078	#LATIN SMALL LETTER X
0xA8	0x0079	#LATIN SMALL LETTER Y
0xA9	0x007A	#LATIN SMALL LETTER Z
0xAA	0x00A1	#INVERTED EXCLAMATION MARK
0xAB	0x00BF	#INVERTED QUESTION MARK
0xAC	0x00D0	#LATIN CAPITAL LETTER ETH (Icelandic)
0xAD	0x00DD	#LATIN CAPITAL LETTER Y WITH ACUTE
0xAE	0x00DE	#LATIN CAPITAL LETTER THORN (Icelandic)
0xAF	0x00AE	#REGISTERED SIGN
0xB0	0x00A2	#CENT SIGN
0xB1	0x00A3	#POUND SIGN
0xB2	0x00A5	#YEN SIGN
0xB3	0x00B7	#MIDDLE DOT
0xB4	0x00A9	#COPYRIGHT SIGN
0xB5	0x0040	#COMMERCIAL AT
0xB6	0x00B6	#PILCROW SIGN
0xB7	0x00BC	#VULGAR FRACTION ONE QUARTER
0xB8	0x00BD	#VULGAR FRACTION ONE HALF
0xB9	0x00BE	#VULGAR FRACTION THREE QUARTERS
0xBA	0x00AC	#NOT SIGN
0xBB	0x007C	#VERTICAL LINE
0xBC	0x203E	#OVERLINE
0xBD	0x00A8	#DIAERESIS
0xBE	0x00B4	#ACUTE ACCENT
0xBF	0x00D7	#MULTIPLICATION SIGN
0xC0	0x00E4	#LATIN SMALL LETTER A WITH DIAERESIS
0xC1	0x0041	#LATIN CAPITAL LETTER A
0xC2	0x0042	#LATIN CAPITAL LETTER B
0xC3	0x0043	#LATIN CAPITAL LETTER C
0xC4	0x0044	#LATIN CAPITAL LETTER D
0xC5	0x0045	#LATIN CAPITAL LETTER E
0xC6	0x0046	#LATIN CAPITAL LETTER F
0xC7	0x0047	#LATIN CAPITAL LETTER G
0xC8	0x0048	#LATIN CAPITAL LETTER H
0xC9	0x0049	#LATIN CAPITAL LETTER I
0xCA	0x00AD	#SOFT HYPHEN
0xCB	0x00F4	#LATIN SMALL LETTER O WITH CIRCUMFLEX
0xCC	0x00A6	#BROKEN BAR
0xCD	0x00F2	#LATIN SMALL LETTER O WITH GRAVE
0xCE	0x00F3	#LATIN SMALL LETTER O WITH ACUTE
0xCF	0x00F5	#LATIN SMALL LETTER O WITH TILDE
0xD0	0x00FC	#LATIN SMALL LETTER U WITH DIAERESIS
0xD1	0x004A	#LATIN CAPITAL LETTER J
0xD2	0x004B	#LATIN CAPITAL LETTER K
0xD3	0x004C	#LATIN CAPITAL LETTER L
0xD4	0x004D	#LATIN CAPITAL LETTER M
0xD5	0x004E	#LATIN CAPITAL LETTER N
0xD6	0x004F	#LATIN CAPITAL LETTER O
0xD7	0x0050	#LATIN CAPITAL LETTER P
0xD8	0x0051	#LATIN CAPITAL LETTER Q
0xD9	0x0052	#LATIN CAPITAL LETTER R
0xDA	0x00B9	#SUPERSCRIPT ONE
0xDB	0x00FB	#LATIN SMALL LETTER U WITH CIRCUMFLEX
0xDC	0x007D	#RIGHT CURLY BRACKET
0xDD	0x00F9	#LATIN SMALL LETTER U WITH GRAVE
0xDE	0x00FA	#LATIN SMALL LETTER U WITH ACUTE
0xDF	0x00FF	#LATIN SMALL LETTER Y WITH DIAERESIS
0xE0	0x00D6	#LATIN CAPITAL LETTER O WITH DIAERESIS
0xE1	0x00F7	#DIVISION SIGN
0xE2	0x0053	#LATIN CAPITAL LETTER S
0xE3	0x0054	#LATIN CAPITAL LETTER T
0xE4	0x0055	#LATIN CAPITAL LETTER U
0xE5	0x0056	#LATIN CAPITAL LETTER V
0xE6	0x0057	#LATIN CAPITAL LETTER W
0xE7	0x0058	#LATIN CAPITAL LETTER X
0xE8	0x0059	#LATIN CAPITAL LETTER Y
0xE9	0x005A	#LATIN CAPITAL LETTER Z
0xEA	0x00B2	#SUPERSCRIPT TWO
0xEB	0x00D4	#LATIN CAPITAL LETTER O WITH CIRCUMFLEX
0xEC	0x005C	#REVERSE SOLIDUS
0xED	0x00D2	#LATIN CAPITAL LETTER O WITH GRAVE
0xEE	0x00D3	#LATIN CAPITAL LETTER O WITH ACUTE
0xEF	0x00D5	#LATIN CAPITAL LETTER O WITH TILDE
0xF0	0x0030	#DIGIT ZERO
0xF1	0x0031	#DIGIT ONE
0xF2	0x0032	#DIGIT TWO
0xF3	0x0033	#DIGIT THREE
0xF4	0x0034	#DIGIT FOUR
0xF5	0x0035	#DIGIT FIVE
0xF6	0x0036	#DIGIT SIX
0xF7	0x0037	#DIGIT SEVEN
0xF8	0x0038	#DIGIT EIGHT
0xF9	0x0039	#DIGIT NINE
0xFA	0x00B3	#SUPERSCRIPT THREE
0xFB	0x00DB	#LATIN CAPITAL LETTER U WITH CIRCUMFLEX
0xFC	0x005D	#RIGHT SQUARE BRACKET
0xFD	0x00D9	#LATIN CAPITAL LETTER U WITH GRAVE
0xFE	0x00DA	#LATIN CAPITAL LETTER U WITH ACUTE
0xFF	0x009F	#APPLICATION PROGRAM COMMAND (APC)

 	  	 


================================================
File: /Tools/unicode/python-mappings/KOI8-U.TXT
================================================
#
#	Name:             KOI8-U (RFC2319) to Unicode
#	Unicode version:  3.2
#	Table version:    1.0
#	Table format:     Format A
#	Date:             2005-10-25
#	Authors:          Marc-Andre Lemburg <mal@egenix.com>
#
#       See RFC2319 for details. This encoding is a modified KOI8-R
#       encoding.
#
#       (c) Copyright Marc-Andre Lemburg, 2005.
#           Licensed to PSF under a Contributor Agreement.
#
#       Based on the file 
#       ftp://ftp.unicode.org/Public/MAPPINGS/VENDORS/MISC/KOI8-R.TXT
#       which is:
#
#	Copyright (c) 1991-1999 Unicode, Inc.  All Rights reserved.
#
#	This file is provided as-is by Unicode, Inc. (The Unicode Consortium).
#	No claims are made as to fitness for any particular purpose.  No
#	warranties of any kind are expressed or implied.  The recipient
#	agrees to determine applicability of information provided.  If this
#	file has been provided on optical media by Unicode, Inc., the sole
#	remedy for any claim will be exchange of defective media within 90
#	days of receipt.
#
#	Unicode, Inc. hereby grants the right to freely use the information
#	supplied in this file in the creation of products supporting the
#	Unicode Standard, and to make copies of this file in any form for
#	internal or external distribution as long as this notice remains
#	attached.
#
0x00	0x0000	#	NULL
0x01	0x0001	#	START OF HEADING
0x02	0x0002	#	START OF TEXT
0x03	0x0003	#	END OF TEXT
0x04	0x0004	#	END OF TRANSMISSION
0x05	0x0005	#	ENQUIRY
0x06	0x0006	#	ACKNOWLEDGE
0x07	0x0007	#	BELL
0x08	0x0008	#	BACKSPACE
0x09	0x0009	#	HORIZONTAL TABULATION
0x0A	0x000A	#	LINE FEED
0x0B	0x000B	#	VERTICAL TABULATION
0x0C	0x000C	#	FORM FEED
0x0D	0x000D	#	CARRIAGE RETURN
0x0E	0x000E	#	SHIFT OUT
0x0F	0x000F	#	SHIFT IN
0x10	0x0010	#	DATA LINK ESCAPE
0x11	0x0011	#	DEVICE CONTROL ONE
0x12	0x0012	#	DEVICE CONTROL TWO
0x13	0x0013	#	DEVICE CONTROL THREE
0x14	0x0014	#	DEVICE CONTROL FOUR
0x15	0x0015	#	NEGATIVE ACKNOWLEDGE
0x16	0x0016	#	SYNCHRONOUS IDLE
0x17	0x0017	#	END OF TRANSMISSION BLOCK
0x18	0x0018	#	CANCEL
0x19	0x0019	#	END OF MEDIUM
0x1A	0x001A	#	SUBSTITUTE
0x1B	0x001B	#	ESCAPE
0x1C	0x001C	#	FILE SEPARATOR
0x1D	0x001D	#	GROUP SEPARATOR
0x1E	0x001E	#	RECORD SEPARATOR
0x1F	0x001F	#	UNIT SEPARATOR
0x20	0x0020	#	SPACE
0x21	0x0021	#	EXCLAMATION MARK
0x22	0x0022	#	QUOTATION MARK
0x23	0x0023	#	NUMBER SIGN
0x24	0x0024	#	DOLLAR SIGN
0x25	0x0025	#	PERCENT SIGN
0x26	0x0026	#	AMPERSAND
0x27	0x0027	#	APOSTROPHE
0x28	0x0028	#	LEFT PARENTHESIS
0x29	0x0029	#	RIGHT PARENTHESIS
0x2A	0x002A	#	ASTERISK
0x2B	0x002B	#	PLUS SIGN
0x2C	0x002C	#	COMMA
0x2D	0x002D	#	HYPHEN-MINUS
0x2E	0x002E	#	FULL STOP
0x2F	0x002F	#	SOLIDUS
0x30	0x0030	#	DIGIT ZERO
0x31	0x0031	#	DIGIT ONE
0x32	0x0032	#	DIGIT TWO
0x33	0x0033	#	DIGIT THREE
0x34	0x0034	#	DIGIT FOUR
0x35	0x0035	#	DIGIT FIVE
0x36	0x0036	#	DIGIT SIX
0x37	0x0037	#	DIGIT SEVEN
0x38	0x0038	#	DIGIT EIGHT
0x39	0x0039	#	DIGIT NINE
0x3A	0x003A	#	COLON
0x3B	0x003B	#	SEMICOLON
0x3C	0x003C	#	LESS-THAN SIGN
0x3D	0x003D	#	EQUALS SIGN
0x3E	0x003E	#	GREATER-THAN SIGN
0x3F	0x003F	#	QUESTION MARK
0x40	0x0040	#	COMMERCIAL AT
0x41	0x0041	#	LATIN CAPITAL LETTER A
0x42	0x0042	#	LATIN CAPITAL LETTER B
0x43	0x0043	#	LATIN CAPITAL LETTER C
0x44	0x0044	#	LATIN CAPITAL LETTER D
0x45	0x0045	#	LATIN CAPITAL LETTER E
0x46	0x0046	#	LATIN CAPITAL LETTER F
0x47	0x0047	#	LATIN CAPITAL LETTER G
0x48	0x0048	#	LATIN CAPITAL LETTER H
0x49	0x0049	#	LATIN CAPITAL LETTER I
0x4A	0x004A	#	LATIN CAPITAL LETTER J
0x4B	0x004B	#	LATIN CAPITAL LETTER K
0x4C	0x004C	#	LATIN CAPITAL LETTER L
0x4D	0x004D	#	LATIN CAPITAL LETTER M
0x4E	0x004E	#	LATIN CAPITAL LETTER N
0x4F	0x004F	#	LATIN CAPITAL LETTER O
0x50	0x0050	#	LATIN CAPITAL LETTER P
0x51	0x0051	#	LATIN CAPITAL LETTER Q
0x52	0x0052	#	LATIN CAPITAL LETTER R
0x53	0x0053	#	LATIN CAPITAL LETTER S
0x54	0x0054	#	LATIN CAPITAL LETTER T
0x55	0x0055	#	LATIN CAPITAL LETTER U
0x56	0x0056	#	LATIN CAPITAL LETTER V
0x57	0x0057	#	LATIN CAPITAL LETTER W
0x58	0x0058	#	LATIN CAPITAL LETTER X
0x59	0x0059	#	LATIN CAPITAL LETTER Y
0x5A	0x005A	#	LATIN CAPITAL LETTER Z
0x5B	0x005B	#	LEFT SQUARE BRACKET
0x5C	0x005C	#	REVERSE SOLIDUS
0x5D	0x005D	#	RIGHT SQUARE BRACKET
0x5E	0x005E	#	CIRCUMFLEX ACCENT
0x5F	0x005F	#	LOW LINE
0x60	0x0060	#	GRAVE ACCENT
0x61	0x0061	#	LATIN SMALL LETTER A
0x62	0x0062	#	LATIN SMALL LETTER B
0x63	0x0063	#	LATIN SMALL LETTER C
0x64	0x0064	#	LATIN SMALL LETTER D
0x65	0x0065	#	LATIN SMALL LETTER E
0x66	0x0066	#	LATIN SMALL LETTER F
0x67	0x0067	#	LATIN SMALL LETTER G
0x68	0x0068	#	LATIN SMALL LETTER H
0x69	0x0069	#	LATIN SMALL LETTER I
0x6A	0x006A	#	LATIN SMALL LETTER J
0x6B	0x006B	#	LATIN SMALL LETTER K
0x6C	0x006C	#	LATIN SMALL LETTER L
0x6D	0x006D	#	LATIN SMALL LETTER M
0x6E	0x006E	#	LATIN SMALL LETTER N
0x6F	0x006F	#	LATIN SMALL LETTER O
0x70	0x0070	#	LATIN SMALL LETTER P
0x71	0x0071	#	LATIN SMALL LETTER Q
0x72	0x0072	#	LATIN SMALL LETTER R
0x73	0x0073	#	LATIN SMALL LETTER S
0x74	0x0074	#	LATIN SMALL LETTER T
0x75	0x0075	#	LATIN SMALL LETTER U
0x76	0x0076	#	LATIN SMALL LETTER V
0x77	0x0077	#	LATIN SMALL LETTER W
0x78	0x0078	#	LATIN SMALL LETTER X
0x79	0x0079	#	LATIN SMALL LETTER Y
0x7A	0x007A	#	LATIN SMALL LETTER Z
0x7B	0x007B	#	LEFT CURLY BRACKET
0x7C	0x007C	#	VERTICAL LINE
0x7D	0x007D	#	RIGHT CURLY BRACKET
0x7E	0x007E	#	TILDE
0x7F	0x007F	#	DELETE
0x80	0x2500	#	BOX DRAWINGS LIGHT HORIZONTAL
0x81	0x2502	#	BOX DRAWINGS LIGHT VERTICAL
0x82	0x250C	#	BOX DRAWINGS LIGHT DOWN AND RIGHT
0x83	0x2510	#	BOX DRAWINGS LIGHT DOWN AND LEFT
0x84	0x2514	#	BOX DRAWINGS LIGHT UP AND RIGHT
0x85	0x2518	#	BOX DRAWINGS LIGHT UP AND LEFT
0x86	0x251C	#	BOX DRAWINGS LIGHT VERTICAL AND RIGHT
0x87	0x2524	#	BOX DRAWINGS LIGHT VERTICAL AND LEFT
0x88	0x252C	#	BOX DRAWINGS LIGHT DOWN AND HORIZONTAL
0x89	0x2534	#	BOX DRAWINGS LIGHT UP AND HORIZONTAL
0x8A	0x253C	#	BOX DRAWINGS LIGHT VERTICAL AND HORIZONTAL
0x8B	0x2580	#	UPPER HALF BLOCK
0x8C	0x2584	#	LOWER HALF BLOCK
0x8D	0x2588	#	FULL BLOCK
0x8E	0x258C	#	LEFT HALF BLOCK
0x8F	0x2590	#	RIGHT HALF BLOCK
0x90	0x2591	#	LIGHT SHADE
0x91	0x2592	#	MEDIUM SHADE
0x92	0x2593	#	DARK SHADE
0x93	0x2320	#	TOP HALF INTEGRAL
0x94	0x25A0	#	BLACK SQUARE
0x95	0x2219	#	BULLET OPERATOR
0x96	0x221A	#	SQUARE ROOT
0x97	0x2248	#	ALMOST EQUAL TO
0x98	0x2264	#	LESS-THAN OR EQUAL TO
0x99	0x2265	#	GREATER-THAN OR EQUAL TO
0x9A	0x00A0	#	NO-BREAK SPACE
0x9B	0x2321	#	BOTTOM HALF INTEGRAL
0x9C	0x00B0	#	DEGREE SIGN
0x9D	0x00B2	#	SUPERSCRIPT TWO
0x9E	0x00B7	#	MIDDLE DOT
0x9F	0x00F7	#	DIVISION SIGN
0xA0	0x2550	#	BOX DRAWINGS DOUBLE HORIZONTAL
0xA1	0x2551	#	BOX DRAWINGS DOUBLE VERTICAL
0xA2	0x2552	#	BOX DRAWINGS DOWN SINGLE AND RIGHT DOUBLE
0xA3	0x0451	#	CYRILLIC SMALL LETTER IO
#0xA4	0x2553	#	BOX DRAWINGS DOWN DOUBLE AND RIGHT SINGLE
0xA4    0x0454  #       CYRILLIC SMALL LETTER UKRAINIAN IE
0xA5	0x2554	#	BOX DRAWINGS DOUBLE DOWN AND RIGHT
#0xA6	0x2555	#	BOX DRAWINGS DOWN SINGLE AND LEFT DOUBLE
0xA6    0x0456  #       CYRILLIC SMALL LETTER BYELORUSSIAN-UKRAINIAN I
#0xA7	0x2556	#	BOX DRAWINGS DOWN DOUBLE AND LEFT SINGLE
0xA7    0x0457  #       CYRILLIC SMALL LETTER YI (UKRAINIAN)
0xA8	0x2557	#	BOX DRAWINGS DOUBLE DOWN AND LEFT
0xA9	0x2558	#	BOX DRAWINGS UP SINGLE AND RIGHT DOUBLE
0xAA	0x2559	#	BOX DRAWINGS UP DOUBLE AND RIGHT SINGLE
0xAB	0x255A	#	BOX DRAWINGS DOUBLE UP AND RIGHT
0xAC	0x255B	#	BOX DRAWINGS UP SINGLE AND LEFT DOUBLE
#0xAD	0x255C	#	BOX DRAWINGS UP DOUBLE AND LEFT SINGLE
0xAD    0x0491  #       CYRILLIC SMALL LETTER UKRAINIAN GHE WITH UPTURN
0xAE	0x255D	#	BOX DRAWINGS DOUBLE UP AND LEFT
0xAF	0x255E	#	BOX DRAWINGS VERTICAL SINGLE AND RIGHT DOUBLE
0xB0	0x255F	#	BOX DRAWINGS VERTICAL DOUBLE AND RIGHT SINGLE
0xB1	0x2560	#	BOX DRAWINGS DOUBLE VERTICAL AND RIGHT
0xB2	0x2561	#	BOX DRAWINGS VERTICAL SINGLE AND LEFT DOUBLE
0xB3	0x0401	#	CYRILLIC CAPITAL LETTER IO
#0xB4	0x2562	#	BOX DRAWINGS VERTICAL DOUBLE AND LEFT SINGLE
0xB4    0x0404  #       CYRILLIC CAPITAL LETTER UKRAINIAN IE
0xB5	0x2563	#	BOX DRAWINGS DOUBLE VERTICAL AND LEFT
#0xB6	0x2564	#	BOX DRAWINGS DOWN SINGLE AND HORIZONTAL DOUBLE
0xB6    0x0406  #       CYRILLIC CAPITAL LETTER BYELORUSSIAN-UKRAINIAN I
#0xB7	0x2565	#	BOX DRAWINGS DOWN DOUBLE AND HORIZONTAL SINGLE
0xB7    0x0407  #       CYRILLIC CAPITAL LETTER YI (UKRAINIAN)
0xB8	0x2566	#	BOX DRAWINGS DOUBLE DOWN AND HORIZONTAL
0xB9	0x2567	#	BOX DRAWINGS UP SINGLE AND HORIZONTAL DOUBLE
0xBA	0x2568	#	BOX DRAWINGS UP DOUBLE AND HORIZONTAL SINGLE
0xBB	0x2569	#	BOX DRAWINGS DOUBLE UP AND HORIZONTAL
0xBC	0x256A	#	BOX DRAWINGS VERTICAL SINGLE AND HORIZONTAL DOUBLE
#0xBD	0x256B	#	BOX DRAWINGS VERTICAL DOUBLE AND HORIZONTAL SINGLE
0xBD    0x0490  #       CYRILLIC CAPITAL LETTER UKRAINIAN GHE WITH UPTURN
0xBE	0x256C	#	BOX DRAWINGS DOUBLE VERTICAL AND HORIZONTAL
0xBF	0x00A9	#	COPYRIGHT SIGN
0xC0	0x044E	#	CYRILLIC SMALL LETTER YU
0xC1	0x0430	#	CYRILLIC SMALL LETTER A
0xC2	0x0431	#	CYRILLIC SMALL LETTER BE
0xC3	0x0446	#	CYRILLIC SMALL LETTER TSE
0xC4	0x0434	#	CYRILLIC SMALL LETTER DE
0xC5	0x0435	#	CYRILLIC SMALL LETTER IE
0xC6	0x0444	#	CYRILLIC SMALL LETTER EF
0xC7	0x0433	#	CYRILLIC SMALL LETTER GHE
0xC8	0x0445	#	CYRILLIC SMALL LETTER HA
0xC9	0x0438	#	CYRILLIC SMALL LETTER I
0xCA	0x0439	#	CYRILLIC SMALL LETTER SHORT I
0xCB	0x043A	#	CYRILLIC SMALL LETTER KA
0xCC	0x043B	#	CYRILLIC SMALL LETTER EL
0xCD	0x043C	#	CYRILLIC SMALL LETTER EM
0xCE	0x043D	#	CYRILLIC SMALL LETTER EN
0xCF	0x043E	#	CYRILLIC SMALL LETTER O
0xD0	0x043F	#	CYRILLIC SMALL LETTER PE
0xD1	0x044F	#	CYRILLIC SMALL LETTER YA
0xD2	0x0440	#	CYRILLIC SMALL LETTER ER
0xD3	0x0441	#	CYRILLIC SMALL LETTER ES
0xD4	0x0442	#	CYRILLIC SMALL LETTER TE
0xD5	0x0443	#	CYRILLIC SMALL LETTER U
0xD6	0x0436	#	CYRILLIC SMALL LETTER ZHE
0xD7	0x0432	#	CYRILLIC SMALL LETTER VE
0xD8	0x044C	#	CYRILLIC SMALL LETTER SOFT SIGN
0xD9	0x044B	#	CYRILLIC SMALL LETTER YERU
0xDA	0x0437	#	CYRILLIC SMALL LETTER ZE
0xDB	0x0448	#	CYRILLIC SMALL LETTER SHA
0xDC	0x044D	#	CYRILLIC SMALL LETTER E
0xDD	0x0449	#	CYRILLIC SMALL LETTER SHCHA
0xDE	0x0447	#	CYRILLIC SMALL LETTER CHE
0xDF	0x044A	#	CYRILLIC SMALL LETTER HARD SIGN
0xE0	0x042E	#	CYRILLIC CAPITAL LETTER YU
0xE1	0x0410	#	CYRILLIC CAPITAL LETTER A
0xE2	0x0411	#	CYRILLIC CAPITAL LETTER BE
0xE3	0x0426	#	CYRILLIC CAPITAL LETTER TSE
0xE4	0x0414	#	CYRILLIC CAPITAL LETTER DE
0xE5	0x0415	#	CYRILLIC CAPITAL LETTER IE
0xE6	0x0424	#	CYRILLIC CAPITAL LETTER EF
0xE7	0x0413	#	CYRILLIC CAPITAL LETTER GHE
0xE8	0x0425	#	CYRILLIC CAPITAL LETTER HA
0xE9	0x0418	#	CYRILLIC CAPITAL LETTER I
0xEA	0x0419	#	CYRILLIC CAPITAL LETTER SHORT I
0xEB	0x041A	#	CYRILLIC CAPITAL LETTER KA
0xEC	0x041B	#	CYRILLIC CAPITAL LETTER EL
0xED	0x041C	#	CYRILLIC CAPITAL LETTER EM
0xEE	0x041D	#	CYRILLIC CAPITAL LETTER EN
0xEF	0x041E	#	CYRILLIC CAPITAL LETTER O
0xF0	0x041F	#	CYRILLIC CAPITAL LETTER PE
0xF1	0x042F	#	CYRILLIC CAPITAL LETTER YA
0xF2	0x0420	#	CYRILLIC CAPITAL LETTER ER
0xF3	0x0421	#	CYRILLIC CAPITAL LETTER ES
0xF4	0x0422	#	CYRILLIC CAPITAL LETTER TE
0xF5	0x0423	#	CYRILLIC CAPITAL LETTER U
0xF6	0x0416	#	CYRILLIC CAPITAL LETTER ZHE
0xF7	0x0412	#	CYRILLIC CAPITAL LETTER VE
0xF8	0x042C	#	CYRILLIC CAPITAL LETTER SOFT SIGN
0xF9	0x042B	#	CYRILLIC CAPITAL LETTER YERU
0xFA	0x0417	#	CYRILLIC CAPITAL LETTER ZE
0xFB	0x0428	#	CYRILLIC CAPITAL LETTER SHA
0xFC	0x042D	#	CYRILLIC CAPITAL LETTER E
0xFD	0x0429	#	CYRILLIC CAPITAL LETTER SHCHA
0xFE	0x0427	#	CYRILLIC CAPITAL LETTER CHE
0xFF	0x042A	#	CYRILLIC CAPITAL LETTER HARD SIGN


================================================
File: /Tools/unicode/python-mappings/TIS-620.TXT
================================================
#
#	Name:             TIS-620
#	Unicode version:  3.2
#	Table version:    1.0
#	Table format:     Format A
#	Date:             2005-10-25
#	Authors:          Marc-Andre Lemburg <mal@egenix.com>
#
#       According to
#       ftp://ftp.unicode.org/Public/MAPPINGS/ISO8859/8859-11.TXT the
#       TIS-620 is the identical to ISO_8859-11 with the 0xA0
#       (no-break space) mapping removed.
#
#       (c) Copyright Marc-Andre Lemburg, 2005.
#           Licensed to PSF under a Contributor Agreement.
#
#       Based on the file 
#       ftp://ftp.unicode.org/Public/MAPPINGS/ISO8859/8859-11.TXT
#       which is:
#
#	Copyright (c) 2002 Unicode, Inc.  All Rights reserved.
#
#	This file is provided as-is by Unicode, Inc. (The Unicode Consortium).
#	No claims are made as to fitness for any particular purpose.  No
#	warranties of any kind are expressed or implied.  The recipient
#	agrees to determine applicability of information provided.  If this
#	file has been provided on optical media by Unicode, Inc., the sole
#	remedy for any claim will be exchange of defective media within 90
#	days of receipt.
#
#	Unicode, Inc. hereby grants the right to freely use the information
#	supplied in this file in the creation of products supporting the
#	Unicode Standard, and to make copies of this file in any form for
#	internal or external distribution as long as this notice remains
#	attached.
#
0x00	0x0000	#	NULL
0x01	0x0001	#	START OF HEADING
0x02	0x0002	#	START OF TEXT
0x03	0x0003	#	END OF TEXT
0x04	0x0004	#	END OF TRANSMISSION
0x05	0x0005	#	ENQUIRY
0x06	0x0006	#	ACKNOWLEDGE
0x07	0x0007	#	BELL
0x08	0x0008	#	BACKSPACE
0x09	0x0009	#	HORIZONTAL TABULATION
0x0A	0x000A	#	LINE FEED
0x0B	0x000B	#	VERTICAL TABULATION
0x0C	0x000C	#	FORM FEED
0x0D	0x000D	#	CARRIAGE RETURN
0x0E	0x000E	#	SHIFT OUT
0x0F	0x000F	#	SHIFT IN
0x10	0x0010	#	DATA LINK ESCAPE
0x11	0x0011	#	DEVICE CONTROL ONE
0x12	0x0012	#	DEVICE CONTROL TWO
0x13	0x0013	#	DEVICE CONTROL THREE
0x14	0x0014	#	DEVICE CONTROL FOUR
0x15	0x0015	#	NEGATIVE ACKNOWLEDGE
0x16	0x0016	#	SYNCHRONOUS IDLE
0x17	0x0017	#	END OF TRANSMISSION BLOCK
0x18	0x0018	#	CANCEL
0x19	0x0019	#	END OF MEDIUM
0x1A	0x001A	#	SUBSTITUTE
0x1B	0x001B	#	ESCAPE
0x1C	0x001C	#	FILE SEPARATOR
0x1D	0x001D	#	GROUP SEPARATOR
0x1E	0x001E	#	RECORD SEPARATOR
0x1F	0x001F	#	UNIT SEPARATOR
0x20	0x0020	#	SPACE
0x21	0x0021	#	EXCLAMATION MARK
0x22	0x0022	#	QUOTATION MARK
0x23	0x0023	#	NUMBER SIGN
0x24	0x0024	#	DOLLAR SIGN
0x25	0x0025	#	PERCENT SIGN
0x26	0x0026	#	AMPERSAND
0x27	0x0027	#	APOSTROPHE
0x28	0x0028	#	LEFT PARENTHESIS
0x29	0x0029	#	RIGHT PARENTHESIS
0x2A	0x002A	#	ASTERISK
0x2B	0x002B	#	PLUS SIGN
0x2C	0x002C	#	COMMA
0x2D	0x002D	#	HYPHEN-MINUS
0x2E	0x002E	#	FULL STOP
0x2F	0x002F	#	SOLIDUS
0x30	0x0030	#	DIGIT ZERO
0x31	0x0031	#	DIGIT ONE
0x32	0x0032	#	DIGIT TWO
0x33	0x0033	#	DIGIT THREE
0x34	0x0034	#	DIGIT FOUR
0x35	0x0035	#	DIGIT FIVE
0x36	0x0036	#	DIGIT SIX
0x37	0x0037	#	DIGIT SEVEN
0x38	0x0038	#	DIGIT EIGHT
0x39	0x0039	#	DIGIT NINE
0x3A	0x003A	#	COLON
0x3B	0x003B	#	SEMICOLON
0x3C	0x003C	#	LESS-THAN SIGN
0x3D	0x003D	#	EQUALS SIGN
0x3E	0x003E	#	GREATER-THAN SIGN
0x3F	0x003F	#	QUESTION MARK
0x40	0x0040	#	COMMERCIAL AT
0x41	0x0041	#	LATIN CAPITAL LETTER A
0x42	0x0042	#	LATIN CAPITAL LETTER B
0x43	0x0043	#	LATIN CAPITAL LETTER C
0x44	0x0044	#	LATIN CAPITAL LETTER D
0x45	0x0045	#	LATIN CAPITAL LETTER E
0x46	0x0046	#	LATIN CAPITAL LETTER F
0x47	0x0047	#	LATIN CAPITAL LETTER G
0x48	0x0048	#	LATIN CAPITAL LETTER H
0x49	0x0049	#	LATIN CAPITAL LETTER I
0x4A	0x004A	#	LATIN CAPITAL LETTER J
0x4B	0x004B	#	LATIN CAPITAL LETTER K
0x4C	0x004C	#	LATIN CAPITAL LETTER L
0x4D	0x004D	#	LATIN CAPITAL LETTER M
0x4E	0x004E	#	LATIN CAPITAL LETTER N
0x4F	0x004F	#	LATIN CAPITAL LETTER O
0x50	0x0050	#	LATIN CAPITAL LETTER P
0x51	0x0051	#	LATIN CAPITAL LETTER Q
0x52	0x0052	#	LATIN CAPITAL LETTER R
0x53	0x0053	#	LATIN CAPITAL LETTER S
0x54	0x0054	#	LATIN CAPITAL LETTER T
0x55	0x0055	#	LATIN CAPITAL LETTER U
0x56	0x0056	#	LATIN CAPITAL LETTER V
0x57	0x0057	#	LATIN CAPITAL LETTER W
0x58	0x0058	#	LATIN CAPITAL LETTER X
0x59	0x0059	#	LATIN CAPITAL LETTER Y
0x5A	0x005A	#	LATIN CAPITAL LETTER Z
0x5B	0x005B	#	LEFT SQUARE BRACKET
0x5C	0x005C	#	REVERSE SOLIDUS
0x5D	0x005D	#	RIGHT SQUARE BRACKET
0x5E	0x005E	#	CIRCUMFLEX ACCENT
0x5F	0x005F	#	LOW LINE
0x60	0x0060	#	GRAVE ACCENT
0x61	0x0061	#	LATIN SMALL LETTER A
0x62	0x0062	#	LATIN SMALL LETTER B
0x63	0x0063	#	LATIN SMALL LETTER C
0x64	0x0064	#	LATIN SMALL LETTER D
0x65	0x0065	#	LATIN SMALL LETTER E
0x66	0x0066	#	LATIN SMALL LETTER F
0x67	0x0067	#	LATIN SMALL LETTER G
0x68	0x0068	#	LATIN SMALL LETTER H
0x69	0x0069	#	LATIN SMALL LETTER I
0x6A	0x006A	#	LATIN SMALL LETTER J
0x6B	0x006B	#	LATIN SMALL LETTER K
0x6C	0x006C	#	LATIN SMALL LETTER L
0x6D	0x006D	#	LATIN SMALL LETTER M
0x6E	0x006E	#	LATIN SMALL LETTER N
0x6F	0x006F	#	LATIN SMALL LETTER O
0x70	0x0070	#	LATIN SMALL LETTER P
0x71	0x0071	#	LATIN SMALL LETTER Q
0x72	0x0072	#	LATIN SMALL LETTER R
0x73	0x0073	#	LATIN SMALL LETTER S
0x74	0x0074	#	LATIN SMALL LETTER T
0x75	0x0075	#	LATIN SMALL LETTER U
0x76	0x0076	#	LATIN SMALL LETTER V
0x77	0x0077	#	LATIN SMALL LETTER W
0x78	0x0078	#	LATIN SMALL LETTER X
0x79	0x0079	#	LATIN SMALL LETTER Y
0x7A	0x007A	#	LATIN SMALL LETTER Z
0x7B	0x007B	#	LEFT CURLY BRACKET
0x7C	0x007C	#	VERTICAL LINE
0x7D	0x007D	#	RIGHT CURLY BRACKET
0x7E	0x007E	#	TILDE
0x7F	0x007F	#	DELETE
0x80	0x0080	#	<control>
0x81	0x0081	#	<control>
0x82	0x0082	#	<control>
0x83	0x0083	#	<control>
0x84	0x0084	#	<control>
0x85	0x0085	#	<control>
0x86	0x0086	#	<control>
0x87	0x0087	#	<control>
0x88	0x0088	#	<control>
0x89	0x0089	#	<control>
0x8A	0x008A	#	<control>
0x8B	0x008B	#	<control>
0x8C	0x008C	#	<control>
0x8D	0x008D	#	<control>
0x8E	0x008E	#	<control>
0x8F	0x008F	#	<control>
0x90	0x0090	#	<control>
0x91	0x0091	#	<control>
0x92	0x0092	#	<control>
0x93	0x0093	#	<control>
0x94	0x0094	#	<control>
0x95	0x0095	#	<control>
0x96	0x0096	#	<control>
0x97	0x0097	#	<control>
0x98	0x0098	#	<control>
0x99	0x0099	#	<control>
0x9A	0x009A	#	<control>
0x9B	0x009B	#	<control>
0x9C	0x009C	#	<control>
0x9D	0x009D	#	<control>
0x9E	0x009E	#	<control>
0x9F	0x009F	#	<control>
#0xA0	0x00A0	#	NO-BREAK SPACE
0xA1	0x0E01	#	THAI CHARACTER KO KAI
0xA2	0x0E02	#	THAI CHARACTER KHO KHAI
0xA3	0x0E03	#	THAI CHARACTER KHO KHUAT
0xA4	0x0E04	#	THAI CHARACTER KHO KHWAI
0xA5	0x0E05	#	THAI CHARACTER KHO KHON
0xA6	0x0E06	#	THAI CHARACTER KHO RAKHANG
0xA7	0x0E07	#	THAI CHARACTER NGO NGU
0xA8	0x0E08	#	THAI CHARACTER CHO CHAN
0xA9	0x0E09	#	THAI CHARACTER CHO CHING
0xAA	0x0E0A	#	THAI CHARACTER CHO CHANG
0xAB	0x0E0B	#	THAI CHARACTER SO SO
0xAC	0x0E0C	#	THAI CHARACTER CHO CHOE
0xAD	0x0E0D	#	THAI CHARACTER YO YING
0xAE	0x0E0E	#	THAI CHARACTER DO CHADA
0xAF	0x0E0F	#	THAI CHARACTER TO PATAK
0xB0	0x0E10	#	THAI CHARACTER THO THAN
0xB1	0x0E11	#	THAI CHARACTER THO NANGMONTHO
0xB2	0x0E12	#	THAI CHARACTER THO PHUTHAO
0xB3	0x0E13	#	THAI CHARACTER NO NEN
0xB4	0x0E14	#	THAI CHARACTER DO DEK
0xB5	0x0E15	#	THAI CHARACTER TO TAO
0xB6	0x0E16	#	THAI CHARACTER THO THUNG
0xB7	0x0E17	#	THAI CHARACTER THO THAHAN
0xB8	0x0E18	#	THAI CHARACTER THO THONG
0xB9	0x0E19	#	THAI CHARACTER NO NU
0xBA	0x0E1A	#	THAI CHARACTER BO BAIMAI
0xBB	0x0E1B	#	THAI CHARACTER PO PLA
0xBC	0x0E1C	#	THAI CHARACTER PHO PHUNG
0xBD	0x0E1D	#	THAI CHARACTER FO FA
0xBE	0x0E1E	#	THAI CHARACTER PHO PHAN
0xBF	0x0E1F	#	THAI CHARACTER FO FAN
0xC0	0x0E20	#	THAI CHARACTER PHO SAMPHAO
0xC1	0x0E21	#	THAI CHARACTER MO MA
0xC2	0x0E22	#	THAI CHARACTER YO YAK
0xC3	0x0E23	#	THAI CHARACTER RO RUA
0xC4	0x0E24	#	THAI CHARACTER RU
0xC5	0x0E25	#	THAI CHARACTER LO LING
0xC6	0x0E26	#	THAI CHARACTER LU
0xC7	0x0E27	#	THAI CHARACTER WO WAEN
0xC8	0x0E28	#	THAI CHARACTER SO SALA
0xC9	0x0E29	#	THAI CHARACTER SO RUSI
0xCA	0x0E2A	#	THAI CHARACTER SO SUA
0xCB	0x0E2B	#	THAI CHARACTER HO HIP
0xCC	0x0E2C	#	THAI CHARACTER LO CHULA
0xCD	0x0E2D	#	THAI CHARACTER O ANG
0xCE	0x0E2E	#	THAI CHARACTER HO NOKHUK
0xCF	0x0E2F	#	THAI CHARACTER PAIYANNOI
0xD0	0x0E30	#	THAI CHARACTER SARA A
0xD1	0x0E31	#	THAI CHARACTER MAI HAN-AKAT
0xD2	0x0E32	#	THAI CHARACTER SARA AA
0xD3	0x0E33	#	THAI CHARACTER SARA AM
0xD4	0x0E34	#	THAI CHARACTER SARA I
0xD5	0x0E35	#	THAI CHARACTER SARA II
0xD6	0x0E36	#	THAI CHARACTER SARA UE
0xD7	0x0E37	#	THAI CHARACTER SARA UEE
0xD8	0x0E38	#	THAI CHARACTER SARA U
0xD9	0x0E39	#	THAI CHARACTER SARA UU
0xDA	0x0E3A	#	THAI CHARACTER PHINTHU
0xDF	0x0E3F	#	THAI CURRENCY SYMBOL BAHT
0xE0	0x0E40	#	THAI CHARACTER SARA E
0xE1	0x0E41	#	THAI CHARACTER SARA AE
0xE2	0x0E42	#	THAI CHARACTER SARA O
0xE3	0x0E43	#	THAI CHARACTER SARA AI MAIMUAN
0xE4	0x0E44	#	THAI CHARACTER SARA AI MAIMALAI
0xE5	0x0E45	#	THAI CHARACTER LAKKHANGYAO
0xE6	0x0E46	#	THAI CHARACTER MAIYAMOK
0xE7	0x0E47	#	THAI CHARACTER MAITAIKHU
0xE8	0x0E48	#	THAI CHARACTER MAI EK
0xE9	0x0E49	#	THAI CHARACTER MAI THO
0xEA	0x0E4A	#	THAI CHARACTER MAI TRI
0xEB	0x0E4B	#	THAI CHARACTER MAI CHATTAWA
0xEC	0x0E4C	#	THAI CHARACTER THANTHAKHAT
0xED	0x0E4D	#	THAI CHARACTER NIKHAHIT
0xEE	0x0E4E	#	THAI CHARACTER YAMAKKAN
0xEF	0x0E4F	#	THAI CHARACTER FONGMAN
0xF0	0x0E50	#	THAI DIGIT ZERO
0xF1	0x0E51	#	THAI DIGIT ONE
0xF2	0x0E52	#	THAI DIGIT TWO
0xF3	0x0E53	#	THAI DIGIT THREE
0xF4	0x0E54	#	THAI DIGIT FOUR
0xF5	0x0E55	#	THAI DIGIT FIVE
0xF6	0x0E56	#	THAI DIGIT SIX
0xF7	0x0E57	#	THAI DIGIT SEVEN
0xF8	0x0E58	#	THAI DIGIT EIGHT
0xF9	0x0E59	#	THAI DIGIT NINE
0xFA	0x0E5A	#	THAI CHARACTER ANGKHANKHU
0xFB	0x0E5B	#	THAI CHARACTER KHOMUT


================================================
File: /Tools/unicode/python-mappings/diff/jisx0213-2000-std.txt.diff
================================================
--- jisx0213-2000-std.txt.orig	Tue Apr 16 23:32:38 2002
+++ jisx0213-2000-std.txt	Wed Jun 16 14:49:05 2004
@@ -23,21 +23,21 @@
 3-2121	U+3000	# IDEOGRAPHIC SPACE
 3-2122	U+3001	# IDEOGRAPHIC COMMA
 3-2123	U+3002	# IDEOGRAPHIC FULL STOP
-3-2124	U+002C	# COMMA	Fullwidth: U+FF0C
-3-2125	U+002E	# FULL STOP	Fullwidth: U+FF0E
+3-2124	U+FF0C	# COMMA	Fullwidth: U+FF0C
+3-2125	U+FF0E	# FULL STOP	Fullwidth: U+FF0E
 3-2126	U+30FB	# KATAKANA MIDDLE DOT
-3-2127	U+003A	# COLON	Fullwidth: U+FF1A
-3-2128	U+003B	# SEMICOLON	Fullwidth: U+FF1B
-3-2129	U+003F	# QUESTION MARK	Fullwidth: U+FF1F
-3-212A	U+0021	# EXCLAMATION MARK	Fullwidth: U+FF01
+3-2127	U+FF1A	# COLON	Fullwidth: U+FF1A
+3-2128	U+FF1B	# SEMICOLON	Fullwidth: U+FF1B
+3-2129	U+FF1F	# QUESTION MARK	Fullwidth: U+FF1F
+3-212A	U+FF01	# EXCLAMATION MARK	Fullwidth: U+FF01
 3-212B	U+309B	# KATAKANA-HIRAGANA VOICED SOUND MARK
 3-212C	U+309C	# KATAKANA-HIRAGANA SEMI-VOICED SOUND MARK
 3-212D	U+00B4	# ACUTE ACCENT
-3-212E	U+0060	# GRAVE ACCENT	Fullwidth: U+FF40
+3-212E	U+FF40	# GRAVE ACCENT	Fullwidth: U+FF40
 3-212F	U+00A8	# DIAERESIS
-3-2130	U+005E	# CIRCUMFLEX ACCENT	Fullwidth: U+FF3E
-3-2131	U+203E	# OVERLINE	Windows: U+FFE3
-3-2132	U+005F	# LOW LINE	Fullwidth: U+FF3F
+3-2130	U+FF3E	# CIRCUMFLEX ACCENT	Fullwidth: U+FF3E
+3-2131	U+FFE3	# OVERLINE	Windows: U+FFE3
+3-2132	U+FF3F	# LOW LINE	Fullwidth: U+FF3F
 3-2133	U+30FD	# KATAKANA ITERATION MARK
 3-2134	U+30FE	# KATAKANA VOICED ITERATION MARK
 3-2135	U+309D	# HIRAGANA ITERATION MARK
@@ -48,27 +48,27 @@
 3-213A	U+3006	# IDEOGRAPHIC CLOSING MARK
 3-213B	U+3007	# IDEOGRAPHIC NUMBER ZERO
 3-213C	U+30FC	# KATAKANA-HIRAGANA PROLONGED SOUND MARK
-3-213D	U+2014	# EM DASH	Windows: U+2015
+3-213D	U+2015	# EM DASH	Windows: U+2015
 3-213E	U+2010	# HYPHEN
-3-213F	U+002F	# SOLIDUS	Fullwidth: U+FF0F
+3-213F	U+FF0F	# SOLIDUS	Fullwidth: U+FF0F
 3-2140	U+005C	# REVERSE SOLIDUS	Fullwidth: U+FF3C
 3-2141	U+301C	# WAVE DASH	Windows: U+FF5E
 3-2142	U+2016	# DOUBLE VERTICAL LINE	Windows: U+2225
-3-2143	U+007C	# VERTICAL LINE	Fullwidth: U+FF5C
+3-2143	U+FF5C	# VERTICAL LINE	Fullwidth: U+FF5C
 3-2144	U+2026	# HORIZONTAL ELLIPSIS
 3-2145	U+2025	# TWO DOT LEADER
 3-2146	U+2018	# LEFT SINGLE QUOTATION MARK
 3-2147	U+2019	# RIGHT SINGLE QUOTATION MARK
 3-2148	U+201C	# LEFT DOUBLE QUOTATION MARK
 3-2149	U+201D	# RIGHT DOUBLE QUOTATION MARK
-3-214A	U+0028	# LEFT PARENTHESIS	Fullwidth: U+FF08
-3-214B	U+0029	# RIGHT PARENTHESIS	Fullwidth: U+FF09
+3-214A	U+FF08	# LEFT PARENTHESIS	Fullwidth: U+FF08
+3-214B	U+FF09	# RIGHT PARENTHESIS	Fullwidth: U+FF09
 3-214C	U+3014	# LEFT TORTOISE SHELL BRACKET
 3-214D	U+3015	# RIGHT TORTOISE SHELL BRACKET
-3-214E	U+005B	# LEFT SQUARE BRACKET	Fullwidth: U+FF3B
-3-214F	U+005D	# RIGHT SQUARE BRACKET	Fullwidth: U+FF3D
-3-2150	U+007B	# LEFT CURLY BRACKET	Fullwidth: U+FF5B
-3-2151	U+007D	# RIGHT CURLY BRACKET	Fullwidth: U+FF5D
+3-214E	U+FF3B	# LEFT SQUARE BRACKET	Fullwidth: U+FF3B
+3-214F	U+FF3D	# RIGHT SQUARE BRACKET	Fullwidth: U+FF3D
+3-2150	U+FF5B	# LEFT CURLY BRACKET	Fullwidth: U+FF5B
+3-2151	U+FF5D	# RIGHT CURLY BRACKET	Fullwidth: U+FF5D
 3-2152	U+3008	# LEFT ANGLE BRACKET
 3-2153	U+3009	# RIGHT ANGLE BRACKET
 3-2154	U+300A	# LEFT DOUBLE ANGLE BRACKET
@@ -79,15 +79,15 @@
 3-2159	U+300F	# RIGHT WHITE CORNER BRACKET
 3-215A	U+3010	# LEFT BLACK LENTICULAR BRACKET
 3-215B	U+3011	# RIGHT BLACK LENTICULAR BRACKET
-3-215C	U+002B	# PLUS SIGN	Fullwidth: U+FF0B
+3-215C	U+FF0B	# PLUS SIGN	Fullwidth: U+FF0B
 3-215D	U+2212	# MINUS SIGN	Windows: U+FF0D
 3-215E	U+00B1	# PLUS-MINUS SIGN
 3-215F	U+00D7	# MULTIPLICATION SIGN
 3-2160	U+00F7	# DIVISION SIGN
-3-2161	U+003D	# EQUALS SIGN	Fullwidth: U+FF1D
+3-2161	U+FF1D	# EQUALS SIGN	Fullwidth: U+FF1D
 3-2162	U+2260	# NOT EQUAL TO
-3-2163	U+003C	# LESS-THAN SIGN	Fullwidth: U+FF1C
-3-2164	U+003E	# GREATER-THAN SIGN	Fullwidth: U+FF1E
+3-2163	U+FF1C	# LESS-THAN SIGN	Fullwidth: U+FF1C
+3-2164	U+FF1E	# GREATER-THAN SIGN	Fullwidth: U+FF1E
 3-2165	U+2266	# LESS-THAN OVER EQUAL TO
 3-2166	U+2267	# GREATER-THAN OVER EQUAL TO
 3-2167	U+221E	# INFINITY
@@ -98,15 +98,15 @@
 3-216C	U+2032	# PRIME
 3-216D	U+2033	# DOUBLE PRIME
 3-216E	U+2103	# DEGREE CELSIUS
-3-216F	U+00A5	# YEN SIGN	Windows: U+FFE5
-3-2170	U+0024	# DOLLAR SIGN	Fullwidth: U+FF04
+3-216F	U+FFE5	# YEN SIGN	Windows: U+FFE5
+3-2170	U+FF04	# DOLLAR SIGN	Fullwidth: U+FF04
 3-2171	U+00A2	# CENT SIGN	Windows: U+FFE0
 3-2172	U+00A3	# POUND SIGN	Windows: U+FFE1
-3-2173	U+0025	# PERCENT SIGN	Fullwidth: U+FF05
-3-2174	U+0023	# NUMBER SIGN	Fullwidth: U+FF03
-3-2175	U+0026	# AMPERSAND	Fullwidth: U+FF06
-3-2176	U+002A	# ASTERISK	Fullwidth: U+FF0A
-3-2177	U+0040	# COMMERCIAL AT	Fullwidth: U+FF20
+3-2173	U+FF05	# PERCENT SIGN	Fullwidth: U+FF05
+3-2174	U+FF03	# NUMBER SIGN	Fullwidth: U+FF03
+3-2175	U+FF06	# AMPERSAND	Fullwidth: U+FF06
+3-2176	U+FF0A	# ASTERISK	Fullwidth: U+FF0A
+3-2177	U+FF20	# COMMERCIAL AT	Fullwidth: U+FF20
 3-2178	U+00A7	# SECTION SIGN
 3-2179	U+2606	# WHITE STAR
 3-217A	U+2605	# BLACK STAR
@@ -128,9 +128,9 @@
 3-222C	U+2191	# UPWARDS ARROW
 3-222D	U+2193	# DOWNWARDS ARROW
 3-222E	U+3013	# GETA MARK
-3-222F	U+0027	# APOSTROPHE	Fullwidth: U+FF07
-3-2230	U+0022	# QUOTATION MARK	[2000]	Fullwidth: U+FF02
-3-2231	U+002D	# HYPHEN-MINUS	[2000]	Fullwidth: U+FF0D
+3-222F	U+FF07	# APOSTROPHE	Fullwidth: U+FF07
+3-2230	U+FF02	# QUOTATION MARK	[2000]	Fullwidth: U+FF02
+3-2231	U+FF0D	# HYPHEN-MINUS	[2000]	Fullwidth: U+FF0D
 3-2232	U+007E	# TILDE	[2000]	Fullwidth: U+FF5E
 3-2233	U+3033	# VERTICAL KANA REPEAT MARK UPPER HALF	[2000]
 3-2234	U+3034	# VERTICAL KANA REPEAT WITH VOICED SOUND MARK UPPER HALF	[2000]
@@ -223,16 +223,16 @@
 3-232D	U+21E9	# DOWNWARDS WHITE ARROW	[2000]
 3-232E	U+2934	# ARROW POINTING RIGHTWARDS THEN CURVING UPWARDS	[2000]	[Unicode3.2]
 3-232F	U+2935	# ARROW POINTING RIGHTWARDS THEN CURVING DOWNWARDS	[2000]	[Unicode3.2]
-3-2330	U+0030	# DIGIT ZERO	Fullwidth: U+FF10
-3-2331	U+0031	# DIGIT ONE	Fullwidth: U+FF11
-3-2332	U+0032	# DIGIT TWO	Fullwidth: U+FF12
-3-2333	U+0033	# DIGIT THREE	Fullwidth: U+FF13
-3-2334	U+0034	# DIGIT FOUR	Fullwidth: U+FF14
-3-2335	U+0035	# DIGIT FIVE	Fullwidth: U+FF15
-3-2336	U+0036	# DIGIT SIX	Fullwidth: U+FF16
-3-2337	U+0037	# DIGIT SEVEN	Fullwidth: U+FF17
-3-2338	U+0038	# DIGIT EIGHT	Fullwidth: U+FF18
-3-2339	U+0039	# DIGIT NINE	Fullwidth: U+FF19
+3-2330	U+FF10	# DIGIT ZERO	Fullwidth: U+FF10
+3-2331	U+FF11	# DIGIT ONE	Fullwidth: U+FF11
+3-2332	U+FF12	# DIGIT TWO	Fullwidth: U+FF12
+3-2333	U+FF13	# DIGIT THREE	Fullwidth: U+FF13
+3-2334	U+FF14	# DIGIT FOUR	Fullwidth: U+FF14
+3-2335	U+FF15	# DIGIT FIVE	Fullwidth: U+FF15
+3-2336	U+FF16	# DIGIT SIX	Fullwidth: U+FF16
+3-2337	U+FF17	# DIGIT SEVEN	Fullwidth: U+FF17
+3-2338	U+FF18	# DIGIT EIGHT	Fullwidth: U+FF18
+3-2339	U+FF19	# DIGIT NINE	Fullwidth: U+FF19
 3-233A	U+29BF	# CIRCLED BULLET	[2000]	[Unicode3.2]
 3-233B	U+25C9	# FISHEYE	[2000]
 3-233C	U+303D	# PART ALTERNATION MARK	[2000]	[Unicode3.2]
@@ -240,64 +240,64 @@
 3-233E	U+FE45	# SESAME DOT	[2000]	[Unicode3.2]
 3-233F	U+25E6	# WHITE BULLET	[2000]
 3-2340	U+2022	# BULLET	[2000]
-3-2341	U+0041	# LATIN CAPITAL LETTER A	Fullwidth: U+FF21
-3-2342	U+0042	# LATIN CAPITAL LETTER B	Fullwidth: U+FF22
-3-2343	U+0043	# LATIN CAPITAL LETTER C	Fullwidth: U+FF23
-3-2344	U+0044	# LATIN CAPITAL LETTER D	Fullwidth: U+FF24
-3-2345	U+0045	# LATIN CAPITAL LETTER E	Fullwidth: U+FF25
-3-2346	U+0046	# LATIN CAPITAL LETTER F	Fullwidth: U+FF26
-3-2347	U+0047	# LATIN CAPITAL LETTER G	Fullwidth: U+FF27
-3-2348	U+0048	# LATIN CAPITAL LETTER H	Fullwidth: U+FF28
-3-2349	U+0049	# LATIN CAPITAL LETTER I	Fullwidth: U+FF29
-3-234A	U+004A	# LATIN CAPITAL LETTER J	Fullwidth: U+FF2A
-3-234B	U+004B	# LATIN CAPITAL LETTER K	Fullwidth: U+FF2B
-3-234C	U+004C	# LATIN CAPITAL LETTER L	Fullwidth: U+FF2C
-3-234D	U+004D	# LATIN CAPITAL LETTER M	Fullwidth: U+FF2D
-3-234E	U+004E	# LATIN CAPITAL LETTER N	Fullwidth: U+FF2E
-3-234F	U+004F	# LATIN CAPITAL LETTER O	Fullwidth: U+FF2F
-3-2350	U+0050	# LATIN CAPITAL LETTER P	Fullwidth: U+FF30
-3-2351	U+0051	# LATIN CAPITAL LETTER Q	Fullwidth: U+FF31
-3-2352	U+0052	# LATIN CAPITAL LETTER R	Fullwidth: U+FF32
-3-2353	U+0053	# LATIN CAPITAL LETTER S	Fullwidth: U+FF33
-3-2354	U+0054	# LATIN CAPITAL LETTER T	Fullwidth: U+FF34
-3-2355	U+0055	# LATIN CAPITAL LETTER U	Fullwidth: U+FF35
-3-2356	U+0056	# LATIN CAPITAL LETTER V	Fullwidth: U+FF36
-3-2357	U+0057	# LATIN CAPITAL LETTER W	Fullwidth: U+FF37
-3-2358	U+0058	# LATIN CAPITAL LETTER X	Fullwidth: U+FF38
-3-2359	U+0059	# LATIN CAPITAL LETTER Y	Fullwidth: U+FF39
-3-235A	U+005A	# LATIN CAPITAL LETTER Z	Fullwidth: U+FF3A
+3-2341	U+FF21	# LATIN CAPITAL LETTER A	Fullwidth: U+FF21
+3-2342	U+FF22	# LATIN CAPITAL LETTER B	Fullwidth: U+FF22
+3-2343	U+FF23	# LATIN CAPITAL LETTER C	Fullwidth: U+FF23
+3-2344	U+FF24	# LATIN CAPITAL LETTER D	Fullwidth: U+FF24
+3-2345	U+FF25	# LATIN CAPITAL LETTER E	Fullwidth: U+FF25
+3-2346	U+FF26	# LATIN CAPITAL LETTER F	Fullwidth: U+FF26
+3-2347	U+FF27	# LATIN CAPITAL LETTER G	Fullwidth: U+FF27
+3-2348	U+FF28	# LATIN CAPITAL LETTER H	Fullwidth: U+FF28
+3-2349	U+FF29	# LATIN CAPITAL LETTER I	Fullwidth: U+FF29
+3-234A	U+FF2A	# LATIN CAPITAL LETTER J	Fullwidth: U+FF2A
+3-234B	U+FF2B	# LATIN CAPITAL LETTER K	Fullwidth: U+FF2B
+3-234C	U+FF2C	# LATIN CAPITAL LETTER L	Fullwidth: U+FF2C
+3-234D	U+FF2D	# LATIN CAPITAL LETTER M	Fullwidth: U+FF2D
+3-234E	U+FF2E	# LATIN CAPITAL LETTER N	Fullwidth: U+FF2E
+3-234F	U+FF2F	# LATIN CAPITAL LETTER O	Fullwidth: U+FF2F
+3-2350	U+FF30	# LATIN CAPITAL LETTER P	Fullwidth: U+FF30
+3-2351	U+FF31	# LATIN CAPITAL LETTER Q	Fullwidth: U+FF31
+3-2352	U+FF32	# LATIN CAPITAL LETTER R	Fullwidth: U+FF32
+3-2353	U+FF33	# LATIN CAPITAL LETTER S	Fullwidth: U+FF33
+3-2354	U+FF34	# LATIN CAPITAL LETTER T	Fullwidth: U+FF34
+3-2355	U+FF35	# LATIN CAPITAL LETTER U	Fullwidth: U+FF35
+3-2356	U+FF36	# LATIN CAPITAL LETTER V	Fullwidth: U+FF36
+3-2357	U+FF37	# LATIN CAPITAL LETTER W	Fullwidth: U+FF37
+3-2358	U+FF38	# LATIN CAPITAL LETTER X	Fullwidth: U+FF38
+3-2359	U+FF39	# LATIN CAPITAL LETTER Y	Fullwidth: U+FF39
+3-235A	U+FF3A	# LATIN CAPITAL LETTER Z	Fullwidth: U+FF3A
 3-235B	U+2213	# MINUS-OR-PLUS SIGN	[2000]
 3-235C	U+2135	# ALEF SYMBOL	[2000]
 3-235D	U+210F	# PLANCK CONSTANT OVER TWO PI	[2000]
 3-235E	U+33CB	# SQUARE HP	[2000]
 3-235F	U+2113	# SCRIPT SMALL L	[2000]
 3-2360	U+2127	# INVERTED OHM SIGN	[2000]
-3-2361	U+0061	# LATIN SMALL LETTER A	Fullwidth: U+FF41
-3-2362	U+0062	# LATIN SMALL LETTER B	Fullwidth: U+FF42
-3-2363	U+0063	# LATIN SMALL LETTER C	Fullwidth: U+FF43
-3-2364	U+0064	# LATIN SMALL LETTER D	Fullwidth: U+FF44
-3-2365	U+0065	# LATIN SMALL LETTER E	Fullwidth: U+FF45
-3-2366	U+0066	# LATIN SMALL LETTER F	Fullwidth: U+FF46
-3-2367	U+0067	# LATIN SMALL LETTER G	Fullwidth: U+FF47
-3-2368	U+0068	# LATIN SMALL LETTER H	Fullwidth: U+FF48
-3-2369	U+0069	# LATIN SMALL LETTER I	Fullwidth: U+FF49
-3-236A	U+006A	# LATIN SMALL LETTER J	Fullwidth: U+FF4A
-3-236B	U+006B	# LATIN SMALL LETTER K	Fullwidth: U+FF4B
-3-236C	U+006C	# LATIN SMALL LETTER L	Fullwidth: U+FF4C
-3-236D	U+006D	# LATIN SMALL LETTER M	Fullwidth: U+FF4D
-3-236E	U+006E	# LATIN SMALL LETTER N	Fullwidth: U+FF4E
-3-236F	U+006F	# LATIN SMALL LETTER O	Fullwidth: U+FF4F
-3-2370	U+0070	# LATIN SMALL LETTER P	Fullwidth: U+FF50
-3-2371	U+0071	# LATIN SMALL LETTER Q	Fullwidth: U+FF51
-3-2372	U+0072	# LATIN SMALL LETTER R	Fullwidth: U+FF52
-3-2373	U+0073	# LATIN SMALL LETTER S	Fullwidth: U+FF53
-3-2374	U+0074	# LATIN SMALL LETTER T	Fullwidth: U+FF54
-3-2375	U+0075	# LATIN SMALL LETTER U	Fullwidth: U+FF55
-3-2376	U+0076	# LATIN SMALL LETTER V	Fullwidth: U+FF56
-3-2377	U+0077	# LATIN SMALL LETTER W	Fullwidth: U+FF57
-3-2378	U+0078	# LATIN SMALL LETTER X	Fullwidth: U+FF58
-3-2379	U+0079	# LATIN SMALL LETTER Y	Fullwidth: U+FF59
-3-237A	U+007A	# LATIN SMALL LETTER Z	Fullwidth: U+FF5A
+3-2361	U+FF41	# LATIN SMALL LETTER A	Fullwidth: U+FF41
+3-2362	U+FF42	# LATIN SMALL LETTER B	Fullwidth: U+FF42
+3-2363	U+FF43	# LATIN SMALL LETTER C	Fullwidth: U+FF43
+3-2364	U+FF44	# LATIN SMALL LETTER D	Fullwidth: U+FF44
+3-2365	U+FF45	# LATIN SMALL LETTER E	Fullwidth: U+FF45
+3-2366	U+FF46	# LATIN SMALL LETTER F	Fullwidth: U+FF46
+3-2367	U+FF47	# LATIN SMALL LETTER G	Fullwidth: U+FF47
+3-2368	U+FF48	# LATIN SMALL LETTER H	Fullwidth: U+FF48
+3-2369	U+FF49	# LATIN SMALL LETTER I	Fullwidth: U+FF49
+3-236A	U+FF4A	# LATIN SMALL LETTER J	Fullwidth: U+FF4A
+3-236B	U+FF4B	# LATIN SMALL LETTER K	Fullwidth: U+FF4B
+3-236C	U+FF4C	# LATIN SMALL LETTER L	Fullwidth: U+FF4C
+3-236D	U+FF4D	# LATIN SMALL LETTER M	Fullwidth: U+FF4D
+3-236E	U+FF4E	# LATIN SMALL LETTER N	Fullwidth: U+FF4E
+3-236F	U+FF4F	# LATIN SMALL LETTER O	Fullwidth: U+FF4F
+3-2370	U+FF50	# LATIN SMALL LETTER P	Fullwidth: U+FF50
+3-2371	U+FF51	# LATIN SMALL LETTER Q	Fullwidth: U+FF51
+3-2372	U+FF52	# LATIN SMALL LETTER R	Fullwidth: U+FF52
+3-2373	U+FF53	# LATIN SMALL LETTER S	Fullwidth: U+FF53
+3-2374	U+FF54	# LATIN SMALL LETTER T	Fullwidth: U+FF54
+3-2375	U+FF55	# LATIN SMALL LETTER U	Fullwidth: U+FF55
+3-2376	U+FF56	# LATIN SMALL LETTER V	Fullwidth: U+FF56
+3-2377	U+FF57	# LATIN SMALL LETTER W	Fullwidth: U+FF57
+3-2378	U+FF58	# LATIN SMALL LETTER X	Fullwidth: U+FF58
+3-2379	U+FF59	# LATIN SMALL LETTER Y	Fullwidth: U+FF59
+3-237A	U+FF5A	# LATIN SMALL LETTER Z	Fullwidth: U+FF5A
 3-237B	U+30A0	# KATAKANA-HIRAGANA DOUBLE HYPHEN	[2000]	[Unicode3.2]
 3-237C	U+2013	# EN DASH	[2000]
 3-237D	U+29FA	# DOUBLE PLUS	[2000]	[Unicode3.2]


================================================
File: /Tools/unicode/python-mappings/diff/jisx0213-2004-std.txt.diff
================================================
--- jisx0213-2000-std.txt.orig	Tue Apr 16 23:32:38 2002
+++ jisx0213-2004-std.txt	Thu Jul  8 11:51:54 2004
@@ -1,6 +1,6 @@
-## JIS X 0213:2000 vs Unicode mapping table
+## JIS X 0213:2004 vs Unicode mapping table
 ## 
-## Date: 16 Apr 2002 13:09:49 GMT
+## Date: 7 Jul 2004 13:09:49 GMT
 ## License:
 ## 	Copyright (C) 2001 earthian@tama.or.jp, All Rights Reserved.
 ## 	Copyright (C) 2001 I'O, All Rights Reserved.
@@ -23,21 +23,21 @@
 3-2121	U+3000	# IDEOGRAPHIC SPACE
 3-2122	U+3001	# IDEOGRAPHIC COMMA
 3-2123	U+3002	# IDEOGRAPHIC FULL STOP
-3-2124	U+002C	# COMMA	Fullwidth: U+FF0C
-3-2125	U+002E	# FULL STOP	Fullwidth: U+FF0E
+3-2124	U+FF0C	# COMMA	Fullwidth: U+FF0C
+3-2125	U+FF0E	# FULL STOP	Fullwidth: U+FF0E
 3-2126	U+30FB	# KATAKANA MIDDLE DOT
-3-2127	U+003A	# COLON	Fullwidth: U+FF1A
-3-2128	U+003B	# SEMICOLON	Fullwidth: U+FF1B
-3-2129	U+003F	# QUESTION MARK	Fullwidth: U+FF1F
-3-212A	U+0021	# EXCLAMATION MARK	Fullwidth: U+FF01
+3-2127	U+FF1A	# COLON	Fullwidth: U+FF1A
+3-2128	U+FF1B	# SEMICOLON	Fullwidth: U+FF1B
+3-2129	U+FF1F	# QUESTION MARK	Fullwidth: U+FF1F
+3-212A	U+FF01	# EXCLAMATION MARK	Fullwidth: U+FF01
 3-212B	U+309B	# KATAKANA-HIRAGANA VOICED SOUND MARK
 3-212C	U+309C	# KATAKANA-HIRAGANA SEMI-VOICED SOUND MARK
 3-212D	U+00B4	# ACUTE ACCENT
-3-212E	U+0060	# GRAVE ACCENT	Fullwidth: U+FF40
+3-212E	U+FF40	# GRAVE ACCENT	Fullwidth: U+FF40
 3-212F	U+00A8	# DIAERESIS
-3-2130	U+005E	# CIRCUMFLEX ACCENT	Fullwidth: U+FF3E
-3-2131	U+203E	# OVERLINE	Windows: U+FFE3
-3-2132	U+005F	# LOW LINE	Fullwidth: U+FF3F
+3-2130	U+FF3E	# CIRCUMFLEX ACCENT	Fullwidth: U+FF3E
+3-2131	U+FFE3	# OVERLINE	Windows: U+FFE3
+3-2132	U+FF3F	# LOW LINE	Fullwidth: U+FF3F
 3-2133	U+30FD	# KATAKANA ITERATION MARK
 3-2134	U+30FE	# KATAKANA VOICED ITERATION MARK
 3-2135	U+309D	# HIRAGANA ITERATION MARK
@@ -48,27 +48,27 @@
 3-213A	U+3006	# IDEOGRAPHIC CLOSING MARK
 3-213B	U+3007	# IDEOGRAPHIC NUMBER ZERO
 3-213C	U+30FC	# KATAKANA-HIRAGANA PROLONGED SOUND MARK
-3-213D	U+2014	# EM DASH	Windows: U+2015
+3-213D	U+2015	# EM DASH	Windows: U+2015
 3-213E	U+2010	# HYPHEN
-3-213F	U+002F	# SOLIDUS	Fullwidth: U+FF0F
+3-213F	U+FF0F	# SOLIDUS	Fullwidth: U+FF0F
 3-2140	U+005C	# REVERSE SOLIDUS	Fullwidth: U+FF3C
 3-2141	U+301C	# WAVE DASH	Windows: U+FF5E
 3-2142	U+2016	# DOUBLE VERTICAL LINE	Windows: U+2225
-3-2143	U+007C	# VERTICAL LINE	Fullwidth: U+FF5C
+3-2143	U+FF5C	# VERTICAL LINE	Fullwidth: U+FF5C
 3-2144	U+2026	# HORIZONTAL ELLIPSIS
 3-2145	U+2025	# TWO DOT LEADER
 3-2146	U+2018	# LEFT SINGLE QUOTATION MARK
 3-2147	U+2019	# RIGHT SINGLE QUOTATION MARK
 3-2148	U+201C	# LEFT DOUBLE QUOTATION MARK
 3-2149	U+201D	# RIGHT DOUBLE QUOTATION MARK
-3-214A	U+0028	# LEFT PARENTHESIS	Fullwidth: U+FF08
-3-214B	U+0029	# RIGHT PARENTHESIS	Fullwidth: U+FF09
+3-214A	U+FF08	# LEFT PARENTHESIS	Fullwidth: U+FF08
+3-214B	U+FF09	# RIGHT PARENTHESIS	Fullwidth: U+FF09
 3-214C	U+3014	# LEFT TORTOISE SHELL BRACKET
 3-214D	U+3015	# RIGHT TORTOISE SHELL BRACKET
-3-214E	U+005B	# LEFT SQUARE BRACKET	Fullwidth: U+FF3B
-3-214F	U+005D	# RIGHT SQUARE BRACKET	Fullwidth: U+FF3D
-3-2150	U+007B	# LEFT CURLY BRACKET	Fullwidth: U+FF5B
-3-2151	U+007D	# RIGHT CURLY BRACKET	Fullwidth: U+FF5D
+3-214E	U+FF3B	# LEFT SQUARE BRACKET	Fullwidth: U+FF3B
+3-214F	U+FF3D	# RIGHT SQUARE BRACKET	Fullwidth: U+FF3D
+3-2150	U+FF5B	# LEFT CURLY BRACKET	Fullwidth: U+FF5B
+3-2151	U+FF5D	# RIGHT CURLY BRACKET	Fullwidth: U+FF5D
 3-2152	U+3008	# LEFT ANGLE BRACKET
 3-2153	U+3009	# RIGHT ANGLE BRACKET
 3-2154	U+300A	# LEFT DOUBLE ANGLE BRACKET
@@ -79,15 +79,15 @@
 3-2159	U+300F	# RIGHT WHITE CORNER BRACKET
 3-215A	U+3010	# LEFT BLACK LENTICULAR BRACKET
 3-215B	U+3011	# RIGHT BLACK LENTICULAR BRACKET
-3-215C	U+002B	# PLUS SIGN	Fullwidth: U+FF0B
+3-215C	U+FF0B	# PLUS SIGN	Fullwidth: U+FF0B
 3-215D	U+2212	# MINUS SIGN	Windows: U+FF0D
 3-215E	U+00B1	# PLUS-MINUS SIGN
 3-215F	U+00D7	# MULTIPLICATION SIGN
 3-2160	U+00F7	# DIVISION SIGN
-3-2161	U+003D	# EQUALS SIGN	Fullwidth: U+FF1D
+3-2161	U+FF1D	# EQUALS SIGN	Fullwidth: U+FF1D
 3-2162	U+2260	# NOT EQUAL TO
-3-2163	U+003C	# LESS-THAN SIGN	Fullwidth: U+FF1C
-3-2164	U+003E	# GREATER-THAN SIGN	Fullwidth: U+FF1E
+3-2163	U+FF1C	# LESS-THAN SIGN	Fullwidth: U+FF1C
+3-2164	U+FF1E	# GREATER-THAN SIGN	Fullwidth: U+FF1E
 3-2165	U+2266	# LESS-THAN OVER EQUAL TO
 3-2166	U+2267	# GREATER-THAN OVER EQUAL TO
 3-2167	U+221E	# INFINITY
@@ -98,15 +98,15 @@
 3-216C	U+2032	# PRIME
 3-216D	U+2033	# DOUBLE PRIME
 3-216E	U+2103	# DEGREE CELSIUS
-3-216F	U+00A5	# YEN SIGN	Windows: U+FFE5
-3-2170	U+0024	# DOLLAR SIGN	Fullwidth: U+FF04
+3-216F	U+FFE5	# YEN SIGN	Windows: U+FFE5
+3-2170	U+FF04	# DOLLAR SIGN	Fullwidth: U+FF04
 3-2171	U+00A2	# CENT SIGN	Windows: U+FFE0
 3-2172	U+00A3	# POUND SIGN	Windows: U+FFE1
-3-2173	U+0025	# PERCENT SIGN	Fullwidth: U+FF05
-3-2174	U+0023	# NUMBER SIGN	Fullwidth: U+FF03
-3-2175	U+0026	# AMPERSAND	Fullwidth: U+FF06
-3-2176	U+002A	# ASTERISK	Fullwidth: U+FF0A
-3-2177	U+0040	# COMMERCIAL AT	Fullwidth: U+FF20
+3-2173	U+FF05	# PERCENT SIGN	Fullwidth: U+FF05
+3-2174	U+FF03	# NUMBER SIGN	Fullwidth: U+FF03
+3-2175	U+FF06	# AMPERSAND	Fullwidth: U+FF06
+3-2176	U+FF0A	# ASTERISK	Fullwidth: U+FF0A
+3-2177	U+FF20	# COMMERCIAL AT	Fullwidth: U+FF20
 3-2178	U+00A7	# SECTION SIGN
 3-2179	U+2606	# WHITE STAR
 3-217A	U+2605	# BLACK STAR
@@ -128,9 +128,9 @@
 3-222C	U+2191	# UPWARDS ARROW
 3-222D	U+2193	# DOWNWARDS ARROW
 3-222E	U+3013	# GETA MARK
-3-222F	U+0027	# APOSTROPHE	Fullwidth: U+FF07
-3-2230	U+0022	# QUOTATION MARK	[2000]	Fullwidth: U+FF02
-3-2231	U+002D	# HYPHEN-MINUS	[2000]	Fullwidth: U+FF0D
+3-222F	U+FF07	# APOSTROPHE	Fullwidth: U+FF07
+3-2230	U+FF02	# QUOTATION MARK	[2000]	Fullwidth: U+FF02
+3-2231	U+FF0D	# HYPHEN-MINUS	[2000]	Fullwidth: U+FF0D
 3-2232	U+007E	# TILDE	[2000]	Fullwidth: U+FF5E
 3-2233	U+3033	# VERTICAL KANA REPEAT MARK UPPER HALF	[2000]
 3-2234	U+3034	# VERTICAL KANA REPEAT WITH VOICED SOUND MARK UPPER HALF	[2000]
@@ -223,16 +223,16 @@
 3-232D	U+21E9	# DOWNWARDS WHITE ARROW	[2000]
 3-232E	U+2934	# ARROW POINTING RIGHTWARDS THEN CURVING UPWARDS	[2000]	[Unicode3.2]
 3-232F	U+2935	# ARROW POINTING RIGHTWARDS THEN CURVING DOWNWARDS	[2000]	[Unicode3.2]
-3-2330	U+0030	# DIGIT ZERO	Fullwidth: U+FF10
-3-2331	U+0031	# DIGIT ONE	Fullwidth: U+FF11
-3-2332	U+0032	# DIGIT TWO	Fullwidth: U+FF12
-3-2333	U+0033	# DIGIT THREE	Fullwidth: U+FF13
-3-2334	U+0034	# DIGIT FOUR	Fullwidth: U+FF14
-3-2335	U+0035	# DIGIT FIVE	Fullwidth: U+FF15
-3-2336	U+0036	# DIGIT SIX	Fullwidth: U+FF16
-3-2337	U+0037	# DIGIT SEVEN	Fullwidth: U+FF17
-3-2338	U+0038	# DIGIT EIGHT	Fullwidth: U+FF18
-3-2339	U+0039	# DIGIT NINE	Fullwidth: U+FF19
+3-2330	U+FF10	# DIGIT ZERO	Fullwidth: U+FF10
+3-2331	U+FF11	# DIGIT ONE	Fullwidth: U+FF11
+3-2332	U+FF12	# DIGIT TWO	Fullwidth: U+FF12
+3-2333	U+FF13	# DIGIT THREE	Fullwidth: U+FF13
+3-2334	U+FF14	# DIGIT FOUR	Fullwidth: U+FF14
+3-2335	U+FF15	# DIGIT FIVE	Fullwidth: U+FF15
+3-2336	U+FF16	# DIGIT SIX	Fullwidth: U+FF16
+3-2337	U+FF17	# DIGIT SEVEN	Fullwidth: U+FF17
+3-2338	U+FF18	# DIGIT EIGHT	Fullwidth: U+FF18
+3-2339	U+FF19	# DIGIT NINE	Fullwidth: U+FF19
 3-233A	U+29BF	# CIRCLED BULLET	[2000]	[Unicode3.2]
 3-233B	U+25C9	# FISHEYE	[2000]
 3-233C	U+303D	# PART ALTERNATION MARK	[2000]	[Unicode3.2]
@@ -240,64 +240,64 @@
 3-233E	U+FE45	# SESAME DOT	[2000]	[Unicode3.2]
 3-233F	U+25E6	# WHITE BULLET	[2000]
 3-2340	U+2022	# BULLET	[2000]
-3-2341	U+0041	# LATIN CAPITAL LETTER A	Fullwidth: U+FF21
-3-2342	U+0042	# LATIN CAPITAL LETTER B	Fullwidth: U+FF22
-3-2343	U+0043	# LATIN CAPITAL LETTER C	Fullwidth: U+FF23
-3-2344	U+0044	# LATIN CAPITAL LETTER D	Fullwidth: U+FF24
-3-2345	U+0045	# LATIN CAPITAL LETTER E	Fullwidth: U+FF25
-3-2346	U+0046	# LATIN CAPITAL LETTER F	Fullwidth: U+FF26
-3-2347	U+0047	# LATIN CAPITAL LETTER G	Fullwidth: U+FF27
-3-2348	U+0048	# LATIN CAPITAL LETTER H	Fullwidth: U+FF28
-3-2349	U+0049	# LATIN CAPITAL LETTER I	Fullwidth: U+FF29
-3-234A	U+004A	# LATIN CAPITAL LETTER J	Fullwidth: U+FF2A
-3-234B	U+004B	# LATIN CAPITAL LETTER K	Fullwidth: U+FF2B
-3-234C	U+004C	# LATIN CAPITAL LETTER L	Fullwidth: U+FF2C
-3-234D	U+004D	# LATIN CAPITAL LETTER M	Fullwidth: U+FF2D
-3-234E	U+004E	# LATIN CAPITAL LETTER N	Fullwidth: U+FF2E
-3-234F	U+004F	# LATIN CAPITAL LETTER O	Fullwidth: U+FF2F
-3-2350	U+0050	# LATIN CAPITAL LETTER P	Fullwidth: U+FF30
-3-2351	U+0051	# LATIN CAPITAL LETTER Q	Fullwidth: U+FF31
-3-2352	U+0052	# LATIN CAPITAL LETTER R	Fullwidth: U+FF32
-3-2353	U+0053	# LATIN CAPITAL LETTER S	Fullwidth: U+FF33
-3-2354	U+0054	# LATIN CAPITAL LETTER T	Fullwidth: U+FF34
-3-2355	U+0055	# LATIN CAPITAL LETTER U	Fullwidth: U+FF35
-3-2356	U+0056	# LATIN CAPITAL LETTER V	Fullwidth: U+FF36
-3-2357	U+0057	# LATIN CAPITAL LETTER W	Fullwidth: U+FF37
-3-2358	U+0058	# LATIN CAPITAL LETTER X	Fullwidth: U+FF38
-3-2359	U+0059	# LATIN CAPITAL LETTER Y	Fullwidth: U+FF39
-3-235A	U+005A	# LATIN CAPITAL LETTER Z	Fullwidth: U+FF3A
+3-2341	U+FF21	# LATIN CAPITAL LETTER A	Fullwidth: U+FF21
+3-2342	U+FF22	# LATIN CAPITAL LETTER B	Fullwidth: U+FF22
+3-2343	U+FF23	# LATIN CAPITAL LETTER C	Fullwidth: U+FF23
+3-2344	U+FF24	# LATIN CAPITAL LETTER D	Fullwidth: U+FF24
+3-2345	U+FF25	# LATIN CAPITAL LETTER E	Fullwidth: U+FF25
+3-2346	U+FF26	# LATIN CAPITAL LETTER F	Fullwidth: U+FF26
+3-2347	U+FF27	# LATIN CAPITAL LETTER G	Fullwidth: U+FF27
+3-2348	U+FF28	# LATIN CAPITAL LETTER H	Fullwidth: U+FF28
+3-2349	U+FF29	# LATIN CAPITAL LETTER I	Fullwidth: U+FF29
+3-234A	U+FF2A	# LATIN CAPITAL LETTER J	Fullwidth: U+FF2A
+3-234B	U+FF2B	# LATIN CAPITAL LETTER K	Fullwidth: U+FF2B
+3-234C	U+FF2C	# LATIN CAPITAL LETTER L	Fullwidth: U+FF2C
+3-234D	U+FF2D	# LATIN CAPITAL LETTER M	Fullwidth: U+FF2D
+3-234E	U+FF2E	# LATIN CAPITAL LETTER N	Fullwidth: U+FF2E
+3-234F	U+FF2F	# LATIN CAPITAL LETTER O	Fullwidth: U+FF2F
+3-2350	U+FF30	# LATIN CAPITAL LETTER P	Fullwidth: U+FF30
+3-2351	U+FF31	# LATIN CAPITAL LETTER Q	Fullwidth: U+FF31
+3-2352	U+FF32	# LATIN CAPITAL LETTER R	Fullwidth: U+FF32
+3-2353	U+FF33	# LATIN CAPITAL LETTER S	Fullwidth: U+FF33
+3-2354	U+FF34	# LATIN CAPITAL LETTER T	Fullwidth: U+FF34
+3-2355	U+FF35	# LATIN CAPITAL LETTER U	Fullwidth: U+FF35
+3-2356	U+FF36	# LATIN CAPITAL LETTER V	Fullwidth: U+FF36
+3-2357	U+FF37	# LATIN CAPITAL LETTER W	Fullwidth: U+FF37
+3-2358	U+FF38	# LATIN CAPITAL LETTER X	Fullwidth: U+FF38
+3-2359	U+FF39	# LATIN CAPITAL LETTER Y	Fullwidth: U+FF39
+3-235A	U+FF3A	# LATIN CAPITAL LETTER Z	Fullwidth: U+FF3A
 3-235B	U+2213	# MINUS-OR-PLUS SIGN	[2000]
 3-235C	U+2135	# ALEF SYMBOL	[2000]
 3-235D	U+210F	# PLANCK CONSTANT OVER TWO PI	[2000]
 3-235E	U+33CB	# SQUARE HP	[2000]
 3-235F	U+2113	# SCRIPT SMALL L	[2000]
 3-2360	U+2127	# INVERTED OHM SIGN	[2000]
-3-2361	U+0061	# LATIN SMALL LETTER A	Fullwidth: U+FF41
-3-2362	U+0062	# LATIN SMALL LETTER B	Fullwidth: U+FF42
-3-2363	U+0063	# LATIN SMALL LETTER C	Fullwidth: U+FF43
-3-2364	U+0064	# LATIN SMALL LETTER D	Fullwidth: U+FF44
-3-2365	U+0065	# LATIN SMALL LETTER E	Fullwidth: U+FF45
-3-2366	U+0066	# LATIN SMALL LETTER F	Fullwidth: U+FF46
-3-2367	U+0067	# LATIN SMALL LETTER G	Fullwidth: U+FF47
-3-2368	U+0068	# LATIN SMALL LETTER H	Fullwidth: U+FF48
-3-2369	U+0069	# LATIN SMALL LETTER I	Fullwidth: U+FF49
-3-236A	U+006A	# LATIN SMALL LETTER J	Fullwidth: U+FF4A
-3-236B	U+006B	# LATIN SMALL LETTER K	Fullwidth: U+FF4B
-3-236C	U+006C	# LATIN SMALL LETTER L	Fullwidth: U+FF4C
-3-236D	U+006D	# LATIN SMALL LETTER M	Fullwidth: U+FF4D
-3-236E	U+006E	# LATIN SMALL LETTER N	Fullwidth: U+FF4E
-3-236F	U+006F	# LATIN SMALL LETTER O	Fullwidth: U+FF4F
-3-2370	U+0070	# LATIN SMALL LETTER P	Fullwidth: U+FF50
-3-2371	U+0071	# LATIN SMALL LETTER Q	Fullwidth: U+FF51
-3-2372	U+0072	# LATIN SMALL LETTER R	Fullwidth: U+FF52
-3-2373	U+0073	# LATIN SMALL LETTER S	Fullwidth: U+FF53
-3-2374	U+0074	# LATIN SMALL LETTER T	Fullwidth: U+FF54
-3-2375	U+0075	# LATIN SMALL LETTER U	Fullwidth: U+FF55
-3-2376	U+0076	# LATIN SMALL LETTER V	Fullwidth: U+FF56
-3-2377	U+0077	# LATIN SMALL LETTER W	Fullwidth: U+FF57
-3-2378	U+0078	# LATIN SMALL LETTER X	Fullwidth: U+FF58
-3-2379	U+0079	# LATIN SMALL LETTER Y	Fullwidth: U+FF59
-3-237A	U+007A	# LATIN SMALL LETTER Z	Fullwidth: U+FF5A
+3-2361	U+FF41	# LATIN SMALL LETTER A	Fullwidth: U+FF41
+3-2362	U+FF42	# LATIN SMALL LETTER B	Fullwidth: U+FF42
+3-2363	U+FF43	# LATIN SMALL LETTER C	Fullwidth: U+FF43
+3-2364	U+FF44	# LATIN SMALL LETTER D	Fullwidth: U+FF44
+3-2365	U+FF45	# LATIN SMALL LETTER E	Fullwidth: U+FF45
+3-2366	U+FF46	# LATIN SMALL LETTER F	Fullwidth: U+FF46
+3-2367	U+FF47	# LATIN SMALL LETTER G	Fullwidth: U+FF47
+3-2368	U+FF48	# LATIN SMALL LETTER H	Fullwidth: U+FF48
+3-2369	U+FF49	# LATIN SMALL LETTER I	Fullwidth: U+FF49
+3-236A	U+FF4A	# LATIN SMALL LETTER J	Fullwidth: U+FF4A
+3-236B	U+FF4B	# LATIN SMALL LETTER K	Fullwidth: U+FF4B
+3-236C	U+FF4C	# LATIN SMALL LETTER L	Fullwidth: U+FF4C
+3-236D	U+FF4D	# LATIN SMALL LETTER M	Fullwidth: U+FF4D
+3-236E	U+FF4E	# LATIN SMALL LETTER N	Fullwidth: U+FF4E
+3-236F	U+FF4F	# LATIN SMALL LETTER O	Fullwidth: U+FF4F
+3-2370	U+FF50	# LATIN SMALL LETTER P	Fullwidth: U+FF50
+3-2371	U+FF51	# LATIN SMALL LETTER Q	Fullwidth: U+FF51
+3-2372	U+FF52	# LATIN SMALL LETTER R	Fullwidth: U+FF52
+3-2373	U+FF53	# LATIN SMALL LETTER S	Fullwidth: U+FF53
+3-2374	U+FF54	# LATIN SMALL LETTER T	Fullwidth: U+FF54
+3-2375	U+FF55	# LATIN SMALL LETTER U	Fullwidth: U+FF55
+3-2376	U+FF56	# LATIN SMALL LETTER V	Fullwidth: U+FF56
+3-2377	U+FF57	# LATIN SMALL LETTER W	Fullwidth: U+FF57
+3-2378	U+FF58	# LATIN SMALL LETTER X	Fullwidth: U+FF58
+3-2379	U+FF59	# LATIN SMALL LETTER Y	Fullwidth: U+FF59
+3-237A	U+FF5A	# LATIN SMALL LETTER Z	Fullwidth: U+FF5A
 3-237B	U+30A0	# KATAKANA-HIRAGANA DOUBLE HYPHEN	[2000]	[Unicode3.2]
 3-237C	U+2013	# EN DASH	[2000]
 3-237D	U+29FA	# DOUBLE PLUS	[2000]	[Unicode3.2]
@@ -1242,7 +1242,7 @@
 3-2D7C		# <reserved>	Windows: U+222A
 3-2D7D	U+2756	# BLACK DIAMOND MINUS WHITE X	[2000]
 3-2D7E	U+261E	# WHITE RIGHT POINTING INDEX	[2000]
-3-2E21		# <reserved>
+3-2E21	U+4FF1	# <cjk> [2004]
 3-2E22	U+2000B	# <cjk>	[2000]	[Unicode3.1]	Private: U+F780
 3-2E23	U+3402	# <cjk>	[2000]
 3-2E24	U+4E28	# <cjk>	[2000]
@@ -1429,7 +1429,7 @@
 3-2F7B	U+218BD	# <cjk>	[2000]	[Unicode3.1]	Private: U+F78F
 3-2F7C	U+5B19	# <cjk>	[2000]
 3-2F7D	U+5B25	# <cjk>	[2000]
-3-2F7E		# <reserved>
+3-2F7E	U+525D	# <cjk> [2004]
 3-3021	U+4E9C	# <cjk>
 3-3022	U+5516	# <cjk>
 3-3023	U+5A03	# <cjk>
@@ -4395,7 +4395,7 @@
 3-4F51	U+6E7E	# <cjk>
 3-4F52	U+7897	# <cjk>
 3-4F53	U+8155	# <cjk>
-3-4F54		# <reserved>
+3-4F54	U+20B9F	# <cjk> [2004]
 3-4F55	U+5B41	# <cjk>	[2000]
 3-4F56	U+5B56	# <cjk>	[2000]
 3-4F57	U+5B7D	# <cjk>	[2000]
@@ -4437,7 +4437,7 @@
 3-4F7B	U+5DA7	# <cjk>	[2000]
 3-4F7C	U+5DB8	# <cjk>	[2000]
 3-4F7D	U+5DCB	# <cjk>	[2000]
-3-4F7E		# <reserved>
+3-4F7E	U+541E	# <cjk> [2004]
 3-5021	U+5F0C	# <cjk>
 3-5022	U+4E10	# <cjk>
 3-5023	U+4E15	# <cjk>
@@ -7828,7 +7828,7 @@
 3-7424	U+7464	# <cjk>	[1983]
 3-7425	U+51DC	# <cjk>	[1990]
 3-7426	U+7199	# <cjk>	[1990]
-3-7427		# <reserved>
+3-7427	U+5653	# <cjk> [2004]
 3-7428	U+5DE2	# <cjk>	[2000]
 3-7429	U+5E14	# <cjk>	[2000]
 3-742A	U+5E18	# <cjk>	[2000]
@@ -8851,11 +8851,11 @@
 3-7E77	U+9F94	# <cjk>	[2000]
 3-7E78	U+9F97	# <cjk>	[2000]
 3-7E79	U+9FA2	# <cjk>	[2000]
-3-7E7A		# <reserved>
-3-7E7B		# <reserved>
-3-7E7C		# <reserved>
-3-7E7D		# <reserved>
-3-7E7E		# <reserved>
+3-7E7A	U+59F8	# <cjk> [2004]
+3-7E7B	U+5C5B	# <cjk> [2004]
+3-7E7C	U+5E77	# <cjk> [2004]
+3-7E7D	U+7626	# <cjk> [2004]
+3-7E7E	U+7E6B	# <cjk> [2004]
 4-2121	U+20089	# <cjk>	[2000]	[Unicode3.1]	Private: U+F7D1
 4-2122	U+4E02	# <cjk>	[2000]
 4-2123	U+4E0F	# <cjk>	[2000]
@@ -11138,7 +11138,7 @@
 4-7D38	U+9B10	# <cjk>	[2000]
 4-7D39	U+9B12	# <cjk>	[2000]
 4-7D3A	U+9B16	# <cjk>	[2000]
-4-7D3B	U+9B1D	# <cjk>	[2000]
+4-7D3B	U+9B1C	# <cjk>	[2000]
 4-7D3C	U+9B2B	# <cjk>	[2000]
 4-7D3D	U+9B33	# <cjk>	[2000]
 4-7D3E	U+9B3D	# <cjk>	[2000]


================================================
File: /Tools/unittestgui/README.txt
================================================
unittestgui.py is GUI framework and application for use with Python unit
testing framework. It executes tests written using the framework provided
by the 'unittest' module.

Based on the original by Steve Purcell, from:

  http://pyunit.sourceforge.net/

Updated for unittest test discovery by Mark Roddy and Python 3
support by Brian Curtin.

For details on how to make your tests work with test discovery,
and for explanations of the configuration options, see the unittest
documentation:

    http://docs.python.org/library/unittest.html#test-discovery


================================================
File: /Tools/unittestgui/unittestgui.py
================================================
#!/usr/bin/env python3
"""
GUI framework and application for use with Python unit testing framework.
Execute tests written using the framework provided by the 'unittest' module.

Updated for unittest test discovery by Mark Roddy and Python 3
support by Brian Curtin.

Based on the original by Steve Purcell, from:

  http://pyunit.sourceforge.net/

Copyright (c) 1999, 2000, 2001 Steve Purcell
This module is free software, and you may redistribute it and/or modify
it under the same terms as Python itself, so long as this copyright message
and disclaimer are retained in their original form.

IN NO EVENT SHALL THE AUTHOR BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT,
SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OF
THIS CODE, EVEN IF THE AUTHOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.

THE AUTHOR SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
PARTICULAR PURPOSE.  THE CODE PROVIDED HEREUNDER IS ON AN "AS IS" BASIS,
AND THERE IS NO OBLIGATION WHATSOEVER TO PROVIDE MAINTENANCE,
SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.
"""

__author__ = "Steve Purcell (stephen_purcell@yahoo.com)"

import sys
import traceback
import unittest

import tkinter as tk
from tkinter import messagebox
from tkinter import filedialog
from tkinter import simpledialog




##############################################################################
# GUI framework classes
##############################################################################

class BaseGUITestRunner(object):
    """Subclass this class to create a GUI TestRunner that uses a specific
    windowing toolkit. The class takes care of running tests in the correct
    manner, and making callbacks to the derived class to obtain information
    or signal that events have occurred.
    """
    def __init__(self, *args, **kwargs):
        self.currentResult = None
        self.running = 0
        self.__rollbackImporter = RollbackImporter()
        self.test_suite = None

        #test discovery variables
        self.directory_to_read = ''
        self.top_level_dir = ''
        self.test_file_glob_pattern = 'test*.py'

        self.initGUI(*args, **kwargs)

    def errorDialog(self, title, message):
        "Override to display an error arising from GUI usage"
        pass

    def getDirectoryToDiscover(self):
        "Override to prompt user for directory to perform test discovery"
        pass

    def runClicked(self):
        "To be called in response to user choosing to run a test"
        if self.running: return
        if not self.test_suite:
            self.errorDialog("Test Discovery", "You discover some tests first!")
            return
        self.currentResult = GUITestResult(self)
        self.totalTests = self.test_suite.countTestCases()
        self.running = 1
        self.notifyRunning()
        self.test_suite.run(self.currentResult)
        self.running = 0
        self.notifyStopped()

    def stopClicked(self):
        "To be called in response to user stopping the running of a test"
        if self.currentResult:
            self.currentResult.stop()

    def discoverClicked(self):
        self.__rollbackImporter.rollbackImports()
        directory = self.getDirectoryToDiscover()
        if not directory:
            return
        self.directory_to_read = directory
        try:
            # Explicitly use 'None' value if no top level directory is
            # specified (indicated by empty string) as discover() explicitly
            # checks for a 'None' to determine if no tld has been specified
            top_level_dir = self.top_level_dir or None
            tests = unittest.defaultTestLoader.discover(directory, self.test_file_glob_pattern, top_level_dir)
            self.test_suite = tests
        except:
            exc_type, exc_value, exc_tb = sys.exc_info()
            traceback.print_exception(*sys.exc_info())
            self.errorDialog("Unable to run test '%s'" % directory,
                             "Error loading specified test: %s, %s" % (exc_type, exc_value))
            return
        self.notifyTestsDiscovered(self.test_suite)

    # Required callbacks

    def notifyTestsDiscovered(self, test_suite):
        "Override to display information about the suite of discovered tests"
        pass

    def notifyRunning(self):
        "Override to set GUI in 'running' mode, enabling 'stop' button etc."
        pass

    def notifyStopped(self):
        "Override to set GUI in 'stopped' mode, enabling 'run' button etc."
        pass

    def notifyTestFailed(self, test, err):
        "Override to indicate that a test has just failed"
        pass

    def notifyTestErrored(self, test, err):
        "Override to indicate that a test has just errored"
        pass

    def notifyTestSkipped(self, test, reason):
        "Override to indicate that test was skipped"
        pass

    def notifyTestFailedExpectedly(self, test, err):
        "Override to indicate that test has just failed expectedly"
        pass

    def notifyTestStarted(self, test):
        "Override to indicate that a test is about to run"
        pass

    def notifyTestFinished(self, test):
        """Override to indicate that a test has finished (it may already have
           failed or errored)"""
        pass


class GUITestResult(unittest.TestResult):
    """A TestResult that makes callbacks to its associated GUI TestRunner.
    Used by BaseGUITestRunner. Need not be created directly.
    """
    def __init__(self, callback):
        unittest.TestResult.__init__(self)
        self.callback = callback

    def addError(self, test, err):
        unittest.TestResult.addError(self, test, err)
        self.callback.notifyTestErrored(test, err)

    def addFailure(self, test, err):
        unittest.TestResult.addFailure(self, test, err)
        self.callback.notifyTestFailed(test, err)

    def addSkip(self, test, reason):
        super(GUITestResult,self).addSkip(test, reason)
        self.callback.notifyTestSkipped(test, reason)

    def addExpectedFailure(self, test, err):
        super(GUITestResult,self).addExpectedFailure(test, err)
        self.callback.notifyTestFailedExpectedly(test, err)

    def stopTest(self, test):
        unittest.TestResult.stopTest(self, test)
        self.callback.notifyTestFinished(test)

    def startTest(self, test):
        unittest.TestResult.startTest(self, test)
        self.callback.notifyTestStarted(test)


class RollbackImporter:
    """This tricky little class is used to make sure that modules under test
    will be reloaded the next time they are imported.
    """
    def __init__(self):
        self.previousModules = sys.modules.copy()

    def rollbackImports(self):
        for modname in sys.modules.copy().keys():
            if not modname in self.previousModules:
                # Force reload when modname next imported
                del(sys.modules[modname])


##############################################################################
# Tkinter GUI
##############################################################################

class DiscoverSettingsDialog(simpledialog.Dialog):
    """
    Dialog box for prompting test discovery settings
    """

    def __init__(self, master, top_level_dir, test_file_glob_pattern, *args, **kwargs):
        self.top_level_dir = top_level_dir
        self.dirVar = tk.StringVar()
        self.dirVar.set(top_level_dir)

        self.test_file_glob_pattern = test_file_glob_pattern
        self.testPatternVar = tk.StringVar()
        self.testPatternVar.set(test_file_glob_pattern)

        simpledialog.Dialog.__init__(self, master, title="Discover Settings",
                                     *args, **kwargs)

    def body(self, master):
        tk.Label(master, text="Top Level Directory").grid(row=0)
        self.e1 = tk.Entry(master, textvariable=self.dirVar)
        self.e1.grid(row = 0, column=1)
        tk.Button(master, text="...",
                  command=lambda: self.selectDirClicked(master)).grid(row=0,column=3)

        tk.Label(master, text="Test File Pattern").grid(row=1)
        self.e2 = tk.Entry(master, textvariable = self.testPatternVar)
        self.e2.grid(row = 1, column=1)
        return None

    def selectDirClicked(self, master):
        dir_path = filedialog.askdirectory(parent=master)
        if dir_path:
            self.dirVar.set(dir_path)

    def apply(self):
        self.top_level_dir = self.dirVar.get()
        self.test_file_glob_pattern = self.testPatternVar.get()

class TkTestRunner(BaseGUITestRunner):
    """An implementation of BaseGUITestRunner using Tkinter.
    """
    def initGUI(self, root, initialTestName):
        """Set up the GUI inside the given root window. The test name entry
        field will be pre-filled with the given initialTestName.
        """
        self.root = root

        self.statusVar = tk.StringVar()
        self.statusVar.set("Idle")

        #tk vars for tracking counts of test result types
        self.runCountVar = tk.IntVar()
        self.failCountVar = tk.IntVar()
        self.errorCountVar = tk.IntVar()
        self.skipCountVar = tk.IntVar()
        self.expectFailCountVar = tk.IntVar()
        self.remainingCountVar = tk.IntVar()

        self.top = tk.Frame()
        self.top.pack(fill=tk.BOTH, expand=1)
        self.createWidgets()

    def getDirectoryToDiscover(self):
        return filedialog.askdirectory()

    def settingsClicked(self):
        d = DiscoverSettingsDialog(self.top, self.top_level_dir, self.test_file_glob_pattern)
        self.top_level_dir = d.top_level_dir
        self.test_file_glob_pattern = d.test_file_glob_pattern

    def notifyTestsDiscovered(self, test_suite):
        discovered = test_suite.countTestCases()
        self.runCountVar.set(0)
        self.failCountVar.set(0)
        self.errorCountVar.set(0)
        self.remainingCountVar.set(discovered)
        self.progressBar.setProgressFraction(0.0)
        self.errorListbox.delete(0, tk.END)
        self.statusVar.set("Discovering tests from %s. Found: %s" %
            (self.directory_to_read, discovered))
        self.stopGoButton['state'] = tk.NORMAL

    def createWidgets(self):
        """Creates and packs the various widgets.

        Why is it that GUI code always ends up looking a mess, despite all the
        best intentions to keep it tidy? Answers on a postcard, please.
        """
        # Status bar
        statusFrame = tk.Frame(self.top, relief=tk.SUNKEN, borderwidth=2)
        statusFrame.pack(anchor=tk.SW, fill=tk.X, side=tk.BOTTOM)
        tk.Label(statusFrame, width=1, textvariable=self.statusVar).pack(side=tk.TOP, fill=tk.X)

        # Area to enter name of test to run
        leftFrame = tk.Frame(self.top, borderwidth=3)
        leftFrame.pack(fill=tk.BOTH, side=tk.LEFT, anchor=tk.NW, expand=1)
        suiteNameFrame = tk.Frame(leftFrame, borderwidth=3)
        suiteNameFrame.pack(fill=tk.X)

        # Progress bar
        progressFrame = tk.Frame(leftFrame, relief=tk.GROOVE, borderwidth=2)
        progressFrame.pack(fill=tk.X, expand=0, anchor=tk.NW)
        tk.Label(progressFrame, text="Progress:").pack(anchor=tk.W)
        self.progressBar = ProgressBar(progressFrame, relief=tk.SUNKEN,
                                       borderwidth=2)
        self.progressBar.pack(fill=tk.X, expand=1)


        # Area with buttons to start/stop tests and quit
        buttonFrame = tk.Frame(self.top, borderwidth=3)
        buttonFrame.pack(side=tk.LEFT, anchor=tk.NW, fill=tk.Y)

        tk.Button(buttonFrame, text="Discover Tests",
                  command=self.discoverClicked).pack(fill=tk.X)


        self.stopGoButton = tk.Button(buttonFrame, text="Start",
                                      command=self.runClicked, state=tk.DISABLED)
        self.stopGoButton.pack(fill=tk.X)

        tk.Button(buttonFrame, text="Close",
                  command=self.top.quit).pack(side=tk.BOTTOM, fill=tk.X)
        tk.Button(buttonFrame, text="Settings",
                  command=self.settingsClicked).pack(side=tk.BOTTOM, fill=tk.X)

        # Area with labels reporting results
        for label, var in (('Run:', self.runCountVar),
                           ('Failures:', self.failCountVar),
                           ('Errors:', self.errorCountVar),
                           ('Skipped:', self.skipCountVar),
                           ('Expected Failures:', self.expectFailCountVar),
                           ('Remaining:', self.remainingCountVar),
                           ):
            tk.Label(progressFrame, text=label).pack(side=tk.LEFT)
            tk.Label(progressFrame, textvariable=var,
                     foreground="blue").pack(side=tk.LEFT, fill=tk.X,
                                             expand=1, anchor=tk.W)

        # List box showing errors and failures
        tk.Label(leftFrame, text="Failures and errors:").pack(anchor=tk.W)
        listFrame = tk.Frame(leftFrame, relief=tk.SUNKEN, borderwidth=2)
        listFrame.pack(fill=tk.BOTH, anchor=tk.NW, expand=1)
        self.errorListbox = tk.Listbox(listFrame, foreground='red',
                                       selectmode=tk.SINGLE,
                                       selectborderwidth=0)
        self.errorListbox.pack(side=tk.LEFT, fill=tk.BOTH, expand=1,
                               anchor=tk.NW)
        listScroll = tk.Scrollbar(listFrame, command=self.errorListbox.yview)
        listScroll.pack(side=tk.LEFT, fill=tk.Y, anchor=tk.N)
        self.errorListbox.bind("<Double-1>",
                               lambda e, self=self: self.showSelectedError())
        self.errorListbox.configure(yscrollcommand=listScroll.set)

    def errorDialog(self, title, message):
        messagebox.showerror(parent=self.root, title=title,
                             message=message)

    def notifyRunning(self):
        self.runCountVar.set(0)
        self.failCountVar.set(0)
        self.errorCountVar.set(0)
        self.remainingCountVar.set(self.totalTests)
        self.errorInfo = []
        while self.errorListbox.size():
            self.errorListbox.delete(0)
        #Stopping seems not to work, so simply disable the start button
        #self.stopGoButton.config(command=self.stopClicked, text="Stop")
        self.stopGoButton.config(state=tk.DISABLED)
        self.progressBar.setProgressFraction(0.0)
        self.top.update_idletasks()

    def notifyStopped(self):
        self.stopGoButton.config(state=tk.DISABLED)
        #self.stopGoButton.config(command=self.runClicked, text="Start")
        self.statusVar.set("Idle")

    def notifyTestStarted(self, test):
        self.statusVar.set(str(test))
        self.top.update_idletasks()

    def notifyTestFailed(self, test, err):
        self.failCountVar.set(1 + self.failCountVar.get())
        self.errorListbox.insert(tk.END, "Failure: %s" % test)
        self.errorInfo.append((test,err))

    def notifyTestErrored(self, test, err):
        self.errorCountVar.set(1 + self.errorCountVar.get())
        self.errorListbox.insert(tk.END, "Error: %s" % test)
        self.errorInfo.append((test,err))

    def notifyTestSkipped(self, test, reason):
        super(TkTestRunner, self).notifyTestSkipped(test, reason)
        self.skipCountVar.set(1 + self.skipCountVar.get())

    def notifyTestFailedExpectedly(self, test, err):
        super(TkTestRunner, self).notifyTestFailedExpectedly(test, err)
        self.expectFailCountVar.set(1 + self.expectFailCountVar.get())


    def notifyTestFinished(self, test):
        self.remainingCountVar.set(self.remainingCountVar.get() - 1)
        self.runCountVar.set(1 + self.runCountVar.get())
        fractionDone = float(self.runCountVar.get())/float(self.totalTests)
        fillColor = len(self.errorInfo) and "red" or "green"
        self.progressBar.setProgressFraction(fractionDone, fillColor)

    def showSelectedError(self):
        selection = self.errorListbox.curselection()
        if not selection: return
        selected = int(selection[0])
        txt = self.errorListbox.get(selected)
        window = tk.Toplevel(self.root)
        window.title(txt)
        window.protocol('WM_DELETE_WINDOW', window.quit)
        test, error = self.errorInfo[selected]
        tk.Label(window, text=str(test),
                 foreground="red", justify=tk.LEFT).pack(anchor=tk.W)
        tracebackLines =  traceback.format_exception(*error)
        tracebackText = "".join(tracebackLines)
        tk.Label(window, text=tracebackText, justify=tk.LEFT).pack()
        tk.Button(window, text="Close",
                  command=window.quit).pack(side=tk.BOTTOM)
        window.bind('<Key-Return>', lambda e, w=window: w.quit())
        window.mainloop()
        window.destroy()


class ProgressBar(tk.Frame):
    """A simple progress bar that shows a percentage progress in
    the given colour."""

    def __init__(self, *args, **kwargs):
        tk.Frame.__init__(self, *args, **kwargs)
        self.canvas = tk.Canvas(self, height='20', width='60',
                                background='white', borderwidth=3)
        self.canvas.pack(fill=tk.X, expand=1)
        self.rect = self.text = None
        self.canvas.bind('<Configure>', self.paint)
        self.setProgressFraction(0.0)

    def setProgressFraction(self, fraction, color='blue'):
        self.fraction = fraction
        self.color = color
        self.paint()
        self.canvas.update_idletasks()

    def paint(self, *args):
        totalWidth = self.canvas.winfo_width()
        width = int(self.fraction * float(totalWidth))
        height = self.canvas.winfo_height()
        if self.rect is not None: self.canvas.delete(self.rect)
        if self.text is not None: self.canvas.delete(self.text)
        self.rect = self.canvas.create_rectangle(0, 0, width, height,
                                                 fill=self.color)
        percentString = "%3.0f%%" % (100.0 * self.fraction)
        self.text = self.canvas.create_text(totalWidth/2, height/2,
                                            anchor=tk.CENTER,
                                            text=percentString)

def main(initialTestName=""):
    root = tk.Tk()
    root.title("PyUnit")
    runner = TkTestRunner(root, initialTestName)
    root.protocol('WM_DELETE_WINDOW', root.quit)
    root.mainloop()


if __name__ == '__main__':
    if len(sys.argv) == 2:
        main(sys.argv[1])
    else:
        main()


================================================
File: /Tools/wasm/README.md
================================================
# Python WebAssembly (WASM) build

**WASI support is [tier 2](https://peps.python.org/pep-0011/#tier-2).**
**Emscripten support is [tier 3](https://peps.python.org/pep-0011/#tier-3).**

This directory contains configuration and helpers to facilitate cross
compilation of CPython to WebAssembly (WASM). Python supports Emscripten
(*wasm32-emscripten*) and WASI (*wasm32-wasi*) targets. Emscripten builds
run in modern browsers and JavaScript runtimes like *Node.js*. WASI builds
use WASM runtimes such as *wasmtime*.

Users and developers are encouraged to use the script
`Tools/wasm/wasm_build.py`. The tool automates the build process and provides
assistance with installation of SDKs, running tests, etc.

**NOTE**: If you are looking for information that is not directly related to
building CPython for WebAssembly (or the resulting build), please see
https://github.com/psf/webassembly for more information.

## wasm32-emscripten

### Build

To cross compile to the ``wasm32-emscripten`` platform you need
[the Emscripten compiler toolchain](https://emscripten.org/), 
a Python interpreter, and an installation of Node version 18 or newer.
Emscripten version 3.1.73 or newer is recommended. All commands below are
relative to a checkout of the Python repository.

#### Install [the Emscripten compiler toolchain](https://emscripten.org/docs/getting_started/downloads.html)

You can install the Emscripten toolchain as follows:
```shell
git clone https://github.com/emscripten-core/emsdk.git --depth 1
./emsdk/emsdk install latest
./emsdk/emsdk activate latest
```
To add the Emscripten compiler to your path:
```shell
source ./emsdk/emsdk_env.sh
```
This adds `emcc` and `emconfigure` to your path.

##### Optionally: enable ccache for EMSDK

The ``EM_COMPILER_WRAPPER`` must be set after the EMSDK environment is
sourced. Otherwise the source script removes the environment variable.

```shell
export EM_COMPILER_WRAPPER=ccache
```

#### Compile and build Python interpreter

You can use `python Tools/wasm/emscripten` to compile and build targetting
Emscripten. You can do everything at once with:
```shell
python Tools/wasm/emscripten build
```
or you can break it out into four separate steps:
```shell
python Tools/wasm/emscripten configure-build-python
python Tools/wasm/emscripten make-build-python
python Tools/wasm/emscripten make-libffi
python Tools/wasm/emscripten configure-host
python Tools/wasm/emscripten make-host
```
Extra arguments to the configure steps are passed along to configure. For
instance, to do a debug build, you can use:
```shell
python Tools/wasm/emscripten build --with-py-debug
```

### Running from node

If you want to run the normal Python CLI, you can use `python.sh`. It takes the
same options as the normal Python CLI entrypoint, though the REPL does not
function and will crash.

`python.sh` invokes `node_entry.mjs` which imports the Emscripten module for the
Python process and starts it up with the appropriate settings. If you wish to
make a node application that "embeds" the interpreter instead of acting like the
CLI you will need to write your own alternative to `node_entry.mjs`.


### The Web Example

When building for Emscripten, the web example will be built automatically. It is
in the ``web_example`` directory. To run the web example, ``cd`` into the
``web_example`` directory, then run ``python server.py``. This will start a web
server; you can then visit ``http://localhost:8000/python.html`` in a browser to
see a simple REPL example.

The web example relies on a bug fix in Emscripten version 3.1.73 so if you build
with earlier versions of Emscripten it may not work. The web example uses
``SharedArrayBuffer``. For security reasons browsers only provide
``SharedArrayBuffer`` in secure environments with cross-origin isolation. The
webserver must send cross-origin headers and correct MIME types for the
JavaScript and WebAssembly files. Otherwise the terminal will fail to load with
an error message like ``ReferenceError: SharedArrayBuffer is not defined``. See
more information here:
https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/SharedArrayBuffer#security_requirements

Note that ``SharedArrayBuffer`` is _not required_ to use Python itself, only the
web example. If cross-origin isolation is not appropriate for your use case you
may make your own application embedding `python.mjs` which does not use
``SharedArrayBuffer`` and serve it without the cross-origin isolation headers.

### Embedding Python in a custom JavaScript application

You can look at `python.worker.mjs` and `node_entry.mjs` for inspiration. At a
minimum you must import ``createEmscriptenModule`` and you need to call
``createEmscriptenModule`` with an appropriate settings object. This settings
object will need a prerun hook that installs the Python standard library into
the Emscripten file system.

#### NodeJs

In Node, you can use the NodeFS to mount the standard library in your native
file system into the Emscripten file system:
```js
import createEmscriptenModule from "./python.mjs";

await createEmscriptenModule({
    preRun(Module) {
        Module.FS.mount(
            Module.FS.filesystems.NODEFS,
            { root: "/path/to/python/stdlib" },
            "/lib/",
        );
    },
});
```

#### Browser

In the browser, the simplest approach is to put the standard library in a zip
file it and install it. With Python 3.14 this could look like:
```js
import createEmscriptenModule from "./python.mjs";

await createEmscriptenModule({
  async preRun(Module) {
    Module.FS.mkdirTree("/lib/python3.14/lib-dynload/");
    Module.addRunDependency("install-stdlib");
    const resp = await fetch("python3.14.zip");
    const stdlibBuffer = await resp.arrayBuffer();
    Module.FS.writeFile(`/lib/python314.zip`, new Uint8Array(stdlibBuffer), {
      canOwn: true,
    });
    Module.removeRunDependency("install-stdlib");
  },
});
```

### Limitations and issues

#### Network stack

- Python's socket module does not work with Emscripten's emulated POSIX
  sockets yet. Network modules like ``asyncio``, ``urllib``, ``selectors``,
  etc. are not available.
- Only ``AF_INET`` and ``AF_INET6`` with ``SOCK_STREAM`` (TCP) or
  ``SOCK_DGRAM`` (UDP) are available. ``AF_UNIX`` is not supported.
- ``socketpair`` does not work.
- Blocking sockets are not available and non-blocking sockets don't work
  correctly, e.g. ``socket.accept`` crashes the runtime. ``gethostbyname``
  does not resolve to a real IP address. IPv6 is not available.
- The ``select`` module is limited. ``select.select()`` crashes the runtime
  due to lack of exectfd support.

#### processes, signals

- Processes are not supported. System calls like fork, popen, and subprocess
  fail with ``ENOSYS`` or ``ENOSUP``.
- Signal support is limited. ``signal.alarm``, ``itimer``, ``sigaction``
  are not available or do not work correctly. ``SIGTERM`` exits the runtime.
- Keyboard interrupt (CTRL+C) handling is not implemented yet.
- Resource-related functions like ``os.nice`` and most functions of the
  ``resource`` module are not available.

#### threading

- Threading is disabled by default. The ``configure`` option
  ``--enable-wasm-pthreads`` adds compiler flag ``-pthread`` and
  linker flags ``-sUSE_PTHREADS -sPROXY_TO_PTHREAD``.
- pthread support requires WASM threads and SharedArrayBuffer (bulk memory).
  The Node.JS runtime keeps a pool of web workers around. Each web worker
  uses several file descriptors (eventfd, epoll, pipe).
- It's not advised to enable threading when building for browsers or with
  dynamic linking support; there are performance and stability issues.

#### file system

- Most user, group, and permission related function and modules are not
  supported or don't work as expected, e.g.``pwd`` module, ``grp`` module,
  ``os.setgroups``, ``os.chown``, and so on. ``lchown`` and ``lchmod`` are
  not available.
- ``umask`` is a no-op.
- hard links (``os.link``) are not supported.
- Offset and iovec I/O functions (e.g. ``os.pread``, ``os.preadv``) are not
  available.
- ``os.mknod`` and ``os.mkfifo``
  [don't work](https://github.com/emscripten-core/emscripten/issues/16158)
  and are disabled.
- Large file support crashes the runtime and is disabled.
- ``mmap`` module is unstable. flush (``msync``) can crash the runtime.

#### Misc

- Heap memory and stack size are limited. Recursion or extensive memory
  consumption can crash Python.
- Most stdlib modules with a dependency on external libraries are missing,
  e.g. ``ctypes``, ``readline``, ``ssl``, and more.
- Shared extension modules are not implemented yet. All extension modules
  are statically linked into the main binary. The experimental configure
  option ``--enable-wasm-dynamic-linking`` enables dynamic extensions
  supports. It's currently known to crash in combination with threading.
- glibc extensions for date and time formatting are not available.
- ``locales`` module is affected by musl libc issues,
  [gh-90548](https://github.com/python/cpython/issues/90548).
- Python's object allocator ``obmalloc`` is disabled by default.
- ``ensurepip`` is not available.

#### In the browser

- The interactive shell does not handle copy 'n paste and unicode support
  well.
- The bundled stdlib is limited. Network-related modules,
  multiprocessing, dbm, tests and similar modules
  are not shipped. All other modules are bundled as pre-compiled
  ``pyc`` files.
- In-memory file system (MEMFS) is not persistent and limited.
- Test modules are disabled by default. Use ``--enable-test-modules`` build
  test modules like ``_testcapi``.

## WASI (wasm32-wasi)

See [the devguide on how to build and run for WASI](https://devguide.python.org/getting-started/setup-building/#wasi).

## Detecting WebAssembly builds

### Python code

```python
import os, sys

if sys.platform == "emscripten":
    # Python on Emscripten
    ...
if sys.platform == "wasi":
    # Python on WASI
    ...

if os.name == "posix":
    # WASM platforms identify as POSIX-like.
    # Windows does not provide os.uname().
    machine = os.uname().machine
    if machine.startswith("wasm"):
        # WebAssembly (wasm32, wasm64 potentially in the future)
```

```python
>>> import os, sys
>>> os.uname()
posix.uname_result(
    sysname='Emscripten',
    nodename='emscripten',
    release='3.1.19',
    version='#1',
    machine='wasm32'
)
>>> os.name
'posix'
>>> sys.platform
'emscripten'
>>> sys._emscripten_info
sys._emscripten_info(
    emscripten_version=(3, 1, 10),
    runtime='Mozilla/5.0 (X11; Linux x86_64; rv:104.0) Gecko/20100101 Firefox/104.0',
    pthreads=False,
    shared_memory=False
)
```

```python
>>> sys._emscripten_info
sys._emscripten_info(
    emscripten_version=(3, 1, 19),
    runtime='Node.js v14.18.2',
    pthreads=True,
    shared_memory=True
)
```

```python
>>> import os, sys
>>> os.uname()
posix.uname_result(
    sysname='wasi',
    nodename='(none)',
    release='0.0.0',
    version='0.0.0',
    machine='wasm32'
)
>>> os.name
'posix'
>>> sys.platform
'wasi'
```

### C code

Emscripten SDK and WASI SDK define several built-in macros. You can dump a
full list of built-ins with ``emcc -dM -E - < /dev/null`` and
``/path/to/wasi-sdk/bin/clang -dM -E - < /dev/null``.

* WebAssembly ``__wasm__`` (also ``__wasm``)
* wasm32 ``__wasm32__`` (also ``__wasm32``)
* wasm64 ``__wasm64__``
* Emscripten ``__EMSCRIPTEN__`` (also ``EMSCRIPTEN``)
* Emscripten version ``__EMSCRIPTEN_major__``, ``__EMSCRIPTEN_minor__``, ``__EMSCRIPTEN_tiny__``
* WASI ``__wasi__``


================================================
File: /Tools/wasm/config.site-wasm32-emscripten
================================================
# config.site override for cross compiling to wasm32-emscripten platform
#
# CONFIG_SITE=Tools/wasm/config.site-wasm32-emscripten \
#     emconfigure ./configure --host=wasm32-unknown-emscripten --build=...
#
# Written by Christian Heimes <christian@python.org>
# Partly based on pyodide's pyconfig.undefs.h file.
#

# cannot be detected in cross builds
ac_cv_buggy_getaddrinfo=no

# Emscripten has no /dev/pt*
ac_cv_file__dev_ptmx=no
ac_cv_file__dev_ptc=no

# new undefined symbols / unsupported features
ac_cv_func_posix_spawn=no
ac_cv_func_posix_spawnp=no
ac_cv_func_eventfd=no
ac_cv_func_memfd_create=no
ac_cv_func_prlimit=no

# unsupported syscall, https://github.com/emscripten-core/emscripten/issues/13393
ac_cv_func_shutdown=no

# The rest is based on pyodide
# https://github.com/pyodide/pyodide/blob/main/cpython/pyconfig.undefs.h

ac_cv_func_epoll_create=no
ac_cv_func_epoll_create1=no
ac_cv_header_linux_vm_sockets_h=no
ac_cv_func_socketpair=no
ac_cv_func_sigaction=no

# symlinkat is implemented, but fails
ac_cv_func_symlinkat=no

# lchmod/lchown are implemented, but fail with ENOTSUP.
ac_cv_func_lchmod=no
ac_cv_func_lchown=no

# geteuid / getegid are stubs and always return 0 (root). The stub breaks
# code that assume effective user root has special permissions.
ac_cv_func_geteuid=no
ac_cv_func_getegid=no
ac_cv_func_seteuid=no
ac_cv_func_setegid=no
ac_cv_func_getresuid=no
ac_cv_func_getresgid=no
ac_cv_func_setresuid=no
ac_cv_func_setresgid=no

# Syscalls not implemented in emscripten
# [Errno 52] Function not implemented
ac_cv_func_preadv2=no
ac_cv_func_preadv=no
ac_cv_func_pwritev2=no
ac_cv_func_pwritev=no
ac_cv_func_pipe2=no
ac_cv_func_nice=no
ac_cv_func_setpriority=no
ac_cv_func_setitimer=no
# unsupported syscall: __syscall_prlimit64
ac_cv_func_prlimit=no
# unsupported syscall: __syscall_getrusage
ac_cv_func_getrusage=no
ac_cv_func_posix_fallocate=no

# Syscalls that resulted in a segfault
ac_cv_func_utimensat=no
ac_cv_header_sys_ioctl_h=no

# sockets are supported, but only AF_INET / AF_INET6 in non-blocking mode.
# Disable AF_UNIX and AF_PACKET support, see socketmodule.h.
ac_cv_header_sys_un_h=no
ac_cv_header_netpacket_packet_h=no

# aborts with bad ioctl
ac_cv_func_openpty=no
ac_cv_func_forkpty=no

# mkfifo and mknod are broken, create regular file
ac_cv_func_mkfifo=no
ac_cv_func_mkfifoat=no
ac_cv_func_mknod=no
ac_cv_func_mknodat=no

# always fails with permission or not implemented error
ac_cv_func_getgroups=no
ac_cv_func_setgroups=no
ac_cv_func_setresuid=no
ac_cv_func_setresgid=no

# Emscripten does not support hard links, always fails with errno 34
# "Too many links". See emscripten_syscall_stubs.c
ac_cv_func_link=no
ac_cv_func_linkat=no

# Emscripten's faccessat does not accept AT_* flags.
ac_cv_func_faccessat=no

# alarm signal is not delivered, may need a callback into the event loop?
ac_cv_func_alarm=no


================================================
File: /Tools/wasm/config.site-wasm32-wasi
================================================
# config.site override for cross compiling to wasm32-wasi platform
#
# Written by Christian Heimes <christian@python.org>

# cannot be detected in cross builds
ac_cv_buggy_getaddrinfo=no

# WASI has no /dev/pt*
ac_cv_file__dev_ptmx=no
ac_cv_file__dev_ptc=no

# get/setrlimit are not supported
ac_cv_header_sys_resource_h=no

# undefined symbols / unsupported features
ac_cv_func_eventfd=no

# WASI SDK 15.0 has no pipe syscall.
ac_cv_func_pipe=no

# WASI SDK 15.0 cannot create fifos and special files.
ac_cv_func_mkfifo=no
ac_cv_func_mkfifoat=no
ac_cv_func_mknod=no
ac_cv_func_mknodat=no
ac_cv_func_makedev=no

# fdopendir() fails on SDK 15.0,
# OSError: [Errno 28] Invalid argument: '.'
ac_cv_func_fdopendir=no

# WASI sockets are limited to operations on given socket fd and inet sockets.
# Disable AF_UNIX and AF_PACKET support, see socketmodule.h.
ac_cv_header_sys_un_h=no
ac_cv_header_netpacket_packet_h=no

# disable accept for WASM runtimes without sock_accept
#ac_cv_func_accept=no
#ac_cv_func_accept4=no

# Disable int-conversion for wask-sdk as it triggers an error from version 17.
ac_cv_disable_int_conversion=yes

# preadv(), readv(), pwritev(), and writev() under wasmtime's WASI 0.2 support
# do not use more than the first buffer provided, failing under test_posix.
# Since wasmtime will not be changing this behaviour, disable the functions.
# https://github.com/bytecodealliance/wasmtime/issues/7830
ac_cv_func_preadv=no
ac_cv_func_readv=no
ac_cv_func_pwritev=no
ac_cv_func_writev=no

# WASI SDK 22 added multiple stubs which we don't implement.
# https://github.com/python/cpython/issues/120371
ac_cv_func_chmod=no
ac_cv_func_fchmod=no
ac_cv_func_fchmodat=no
ac_cv_func_statvfs=no
ac_cv_func_fstatvfs=no


================================================
File: /Tools/wasm/mypy.ini
================================================
[mypy]
files = Tools/wasm/wasm_*.py
pretty = True
show_traceback = True

# Make sure the wasm can be run using Python 3.8:
python_version = 3.8

# Be strict...
strict = True
enable_error_code = truthy-bool,ignore-without-code


================================================
File: /Tools/wasm/wasi-env
================================================
#!/bin/sh
set -e

# NOTE: to be removed once no longer used in https://github.com/python/buildmaster-config/blob/main/master/custom/factories.py .

# function
usage() {
    echo "wasi-env - Run command with WASI-SDK"
    echo ""
    echo "wasi-env is a helper to set various environment variables to"
    echo "run configure and make with WASI-SDK. A WASI-SDK must be either"
    echo "installed at /opt/wasi-sdk or the env var 'WASI_SDK_PATH' must"
    echo "set to the root of a WASI-SDK."
    echo ""
    echo "Usage: wasi-env command [...]"
    echo ""
    echo "    -h --help     display this help and exit"
    echo ""
}

case $1 in
    -h|--help)
        usage
        exit
        ;;
esac

if test -z "$1"; then
    echo "ERROR: command required" >&2
    usage
    exit 1
fi

WASI_SDK_PATH="${WASI_SDK_PATH:-/opt/wasi-sdk}"
WASI_SYSROOT="${WASI_SDK_PATH}/share/wasi-sysroot"

if ! test -x "${WASI_SDK_PATH}/bin/clang"; then
    echo "Error: ${WASI_SDK_PATH}/bin/clang does not exist." >&2
    exit 2
fi

CC="${WASI_SDK_PATH}/bin/clang"
CPP="${WASI_SDK_PATH}/bin/clang-cpp"
CXX="${WASI_SDK_PATH}/bin/clang++"

# --sysroot is required if WASI-SDK is not installed in /opt/wasi-sdk.
if test "${WASI_SDK_PATH}" != "/opt/wasi-sdk"; then
    CC="${CC} --sysroot=${WASI_SYSROOT}"
    CPP="${CPP} --sysroot=${WASI_SYSROOT}"
    CXX="${CXX} --sysroot=${WASI_SYSROOT}"
fi

# use ccache if available
if command -v ccache >/dev/null 2>&1; then
    CC="ccache ${CC}"
    CPP="ccache ${CPP}"
    CXX="ccache ${CXX}"
fi

AR="${WASI_SDK_PATH}/bin/llvm-ar"
RANLIB="${WASI_SDK_PATH}/bin/ranlib"

# instruct pkg-config to use sysroot
PKG_CONFIG_PATH=""
PKG_CONFIG_LIBDIR="${WASI_SYSROOT}/lib/pkgconfig:${WASI_SYSROOT}/share/pkgconfig"
PKG_CONFIG_SYSROOT_DIR="${WASI_SYSROOT}"

PATH="${WASI_SDK_PATH}/bin:${PATH}"

export WASI_SDK_PATH WASI_SYSROOT
export CC CPP CXX LDSHARED AR RANLIB
export CFLAGS LDFLAGS
export PKG_CONFIG_PATH PKG_CONFIG_LIBDIR PKG_CONFIG_SYSROOT_DIR
export PATH

# no exec, it makes argv[0] path absolute.
"$@"


================================================
File: /Tools/wasm/wasi.py
================================================
#!/usr/bin/env python3

import argparse
import contextlib
import functools
import os
try:
    from os import process_cpu_count as cpu_count
except ImportError:
    from os import cpu_count
import pathlib
import shutil
import subprocess
import sys
import sysconfig
import tempfile


CHECKOUT = pathlib.Path(__file__).parent.parent.parent

CROSS_BUILD_DIR = CHECKOUT / "cross-build"
BUILD_DIR = CROSS_BUILD_DIR / "build"

LOCAL_SETUP = CHECKOUT / "Modules" / "Setup.local"
LOCAL_SETUP_MARKER = "# Generated by Tools/wasm/wasi.py\n".encode("utf-8")

WASMTIME_VAR_NAME = "WASMTIME"
WASMTIME_HOST_RUNNER_VAR = f"{{{WASMTIME_VAR_NAME}}}"


def updated_env(updates={}):
    """Create a new dict representing the environment to use.

    The changes made to the execution environment are printed out.
    """
    env_defaults = {}
    # https://reproducible-builds.org/docs/source-date-epoch/
    git_epoch_cmd = ["git", "log", "-1", "--pretty=%ct"]
    try:
        epoch = subprocess.check_output(git_epoch_cmd, encoding="utf-8").strip()
        env_defaults["SOURCE_DATE_EPOCH"] = epoch
    except subprocess.CalledProcessError:
        pass  # Might be building from a tarball.
    # This layering lets SOURCE_DATE_EPOCH from os.environ takes precedence.
    environment = env_defaults | os.environ | updates

    env_diff = {}
    for key, value in environment.items():
        if os.environ.get(key) != value:
            env_diff[key] = value

    print("🌎 Environment changes:")
    for key in sorted(env_diff.keys()):
        print(f"  {key}={env_diff[key]}")

    return environment


def subdir(working_dir, *, clean_ok=False):
    """Decorator to change to a working directory."""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(context):
            nonlocal working_dir

            if callable(working_dir):
                working_dir = working_dir(context)
            try:
                tput_output = subprocess.check_output(["tput", "cols"],
                                                      encoding="utf-8")
            except subprocess.CalledProcessError:
                terminal_width = 80
            else:
                terminal_width = int(tput_output.strip())
            print("⎯" * terminal_width)
            print("📁", working_dir)
            if (clean_ok and getattr(context, "clean", False) and
                working_dir.exists()):
                print(f"🚮 Deleting directory (--clean)...")
                shutil.rmtree(working_dir)

            working_dir.mkdir(parents=True, exist_ok=True)

            with contextlib.chdir(working_dir):
                return func(context, working_dir)

        return wrapper

    return decorator


def call(command, *, quiet, **kwargs):
    """Execute a command.

    If 'quiet' is true, then redirect stdout and stderr to a temporary file.
    """
    print("❯", " ".join(map(str, command)))
    if not quiet:
        stdout = None
        stderr = None
    else:
        stdout = tempfile.NamedTemporaryFile("w", encoding="utf-8",
                                             delete=False,
                                             prefix="cpython-wasi-",
                                             suffix=".log")
        stderr = subprocess.STDOUT
        print(f"📝 Logging output to {stdout.name} (--quiet)...")

    subprocess.check_call(command, **kwargs, stdout=stdout, stderr=stderr)


def build_platform():
    """The name of the build/host platform."""
    # Can also be found via `config.guess`.`
    return sysconfig.get_config_var("BUILD_GNU_TYPE")


def build_python_path():
    """The path to the build Python binary."""
    binary = BUILD_DIR / "python"
    if not binary.is_file():
        binary = binary.with_suffix(".exe")
        if not binary.is_file():
            raise FileNotFoundError("Unable to find `python(.exe)` in "
                                    f"{BUILD_DIR}")

    return binary


@subdir(BUILD_DIR, clean_ok=True)
def configure_build_python(context, working_dir):
    """Configure the build/host Python."""
    if LOCAL_SETUP.exists():
        print(f"👍 {LOCAL_SETUP} exists ...")
    else:
        print(f"📝 Touching {LOCAL_SETUP} ...")
        LOCAL_SETUP.write_bytes(LOCAL_SETUP_MARKER)

    configure = [os.path.relpath(CHECKOUT / 'configure', working_dir)]
    if context.args:
        configure.extend(context.args)

    call(configure, quiet=context.quiet)


@subdir(BUILD_DIR)
def make_build_python(context, working_dir):
    """Make/build the build Python."""
    call(["make", "--jobs", str(cpu_count()), "all"],
            quiet=context.quiet)

    binary = build_python_path()
    cmd = [binary, "-c",
            "import sys; "
            "print(f'{sys.version_info.major}.{sys.version_info.minor}')"]
    version = subprocess.check_output(cmd, encoding="utf-8").strip()

    print(f"🎉 {binary} {version}")


def find_wasi_sdk():
    """Find the path to wasi-sdk."""
    if wasi_sdk_path := os.environ.get("WASI_SDK_PATH"):
        return pathlib.Path(wasi_sdk_path)
    elif (default_path := pathlib.Path("/opt/wasi-sdk")).exists():
        return default_path


def wasi_sdk_env(context):
    """Calculate environment variables for building with wasi-sdk."""
    wasi_sdk_path = context.wasi_sdk_path
    sysroot = wasi_sdk_path / "share" / "wasi-sysroot"
    env = {"CC": "clang", "CPP": "clang-cpp", "CXX": "clang++",
           "AR": "llvm-ar", "RANLIB": "ranlib"}

    for env_var, binary_name in list(env.items()):
        env[env_var] = os.fsdecode(wasi_sdk_path / "bin" / binary_name)

    if wasi_sdk_path != pathlib.Path("/opt/wasi-sdk"):
        for compiler in ["CC", "CPP", "CXX"]:
            env[compiler] += f" --sysroot={sysroot}"

    env["PKG_CONFIG_PATH"] = ""
    env["PKG_CONFIG_LIBDIR"] = os.pathsep.join(
                                map(os.fsdecode,
                                    [sysroot / "lib" / "pkgconfig",
                                     sysroot / "share" / "pkgconfig"]))
    env["PKG_CONFIG_SYSROOT_DIR"] = os.fsdecode(sysroot)

    env["WASI_SDK_PATH"] = os.fsdecode(wasi_sdk_path)
    env["WASI_SYSROOT"] = os.fsdecode(sysroot)

    env["PATH"] = os.pathsep.join([os.fsdecode(wasi_sdk_path / "bin"),
                                   os.environ["PATH"]])

    return env


@subdir(lambda context: CROSS_BUILD_DIR / context.host_triple, clean_ok=True)
def configure_wasi_python(context, working_dir):
    """Configure the WASI/host build."""
    if not context.wasi_sdk_path or not context.wasi_sdk_path.exists():
        raise ValueError("WASI-SDK not found; "
                        "download from "
                        "https://github.com/WebAssembly/wasi-sdk and/or "
                        "specify via $WASI_SDK_PATH or --wasi-sdk")

    config_site = os.fsdecode(CHECKOUT / "Tools" / "wasm" / "config.site-wasm32-wasi")

    wasi_build_dir = working_dir.relative_to(CHECKOUT)

    python_build_dir = BUILD_DIR / "build"
    lib_dirs = list(python_build_dir.glob("lib.*"))
    assert len(lib_dirs) == 1, f"Expected a single lib.* directory in {python_build_dir}"
    lib_dir = os.fsdecode(lib_dirs[0])
    pydebug = lib_dir.endswith("-pydebug")
    python_version = lib_dir.removesuffix("-pydebug").rpartition("-")[-1]
    sysconfig_data = f"{wasi_build_dir}/build/lib.wasi-wasm32-{python_version}"
    if pydebug:
        sysconfig_data += "-pydebug"

    # Use PYTHONPATH to include sysconfig data which must be anchored to the
    # WASI guest's `/` directory.
    args = {"GUEST_DIR": "/",
            "HOST_DIR": CHECKOUT,
