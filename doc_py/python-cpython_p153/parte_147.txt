        kind = 'static' if static else None
    elif excname:
        name = f'_PyExc_{excname}'
        isdecl = False
        kind = 'static'
    else:
        isdecl = True
        if static:
            kind = 'static'
        elif extern:
            kind = 'extern'
        elif capi:
            kind = 'capi'
        else:
            kind = None
    return name, isdecl, kind


class BuiltinTypeDecl(namedtuple('BuiltinTypeDecl', 'file lno name kind')):

    KINDS = {
        'static',
        'extern',
        'capi',
        'forward',
    }

    @classmethod
    def from_line(cls, line, filename, lno):
        # This is similar to ._capi.CAPIItem.from_line().
        parsed = _parse_line(line)
        if not parsed:
            return None
        name, isdecl, kind = parsed
        if not isdecl:
            return None
        return cls.from_parsed(name, kind, filename, lno)

    @classmethod
    def from_parsed(cls, name, kind, filename, lno):
        if not kind:
            kind = 'forward'
        return cls.from_values(filename, lno, name, kind)

    @classmethod
    def from_values(cls, filename, lno, name, kind):
        if kind not in cls.KINDS:
            raise ValueError(f'unsupported kind {kind!r}')
        self = cls(filename, lno, name, kind)
        if self.kind not in ('extern', 'capi') and self.api:
            raise NotImplementedError(self)
        elif self.kind == 'capi' and not self.api:
            raise NotImplementedError(self)
        return self

    @property
    def relfile(self):
        return self.file[len(REPO_ROOT) + 1:]

    @property
    def api(self):
        return self.relfile.startswith(CAPI_PREFIX)

    @property
    def internal(self):
        return self.relfile.startswith(INTERNAL_PREFIX)

    @property
    def private(self):
        if not self.name.startswith('_'):
            return False
        return self.api and not self.internal

    @property
    def public(self):
        if self.kind != 'capi':
            return False
        return not self.internal and not self.private


class BuiltinTypeInfo(namedtuple('BuiltinTypeInfo', 'file lno name static decl')):

    @classmethod
    def from_line(cls, line, filename, lno, *, decls=None):
        parsed = _parse_line(line)
        if not parsed:
            return None
        name, isdecl, kind = parsed
        if isdecl:
            return None
        return cls.from_parsed(name, kind, filename, lno, decls=decls)

    @classmethod
    def from_parsed(cls, name, kind, filename, lno, *, decls=None):
        if not kind:
            static = False
        elif kind == 'static':
            static = True
        else:
            raise NotImplementedError((filename, line, kind))
        decl = decls.get(name) if decls else None
        return cls(filename, lno, name, static, decl)

    @property
    def relfile(self):
        return self.file[len(REPO_ROOT) + 1:]

    @property
    def exported(self):
        return not self.static

    @property
    def api(self):
        if not self.decl:
            return False
        return self.decl.api

    @property
    def internal(self):
        if not self.decl:
            return False
        return self.decl.internal

    @property
    def private(self):
        if not self.decl:
            return False
        return self.decl.private

    @property
    def public(self):
        if not self.decl:
            return False
        return self.decl.public

    @property
    def inmodule(self):
        return self.relfile.startswith('Modules' + os.path.sep)

    def render_rowvalues(self, kinds):
        row = {
            'name': self.name,
            **{k: '' for k in kinds},
            'filename': f'{self.relfile}:{self.lno}',
        }
        if self.static:
            kind = 'static'
        else:
            if self.internal:
                kind = 'internal'
            elif self.private:
                kind = 'private'
            elif self.public:
                kind = 'public'
            else:
                kind = 'global'
        row['kind'] = kind
        row[kind] = kind
        return row


def _ensure_decl(decl, decls):
    prev = decls.get(decl.name)
    if prev:
        if decl.kind == 'forward':
            return None
        if prev.kind != 'forward':
            if decl.kind == prev.kind and decl.file == prev.file:
                assert decl.lno != prev.lno, (decl, prev)
                return None
            raise NotImplementedError(f'duplicate {decl} (was {prev}')
    decls[decl.name] = decl


def iter_builtin_types(filenames=None):
    decls = {}
    seen = set()
    for filename in iter_header_files():
        seen.add(filename)
        with open(filename) as infile:
            for lno, line in enumerate(infile, 1):
                decl = BuiltinTypeDecl.from_line(line, filename, lno)
                if not decl:
                    continue
                _ensure_decl(decl, decls)
    srcfiles = []
    for filename in iter_filenames():
        if filename.endswith('.c'):
            srcfiles.append(filename)
            continue
        if filename in seen:
            continue
        with open(filename) as infile:
            for lno, line in enumerate(infile, 1):
                decl = BuiltinTypeDecl.from_line(line, filename, lno)
                if not decl:
                    continue
                _ensure_decl(decl, decls)

    for filename in srcfiles:
        with open(filename) as infile:
            localdecls = {}
            for lno, line in enumerate(infile, 1):
                parsed = _parse_line(line)
                if not parsed:
                    continue
                name, isdecl, kind = parsed
                if isdecl:
                    decl = BuiltinTypeDecl.from_parsed(name, kind, filename, lno)
                    if not decl:
                        raise NotImplementedError((filename, line))
                    _ensure_decl(decl, localdecls)
                else:
                    builtin = BuiltinTypeInfo.from_parsed(
                            name, kind, filename, lno,
                            decls=decls if name in decls else localdecls)
                    if not builtin:
                        raise NotImplementedError((filename, line))
                    yield builtin


def resolve_matcher(showmodules=False):
    def match(info, *, log=None):
        if not info.inmodule:
            return True
        if log is not None:
            log(f'ignored {info.name!r}')
        return False
    return match


##################################
# CLI rendering

def resolve_format(fmt):
    if not fmt:
        return 'table'
    elif isinstance(fmt, str) and fmt in _FORMATS:
        return fmt
    else:
        raise NotImplementedError(fmt)


def get_renderer(fmt):
    fmt = resolve_format(fmt)
    if isinstance(fmt, str):
        try:
            return _FORMATS[fmt]
        except KeyError:
            raise ValueError(f'unsupported format {fmt!r}')
    else:
        raise NotImplementedError(fmt)


def render_table(types):
    types = sorted(types, key=(lambda t: t.name))
    colspecs = tables.resolve_columns(
            'name:<33 static:^ global:^ internal:^ private:^ public:^ filename:<30')
    header, div, rowfmt = tables.build_table(colspecs)
    leader = ' ' * sum(c.width+2 for c in colspecs[:3]) + '   '
    yield leader + f'{"API":^29}'
    yield leader + '-' * 29
    yield header
    yield div
    kinds = [c[0] for c in colspecs[1:-1]]
    counts = {k: 0 for k in kinds}
    base = {k: '' for k in kinds}
    for t in types:
        row = t.render_rowvalues(kinds)
        kind = row['kind']
        yield rowfmt.format(**row)
        counts[kind] += 1
    yield ''
    yield f'total: {sum(counts.values()):>3}'
    for kind in kinds:
        yield f'  {kind:>10}: {counts[kind]:>3}'


def render_repr(types):
    for t in types:
        yield repr(t)


_FORMATS = {
    'table': render_table,
    'repr': render_repr,
}


================================================
File: /Tools/c-analyzer/cpython/_capi.py
================================================
from collections import namedtuple
import logging
import os
import os.path
import re
import textwrap

from c_common.tables import build_table, resolve_columns
from c_parser.parser._regexes import _ind
from ._files import iter_header_files
from . import REPO_ROOT


logger = logging.getLogger(__name__)


INCLUDE_ROOT = os.path.join(REPO_ROOT, 'Include')
INCLUDE_CPYTHON = os.path.join(INCLUDE_ROOT, 'cpython')
INCLUDE_INTERNAL = os.path.join(INCLUDE_ROOT, 'internal')

_MAYBE_NESTED_PARENS = textwrap.dedent(r'''
    (?:
        (?: [^(]* [(] [^()]* [)] )* [^(]*
    )
''')

CAPI_FUNC = textwrap.dedent(rf'''
    (?:
        ^
        \s*
        PyAPI_FUNC \s*
        [(]
        {_ind(_MAYBE_NESTED_PARENS, 2)}
        [)] \s*
        (\w+)  # <func>
        \s* [(]
    )
''')
CAPI_DATA = textwrap.dedent(rf'''
    (?:
        ^
        \s*
        PyAPI_DATA \s*
        [(]
        {_ind(_MAYBE_NESTED_PARENS, 2)}
        [)] \s*
        (\w+)  # <data>
        \b [^(]
    )
''')
CAPI_INLINE = textwrap.dedent(r'''
    (?:
        ^
        \s*
        static \s+ inline \s+
        .*?
        \s+
        ( \w+ )  # <inline>
        \s* [(]
    )
''')
CAPI_MACRO = textwrap.dedent(r'''
    (?:
        (\w+)  # <macro>
        [(]
    )
''')
CAPI_CONSTANT = textwrap.dedent(r'''
    (?:
        (\w+)  # <constant>
        \s+ [^(]
    )
''')
CAPI_DEFINE = textwrap.dedent(rf'''
    (?:
        ^
        \s* [#] \s* define \s+
        (?:
            {_ind(CAPI_MACRO, 3)}
            |
            {_ind(CAPI_CONSTANT, 3)}
            |
            (?:
                # ignored
                \w+   # <defined_name>
                \s*
                $
            )
        )
    )
''')
CAPI_RE = re.compile(textwrap.dedent(rf'''
    (?:
        {_ind(CAPI_FUNC, 2)}
        |
        {_ind(CAPI_DATA, 2)}
        |
        {_ind(CAPI_INLINE, 2)}
        |
        {_ind(CAPI_DEFINE, 2)}
    )
'''), re.VERBOSE)

KINDS = [
    'func',
    'data',
    'inline',
    'macro',
    'constant',
]


def _parse_line(line, prev=None):
    last = line
    if prev:
        if not prev.endswith(os.linesep):
            prev += os.linesep
        line = prev + line
    m = CAPI_RE.match(line)
    if not m:
        if not prev and line.startswith('static inline '):
            return line  # the new "prev"
        #if 'PyAPI_' in line or '#define ' in line or ' define ' in line:
        #    print(line)
        return None
    results = zip(KINDS, m.groups())
    for kind, name in results:
        if name:
            clean = last.split('//')[0].rstrip()
            if clean.endswith('*/'):
                clean = clean.split('/*')[0].rstrip()

            if kind == 'macro' or kind == 'constant':
                if not clean.endswith('\\'):
                    return name, kind
            elif kind == 'inline':
                if clean.endswith('}'):
                    if not prev or clean == '}':
                        return name, kind
            elif kind == 'func' or kind == 'data':
                if clean.endswith(';'):
                    return name, kind
            else:
                # This should not be reached.
                raise NotImplementedError
            return line  # the new "prev"
    # It was a plain #define.
    return None


LEVELS = [
    'stable',
    'cpython',
    'private',
    'internal',
]

def _get_level(filename, name, *,
               _cpython=INCLUDE_CPYTHON + os.path.sep,
               _internal=INCLUDE_INTERNAL + os.path.sep,
               ):
    if filename.startswith(_internal):
        return 'internal'
    elif name.startswith('_'):
        return 'private'
    elif os.path.dirname(filename) == INCLUDE_ROOT:
        return 'stable'
    elif filename.startswith(_cpython):
        return 'cpython'
    else:
        raise NotImplementedError
    #return '???'


GROUPINGS = {
    'kind': KINDS,
    'level': LEVELS,
}


class CAPIItem(namedtuple('CAPIItem', 'file lno name kind level')):

    @classmethod
    def from_line(cls, line, filename, lno, prev=None):
        parsed = _parse_line(line, prev)
        if not parsed:
            return None, None
        if isinstance(parsed, str):
            # incomplete
            return None, parsed
        name, kind = parsed
        level = _get_level(filename, name)
        self = cls(filename, lno, name, kind, level)
        if prev:
            self._text = (prev + line).rstrip().splitlines()
        else:
            self._text = [line.rstrip()]
        return self, None

    @property
    def relfile(self):
        return self.file[len(REPO_ROOT) + 1:]

    @property
    def text(self):
        try:
            return self._text
        except AttributeError:
            # XXX Actually ready the text from disk?.
            self._text = []
            if self.kind == 'data':
                self._text = [
                    f'PyAPI_DATA(...) {self.name}',
                ]
            elif self.kind == 'func':
                self._text = [
                    f'PyAPI_FUNC(...) {self.name}(...);',
                ]
            elif self.kind == 'inline':
                self._text = [
                    f'static inline {self.name}(...);',
                ]
            elif self.kind == 'macro':
                self._text = [
                    f'#define {self.name}(...) \\',
                    f'    ...',
                ]
            elif self.kind == 'constant':
                self._text = [
                    f'#define {self.name} ...',
                ]
            else:
                raise NotImplementedError

            return self._text


def _parse_groupby(raw):
    if not raw:
        raw = 'kind'

    if isinstance(raw, str):
        groupby = raw.replace(',', ' ').strip().split()
    else:
        raise NotImplementedError

    if not all(v in GROUPINGS for v in groupby):
        raise ValueError(f'invalid groupby value {raw!r}')
    return groupby


def _resolve_full_groupby(groupby):
    if isinstance(groupby, str):
        groupby = [groupby]
    groupings = []
    for grouping in groupby + list(GROUPINGS):
        if grouping not in groupings:
            groupings.append(grouping)
    return groupings


def summarize(items, *, groupby='kind', includeempty=True, minimize=None):
    if minimize is None:
        if includeempty is None:
            minimize = True
            includeempty = False
        else:
            minimize = includeempty
    elif includeempty is None:
        includeempty = minimize
    elif minimize and includeempty:
        raise ValueError(f'cannot minimize and includeempty at the same time')

    groupby = _parse_groupby(groupby)[0]
    _outer, _inner = _resolve_full_groupby(groupby)
    outers = GROUPINGS[_outer]
    inners = GROUPINGS[_inner]

    summary = {
        'totals': {
            'all': 0,
            'subs': {o: 0 for o in outers},
            'bygroup': {o: {i: 0 for i in inners}
                        for o in outers},
        },
    }

    for item in items:
        outer = getattr(item, _outer)
        inner = getattr(item, _inner)
        # Update totals.
        summary['totals']['all'] += 1
        summary['totals']['subs'][outer] += 1
        summary['totals']['bygroup'][outer][inner] += 1

    if not includeempty:
        subtotals = summary['totals']['subs']
        bygroup = summary['totals']['bygroup']
        for outer in outers:
            if subtotals[outer] == 0:
                del subtotals[outer]
                del bygroup[outer]
                continue

            for inner in inners:
                if bygroup[outer][inner] == 0:
                    del bygroup[outer][inner]
            if minimize:
                if len(bygroup[outer]) == 1:
                    del bygroup[outer]

    return summary


def _parse_capi(lines, filename):
    if isinstance(lines, str):
        lines = lines.splitlines()
    prev = None
    for lno, line in enumerate(lines, 1):
        parsed, prev = CAPIItem.from_line(line, filename, lno, prev)
        if parsed:
            yield parsed
    if prev:
        parsed, prev = CAPIItem.from_line('', filename, lno, prev)
        if parsed:
            yield parsed
        if prev:
            print('incomplete match:')
            print(filename)
            print(prev)
            raise Exception


def iter_capi(filenames=None):
    for filename in iter_header_files(filenames):
        with open(filename) as infile:
            for item in _parse_capi(infile, filename):
                yield item


def resolve_filter(ignored):
    if not ignored:
        return None
    ignored = set(_resolve_ignored(ignored))
    def filter(item, *, log=None):
        if item.name not in ignored:
            return True
        if log is not None:
            log(f'ignored {item.name!r}')
        return False
    return filter


def _resolve_ignored(ignored):
    if isinstance(ignored, str):
        ignored = [ignored]
    for raw in ignored:
        if isinstance(raw, str):
            if raw.startswith('|'):
                yield raw[1:]
            elif raw.startswith('<') and raw.endswith('>'):
                filename = raw[1:-1]
                try:
                    infile = open(filename)
                except Exception as exc:
                    logger.error(f'ignore file failed: {exc}')
                    continue
                logger.log(1, f'reading ignored names from {filename!r}')
                with infile:
                    for line in infile:
                        if not line:
                            continue
                        if line[0].isspace():
                            continue
                        line = line.partition('#')[0].rstrip()
                        if line:
                            # XXX Recurse?
                            yield line
            else:
                raw = raw.strip()
                if raw:
                    yield raw
        else:
            raise NotImplementedError


def _collate(items, groupby, includeempty):
    groupby = _parse_groupby(groupby)[0]
    maxfilename = maxname = maxkind = maxlevel = 0

    collated = {}
    groups = GROUPINGS[groupby]
    for group in groups:
        collated[group] = []

    for item in items:
        key = getattr(item, groupby)
        collated[key].append(item)
        maxfilename = max(len(item.relfile), maxfilename)
        maxname = max(len(item.name), maxname)
        maxkind = max(len(item.kind), maxkind)
        maxlevel = max(len(item.level), maxlevel)
    if not includeempty:
        for group in groups:
            if not collated[group]:
                del collated[group]
    maxextra = {
        'kind': maxkind,
        'level': maxlevel,
    }
    return collated, groupby, maxfilename, maxname, maxextra


def _get_sortkey(sort, _groupby, _columns):
    if sort is True or sort is None:
        # For now:
        def sortkey(item):
            return (
                item.level == 'private',
                LEVELS.index(item.level),
                KINDS.index(item.kind),
                os.path.dirname(item.file),
                os.path.basename(item.file),
                item.name,
            )
        return sortkey

        sortfields = 'not-private level kind dirname basename name'.split()
    elif isinstance(sort, str):
        sortfields = sort.replace(',', ' ').strip().split()
    elif callable(sort):
        return sort
    else:
        raise NotImplementedError

    # XXX Build a sortkey func from sortfields.
    raise NotImplementedError


##################################
# CLI rendering

_MARKERS = {
    'level': {
        'S': 'stable',
        'C': 'cpython',
        'P': 'private',
        'I': 'internal',
    },
    'kind': {
        'F': 'func',
        'D': 'data',
        'I': 'inline',
        'M': 'macro',
        'C': 'constant',
    },
}


def resolve_format(format):
    if not format:
        return 'table'
    elif isinstance(format, str) and format in _FORMATS:
        return format
    else:
        return resolve_columns(format)


def get_renderer(format):
    format = resolve_format(format)
    if isinstance(format, str):
        try:
            return _FORMATS[format]
        except KeyError:
            raise ValueError(f'unsupported format {format!r}')
    else:
        def render(items, **kwargs):
            return render_table(items, columns=format, **kwargs)
        return render


def render_table(items, *,
                 columns=None,
                 groupby='kind',
                 sort=True,
                 showempty=False,
                 verbose=False,
                 ):
    if groupby is None:
        groupby = 'kind'
    if showempty is None:
        showempty = False

    if groupby:
        (collated, groupby, maxfilename, maxname, maxextra,
         ) = _collate(items, groupby, showempty)
        for grouping in GROUPINGS:
            maxextra[grouping] = max(len(g) for g in GROUPINGS[grouping])

        _, extra = _resolve_full_groupby(groupby)
        extras = [extra]
        markers = {extra: _MARKERS[extra]}

        groups = GROUPINGS[groupby]
    else:
        # XXX Support no grouping?
        raise NotImplementedError

    if columns:
        def get_extra(item):
            return {extra: getattr(item, extra)
                    for extra in ('kind', 'level')}
    else:
        if verbose:
            extracols = [f'{extra}:{maxextra[extra]}'
                         for extra in extras]
            def get_extra(item):
                return {extra: getattr(item, extra)
                        for extra in extras}
        elif len(extras) == 1:
            extra, = extras
            extracols = [f'{m}:1' for m in markers[extra]]
            def get_extra(item):
                return {m: m if getattr(item, extra) == markers[extra][m] else ''
                        for m in markers[extra]}
        else:
            raise NotImplementedError
            #extracols = [[f'{m}:1' for m in markers[extra]]
            #             for extra in extras]
            #def get_extra(item):
            #    values = {}
            #    for extra in extras:
            #        cur = markers[extra]
            #        for m in cur:
            #            values[m] = m if getattr(item, m) == cur[m] else ''
            #    return values
        columns = [
            f'filename:{maxfilename}',
            f'name:{maxname}',
            *extracols,
        ]
    header, div, fmt = build_table(columns)

    if sort:
        sortkey = _get_sortkey(sort, groupby, columns)

    total = 0
    for group, grouped in collated.items():
        if not showempty and group not in collated:
            continue
        yield ''
        yield f' === {group} ==='
        yield ''
        yield header
        yield div
        if grouped:
            if sort:
                grouped = sorted(grouped, key=sortkey)
            for item in grouped:
                yield fmt.format(
                    filename=item.relfile,
                    name=item.name,
                    **get_extra(item),
                )
        yield div
        subtotal = len(grouped)
        yield f'  sub-total: {subtotal}'
        total += subtotal
    yield ''
    yield f'total: {total}'


def render_full(items, *,
                groupby='kind',
                sort=None,
                showempty=None,
                verbose=False,
                ):
    if groupby is None:
        groupby = 'kind'
    if showempty is None:
        showempty = False

    if sort:
        sortkey = _get_sortkey(sort, groupby, None)

    if groupby:
        collated, groupby, _, _, _ = _collate(items, groupby, showempty)
        for group, grouped in collated.items():
            yield '#' * 25
            yield f'# {group} ({len(grouped)})'
            yield '#' * 25
            yield ''
            if not grouped:
                continue
            if sort:
                grouped = sorted(grouped, key=sortkey)
            for item in grouped:
                yield from _render_item_full(item, groupby, verbose)
                yield ''
    else:
        if sort:
            items = sorted(items, key=sortkey)
        for item in items:
            yield from _render_item_full(item, None, verbose)
            yield ''


def _render_item_full(item, groupby, verbose):
    yield item.name
    yield f'  {"filename:":10} {item.relfile}'
    for extra in ('kind', 'level'):
        yield f'  {extra+":":10} {getattr(item, extra)}'
    if verbose:
        print('  ---------------------------------------')
        for lno, line in enumerate(item.text, item.lno):
            print(f'  | {lno:3} {line}')
        print('  ---------------------------------------')


def render_summary(items, *,
                   groupby='kind',
                   sort=None,
                   showempty=None,
                   verbose=False,
                   ):
    if groupby is None:
        groupby = 'kind'
    summary = summarize(
        items,
        groupby=groupby,
        includeempty=showempty,
        minimize=None if showempty else not verbose,
    )

    subtotals = summary['totals']['subs']
    bygroup = summary['totals']['bygroup']
    for outer, subtotal in subtotals.items():
        if bygroup:
            subtotal = f'({subtotal})'
            yield f'{outer + ":":20} {subtotal:>8}'
        else:
            yield f'{outer + ":":10} {subtotal:>8}'
        if outer in bygroup:
            for inner, count in bygroup[outer].items():
                yield f'   {inner + ":":9} {count}'
    total = f'*{summary["totals"]["all"]}*'
    label = '*total*:'
    if bygroup:
        yield f'{label:20} {total:>8}'
    else:
        yield f'{label:10} {total:>9}'


_FORMATS = {
    'table': render_table,
    'full': render_full,
    'summary': render_summary,
}


================================================
File: /Tools/c-analyzer/cpython/_files.py
================================================
import os.path

from c_common.fsutil import expand_filenames, iter_files_by_suffix
from . import REPO_ROOT, INCLUDE_DIRS, SOURCE_DIRS


GLOBS = [
    'Include/*.h',
    # Technically, this is covered by "Include/*.h":
    #'Include/cpython/*.h',
    'Include/internal/*.h',
    'Include/internal/mimalloc/**/*.h',
    'Modules/**/*.h',
    'Modules/**/*.c',
    'Objects/**/*.h',
    'Objects/**/*.c',
    'Parser/**/*.h',
    'Parser/**/*.c',
    'Python/**/*.h',
    'Python/**/*.c',
]
LEVEL_GLOBS = {
    'stable': 'Include/*.h',
    'cpython': 'Include/cpython/*.h',
    'internal': 'Include/internal/*.h',
}


def resolve_filename(filename):
    orig = filename
    filename = os.path.normcase(os.path.normpath(filename))
    if os.path.isabs(filename):
        if os.path.relpath(filename, REPO_ROOT).startswith('.'):
            raise Exception(f'{orig!r} is outside the repo ({REPO_ROOT})')
        return filename
    else:
        return os.path.join(REPO_ROOT, filename)


def iter_filenames(*, search=False):
    if search:
        yield from iter_files_by_suffix(INCLUDE_DIRS, ('.h',))
        yield from iter_files_by_suffix(SOURCE_DIRS, ('.c',))
    else:
        globs = (os.path.join(REPO_ROOT, file) for file in GLOBS)
        yield from expand_filenames(globs)


def iter_header_files(filenames=None, *, levels=None):
    if not filenames:
        if levels:
            levels = set(levels)
            if 'private' in levels:
                levels.add('stable')
                levels.add('cpython')
            for level, glob in LEVEL_GLOBS.items():
                if level in levels:
                    yield from expand_filenames([glob])
        else:
            yield from iter_files_by_suffix(INCLUDE_DIRS, ('.h',))
        return

    for filename in filenames:
        orig = filename
        filename = resolve_filename(filename)
        if filename.endswith(os.path.sep):
            yield from iter_files_by_suffix(INCLUDE_DIRS, ('.h',))
        elif filename.endswith('.h'):
            yield filename
        else:
            # XXX Log it and continue instead?
            raise ValueError(f'expected .h file, got {orig!r}')


================================================
File: /Tools/c-analyzer/cpython/_parser.py
================================================
import os.path
import re

from c_parser.preprocessor import (
    get_preprocessor as _get_preprocessor,
)
from c_parser import (
    parse_file as _parse_file,
    parse_files as _parse_files,
)
from . import REPO_ROOT


GLOB_ALL = '**/*'


def _abs(relfile):
    return os.path.join(REPO_ROOT, relfile)


def clean_lines(text):
    """Clear out comments, blank lines, and leading/trailing whitespace."""
    lines = (line.strip() for line in text.splitlines())
    lines = (line.partition('#')[0].rstrip()
             for line in lines
             if line and not line.startswith('#'))
    glob_all = f'{GLOB_ALL} '
    lines = (re.sub(r'^[*] ', glob_all, line) for line in lines)
    lines = (_abs(line) for line in lines)
    return list(lines)


'''
@begin=sh@
./python ../c-parser/cpython.py
    --exclude '+../c-parser/EXCLUDED'
    --macros '+../c-parser/MACROS'
    --incldirs '+../c-parser/INCL_DIRS'
    --same './Include/cpython/'
    Include/*.h
    Include/internal/*.h
    Modules/**/*.c
    Objects/**/*.c
    Parser/**/*.c
    Python/**/*.c
@end=sh@
'''

# XXX Handle these.
# Tab separated:
EXCLUDED = clean_lines('''
# @begin=conf@

# OSX
Modules/_scproxy.c                # SystemConfiguration/SystemConfiguration.h

# Windows
Modules/_winapi.c               # windows.h
Modules/expat/winconfig.h
Modules/overlapped.c            # winsock.h
Python/dynload_win.c            # windows.h
Python/thread_nt.h

# other OS-dependent
Python/dynload_aix.c            # sys/ldr.h
Python/dynload_dl.c             # dl.h
Python/dynload_hpux.c           # dl.h
Python/emscripten_signal.c
Python/thread_pthread.h
Python/thread_pthread_stubs.h

# only huge constants (safe but parsing is slow)
Modules/_ssl_data_*.h
Modules/cjkcodecs/mappings_*.h
Modules/unicodedata_db.h
Modules/unicodename_db.h
Objects/unicodetype_db.h

# generated
Python/deepfreeze/*.c
Python/frozen_modules/*.h
Python/generated_cases.c.h
Python/executor_cases.c.h
Python/optimizer_cases.c.h

# not actually source
Python/bytecodes.c
Python/optimizer_bytecodes.c

# mimalloc
Objects/mimalloc/*.c
Include/internal/mimalloc/*.h
Include/internal/mimalloc/mimalloc/*.h

# @end=conf@
''')

# XXX Fix the parser.
EXCLUDED += clean_lines('''
# The tool should be able to parse these...

# The problem with xmlparse.c is that something
# has gone wrong where # we handle "maybe inline actual"
# in Tools/c-analyzer/c_parser/parser/_global.py.
Modules/expat/internal.h
Modules/expat/xmlparse.c
''')

INCL_DIRS = clean_lines('''
# @begin=tsv@

glob	dirname
*	.
*	./Include
*	./Include/internal
*   ./Include/internal/mimalloc

Modules/_decimal/**/*.c	Modules/_decimal/libmpdec
Modules/_elementtree.c	Modules/expat
Modules/_hacl/*.c	Modules/_hacl/include
Modules/_hacl/*.c	Modules/_hacl/
Modules/_hacl/*.h	Modules/_hacl/include
Modules/_hacl/*.h	Modules/_hacl/
Modules/md5module.c	Modules/_hacl/include
Modules/sha1module.c	Modules/_hacl/include
Modules/sha2module.c	Modules/_hacl/include
Modules/sha3module.c	Modules/_hacl/include
Modules/blake2module.c	Modules/_hacl/include
Objects/stringlib/*.h	Objects

# possible system-installed headers, just in case
Modules/_tkinter.c	/usr/include/tcl8.6
Modules/_uuidmodule.c	/usr/include/uuid
Modules/tkappinit.c	/usr/include/tcl

# @end=tsv@
''')[1:]

INCLUDES = clean_lines('''
# @begin=tsv@

glob	include

**/*.h	Python.h
Include/**/*.h	object.h

# for Py_HAVE_CONDVAR
Include/internal/pycore_gil.h	pycore_condvar.h
Python/thread_pthread.h	pycore_condvar.h

# other

Objects/stringlib/join.h	stringlib/stringdefs.h
Objects/stringlib/ctype.h	stringlib/stringdefs.h
Objects/stringlib/transmogrify.h	stringlib/stringdefs.h
#Objects/stringlib/fastsearch.h	stringlib/stringdefs.h
#Objects/stringlib/count.h	stringlib/stringdefs.h
#Objects/stringlib/find.h	stringlib/stringdefs.h
#Objects/stringlib/partition.h	stringlib/stringdefs.h
#Objects/stringlib/split.h	stringlib/stringdefs.h
Objects/stringlib/fastsearch.h	stringlib/ucs1lib.h
Objects/stringlib/count.h	stringlib/ucs1lib.h
Objects/stringlib/find.h	stringlib/ucs1lib.h
Objects/stringlib/partition.h	stringlib/ucs1lib.h
Objects/stringlib/split.h	stringlib/ucs1lib.h
Objects/stringlib/find_max_char.h	Objects/stringlib/ucs1lib.h
Objects/stringlib/count.h	Objects/stringlib/fastsearch.h
Objects/stringlib/find.h	Objects/stringlib/fastsearch.h
Objects/stringlib/partition.h	Objects/stringlib/fastsearch.h
Objects/stringlib/replace.h	Objects/stringlib/fastsearch.h
Objects/stringlib/repr.h	Objects/stringlib/fastsearch.h
Objects/stringlib/split.h	Objects/stringlib/fastsearch.h

# @end=tsv@
''')[1:]

MACROS = clean_lines('''
# @begin=tsv@

glob	name	value

Include/internal/*.h	Py_BUILD_CORE	1
Python/**/*.c	Py_BUILD_CORE	1
Python/**/*.h	Py_BUILD_CORE	1
Parser/**/*.c	Py_BUILD_CORE	1
Parser/**/*.h	Py_BUILD_CORE	1
Objects/**/*.c	Py_BUILD_CORE	1
Objects/**/*.h	Py_BUILD_CORE	1

Modules/_asynciomodule.c	Py_BUILD_CORE	1
Modules/_codecsmodule.c	Py_BUILD_CORE	1
Modules/_collectionsmodule.c	Py_BUILD_CORE	1
Modules/_ctypes/_ctypes.c	Py_BUILD_CORE	1
Modules/_ctypes/cfield.c	Py_BUILD_CORE	1
Modules/_cursesmodule.c	Py_BUILD_CORE	1
Modules/_datetimemodule.c	Py_BUILD_CORE	1
Modules/_functoolsmodule.c	Py_BUILD_CORE	1
Modules/_heapqmodule.c	Py_BUILD_CORE	1
Modules/_io/*.c	Py_BUILD_CORE	1
Modules/_io/*.h	Py_BUILD_CORE	1
Modules/_localemodule.c	Py_BUILD_CORE	1
Modules/_operator.c	Py_BUILD_CORE	1
Modules/_posixsubprocess.c	Py_BUILD_CORE	1
Modules/_sre/sre.c	Py_BUILD_CORE	1
Modules/_threadmodule.c	Py_BUILD_CORE	1
Modules/_tracemalloc.c	Py_BUILD_CORE	1
Modules/_weakref.c	Py_BUILD_CORE	1
Modules/_zoneinfo.c	Py_BUILD_CORE	1
Modules/atexitmodule.c	Py_BUILD_CORE	1
Modules/cmathmodule.c	Py_BUILD_CORE	1
Modules/faulthandler.c	Py_BUILD_CORE	1
Modules/gcmodule.c	Py_BUILD_CORE	1
Modules/getpath.c	Py_BUILD_CORE	1
Modules/getpath_noop.c	Py_BUILD_CORE	1
Modules/itertoolsmodule.c	Py_BUILD_CORE	1
Modules/main.c	Py_BUILD_CORE	1
Modules/mathmodule.c	Py_BUILD_CORE	1
Modules/posixmodule.c	Py_BUILD_CORE	1
Modules/sha256module.c	Py_BUILD_CORE	1
Modules/sha512module.c	Py_BUILD_CORE	1
Modules/signalmodule.c	Py_BUILD_CORE	1
Modules/symtablemodule.c	Py_BUILD_CORE	1
Modules/timemodule.c	Py_BUILD_CORE	1
Modules/unicodedata.c	Py_BUILD_CORE	1

Modules/_json.c	Py_BUILD_CORE_BUILTIN	1
Modules/_pickle.c	Py_BUILD_CORE_BUILTIN	1
Modules/_testinternalcapi.c	Py_BUILD_CORE_BUILTIN	1

Include/cpython/abstract.h	Py_CPYTHON_ABSTRACTOBJECT_H	1
Include/cpython/bytearrayobject.h	Py_CPYTHON_BYTEARRAYOBJECT_H	1
Include/cpython/bytesobject.h	Py_CPYTHON_BYTESOBJECT_H	1
Include/cpython/ceval.h	Py_CPYTHON_CEVAL_H	1
Include/cpython/code.h	Py_CPYTHON_CODE_H	1
Include/cpython/dictobject.h	Py_CPYTHON_DICTOBJECT_H	1
Include/cpython/fileobject.h	Py_CPYTHON_FILEOBJECT_H	1
Include/cpython/fileutils.h	Py_CPYTHON_FILEUTILS_H	1
Include/cpython/frameobject.h	Py_CPYTHON_FRAMEOBJECT_H	1
Include/cpython/import.h	Py_CPYTHON_IMPORT_H	1
Include/cpython/interpreteridobject.h	Py_CPYTHON_INTERPRETERIDOBJECT_H	1
Include/cpython/listobject.h	Py_CPYTHON_LISTOBJECT_H	1
Include/cpython/methodobject.h	Py_CPYTHON_METHODOBJECT_H	1
Include/cpython/object.h	Py_CPYTHON_OBJECT_H	1
Include/cpython/objimpl.h	Py_CPYTHON_OBJIMPL_H	1
Include/cpython/pyerrors.h	Py_CPYTHON_ERRORS_H	1
Include/cpython/pylifecycle.h	Py_CPYTHON_PYLIFECYCLE_H	1
Include/cpython/pymem.h	Py_CPYTHON_PYMEM_H	1
Include/cpython/pystate.h	Py_CPYTHON_PYSTATE_H	1
Include/cpython/sysmodule.h	Py_CPYTHON_SYSMODULE_H	1
Include/cpython/traceback.h	Py_CPYTHON_TRACEBACK_H	1
Include/cpython/tupleobject.h	Py_CPYTHON_TUPLEOBJECT_H	1
Include/cpython/unicodeobject.h	Py_CPYTHON_UNICODEOBJECT_H	1

# implied include of <unistd.h>
Include/**/*.h	_POSIX_THREADS	1
Include/**/*.h	HAVE_PTHREAD_H	1

# from pyconfig.h
Include/cpython/pthread_stubs.h	HAVE_PTHREAD_STUBS	1
Python/thread_pthread_stubs.h	HAVE_PTHREAD_STUBS	1

# from Objects/bytesobject.c
Objects/stringlib/partition.h	STRINGLIB_GET_EMPTY()	bytes_get_empty()
Objects/stringlib/join.h	STRINGLIB_MUTABLE	0
Objects/stringlib/partition.h	STRINGLIB_MUTABLE	0
Objects/stringlib/split.h	STRINGLIB_MUTABLE	0
Objects/stringlib/transmogrify.h	STRINGLIB_MUTABLE	0

# from Makefile
Modules/getpath.c	PYTHONPATH	1
Modules/getpath.c	PREFIX	...
Modules/getpath.c	EXEC_PREFIX	...
Modules/getpath.c	VERSION	...
Modules/getpath.c	VPATH	...
Modules/getpath.c	PLATLIBDIR	...
#Modules/_dbmmodule.c	USE_GDBM_COMPAT	1
Modules/_dbmmodule.c	USE_NDBM	1
#Modules/_dbmmodule.c	USE_BERKDB	1

# See: setup.py
Modules/_decimal/**/*.c	CONFIG_64	1
Modules/_decimal/**/*.c	ASM	1
Modules/expat/xmlparse.c	HAVE_EXPAT_CONFIG_H	1
Modules/expat/xmlparse.c	XML_POOR_ENTROPY	1
Modules/_dbmmodule.c	HAVE_GDBM_DASH_NDBM_H	1

# others
Modules/_sre/sre_lib.h	LOCAL(type)	static inline type
Modules/_sre/sre_lib.h	SRE(F)	sre_ucs2_##F
Objects/stringlib/codecs.h	STRINGLIB_IS_UNICODE	1
Include/internal/pycore_crossinterp_data_registry.h	Py_CORE_CROSSINTERP_DATA_REGISTRY_H	1

# @end=tsv@
''')[1:]

# -pthread
# -Wno-unused-result
# -Wsign-compare
# -g
# -Og
# -Wall
# -std=c99
# -Wextra
# -Wno-unused-result -Wno-unused-parameter
# -Wno-missing-field-initializers
# -Werror=implicit-function-declaration

SAME = {
    _abs('Include/*.h'): [_abs('Include/cpython/')],
    _abs('Python/ceval.c'): ['Python/generated_cases.c.h'],
}

MAX_SIZES = {
    # GLOB: (MAXTEXT, MAXLINES),
    # default: (10_000, 200)
    # First match wins.
    _abs('Modules/_ctypes/ctypes.h'): (5_000, 500),
    _abs('Modules/_datetimemodule.c'): (20_000, 300),
    _abs('Modules/_hacl/*.c'): (200_000, 500),
    _abs('Modules/posixmodule.c'): (20_000, 500),
    _abs('Modules/termios.c'): (10_000, 800),
    _abs('Modules/_testcapimodule.c'): (20_000, 400),
    _abs('Modules/expat/expat.h'): (10_000, 400),
    _abs('Objects/stringlib/unicode_format.h'): (10_000, 400),
    _abs('Objects/typeobject.c'): (35_000, 200),
    _abs('Python/compile.c'): (20_000, 500),
    _abs('Python/optimizer.c'): (100_000, 5_000),
    _abs('Python/parking_lot.c'): (40_000, 1000),
    _abs('Python/pylifecycle.c'): (500_000, 5000),
    _abs('Python/pystate.c'): (500_000, 5000),
    _abs('Python/initconfig.c'): (50_000, 500),

    # Generated files:
    _abs('Include/internal/pycore_opcode.h'): (10_000, 1000),
    _abs('Include/internal/pycore_global_strings.h'): (5_000, 1000),
    _abs('Include/internal/pycore_runtime_init_generated.h'): (5_000, 1000),
    _abs('Python/deepfreeze/*.c'): (20_000, 500),
    _abs('Python/frozen_modules/*.h'): (20_000, 500),
    _abs('Python/opcode_targets.h'): (10_000, 500),
    _abs('Python/stdlib_module_names.h'): (5_000, 500),

    # These large files are currently ignored (see above).
    _abs('Modules/_ssl_data_31.h'): (80_000, 10_000),
    _abs('Modules/_ssl_data_300.h'): (80_000, 10_000),
    _abs('Modules/_ssl_data_111.h'): (80_000, 10_000),
    _abs('Modules/cjkcodecs/mappings_*.h'): (160_000, 2_000),
    _abs('Modules/unicodedata_db.h'): (180_000, 3_000),
    _abs('Modules/unicodename_db.h'): (1_200_000, 15_000),
    _abs('Objects/unicodetype_db.h'): (240_000, 3_000),

    # Catch-alls:
    _abs('Include/**/*.h'): (5_000, 500),
}


def get_preprocessor(*,
                     file_macros=None,
                     file_includes=None,
                     file_incldirs=None,
                     file_same=None,
                     **kwargs
                     ):
    macros = tuple(MACROS)
    if file_macros:
        macros += tuple(file_macros)
    includes = tuple(INCLUDES)
    if file_includes:
        includes += tuple(file_includes)
    incldirs = tuple(INCL_DIRS)
    if file_incldirs:
        incldirs += tuple(file_incldirs)
    samefiles = dict(SAME)
    if file_same:
        samefiles.update(file_same)
    return _get_preprocessor(
        file_macros=macros,
        file_includes=includes,
        file_incldirs=incldirs,
        file_same=samefiles,
        **kwargs
    )


def parse_file(filename, *,
               match_kind=None,
               ignore_exc=None,
               log_err=None,
               ):
    get_file_preprocessor = get_preprocessor(
        ignore_exc=ignore_exc,
        log_err=log_err,
    )
    yield from _parse_file(
        filename,
        match_kind=match_kind,
        get_file_preprocessor=get_file_preprocessor,
        file_maxsizes=MAX_SIZES,
    )


def parse_files(filenames=None, *,
                match_kind=None,
                ignore_exc=None,
                log_err=None,
                get_file_preprocessor=None,
                **file_kwargs
                ):
    if get_file_preprocessor is None:
        get_file_preprocessor = get_preprocessor(
            ignore_exc=ignore_exc,
            log_err=log_err,
        )
    yield from _parse_files(
        filenames,
        match_kind=match_kind,
        get_file_preprocessor=get_file_preprocessor,
        file_maxsizes=MAX_SIZES,
        **file_kwargs
    )


================================================
File: /Tools/c-analyzer/cpython/globals-to-fix.tsv
================================================
filename	funcname	name	reason
#???	-	somevar	???

# These are all variables that we will be making non-global.

##################################
## global objects to fix in core code

##-----------------------
## exported builtin types (C-API)

Objects/boolobject.c	-	PyBool_Type	-
Objects/bytearrayobject.c	-	PyByteArrayIter_Type	-
Objects/bytearrayobject.c	-	PyByteArray_Type	-
Objects/bytesobject.c	-	PyBytesIter_Type	-
Objects/bytesobject.c	-	PyBytes_Type	-
Objects/capsule.c	-	PyCapsule_Type	-
Objects/cellobject.c	-	PyCell_Type	-
Objects/classobject.c	-	PyInstanceMethod_Type	-
Objects/classobject.c	-	PyMethod_Type	-
Objects/codeobject.c	-	PyCode_Type	-
Objects/complexobject.c	-	PyComplex_Type	-
Objects/descrobject.c	-	PyClassMethodDescr_Type	-
Objects/descrobject.c	-	PyDictProxy_Type	-
Objects/descrobject.c	-	PyGetSetDescr_Type	-
Objects/descrobject.c	-	PyMemberDescr_Type	-
Objects/descrobject.c	-	PyMethodDescr_Type	-
Objects/descrobject.c	-	PyProperty_Type	-
Objects/descrobject.c	-	PyWrapperDescr_Type	-
Objects/descrobject.c	-	_PyMethodWrapper_Type	-
Objects/dictobject.c	-	PyDictItems_Type	-
Objects/dictobject.c	-	PyDictIterItem_Type	-
Objects/dictobject.c	-	PyDictIterKey_Type	-
Objects/dictobject.c	-	PyDictIterValue_Type	-
Objects/dictobject.c	-	PyDictKeys_Type	-
Objects/dictobject.c	-	PyDictRevIterItem_Type	-
Objects/dictobject.c	-	PyDictRevIterKey_Type	-
Objects/dictobject.c	-	PyDictRevIterValue_Type	-
Objects/dictobject.c	-	PyDictValues_Type	-
Objects/dictobject.c	-	PyDict_Type	-
Objects/enumobject.c	-	PyEnum_Type	-
Objects/enumobject.c	-	PyReversed_Type	-
Objects/fileobject.c	-	PyStdPrinter_Type	-
Objects/floatobject.c	-	PyFloat_Type	-
Objects/frameobject.c	-	PyFrame_Type	-
Objects/frameobject.c	-	PyFrameLocalsProxy_Type	-
Objects/funcobject.c	-	PyClassMethod_Type	-
Objects/funcobject.c	-	PyFunction_Type	-
Objects/funcobject.c	-	PyStaticMethod_Type	-
Objects/genericaliasobject.c	-	Py_GenericAliasType	-
Objects/genobject.c	-	PyAsyncGen_Type	-
Objects/genobject.c	-	PyCoro_Type	-
Objects/genobject.c	-	PyGen_Type	-
Objects/genobject.c	-	_PyAsyncGenASend_Type	-
Objects/genobject.c	-	_PyAsyncGenAThrow_Type	-
Objects/genobject.c	-	_PyAsyncGenWrappedValue_Type	-
Objects/genobject.c	-	_PyCoroWrapper_Type	-
Objects/iterobject.c	-	PyCallIter_Type	-
Objects/iterobject.c	-	PySeqIter_Type	-
Objects/iterobject.c	-	_PyAnextAwaitable_Type	-
Objects/listobject.c	-	PyListIter_Type	-
Objects/listobject.c	-	PyListRevIter_Type	-
Objects/listobject.c	-	PyList_Type	-
Objects/longobject.c	-	PyLong_Type	-
Objects/memoryobject.c	-	PyMemoryView_Type	-
Objects/memoryobject.c	-	_PyManagedBuffer_Type	-
Objects/methodobject.c	-	PyCFunction_Type	-
Objects/methodobject.c	-	PyCMethod_Type	-
Objects/moduleobject.c	-	PyModuleDef_Type	-
Objects/moduleobject.c	-	PyModule_Type	-
Objects/namespaceobject.c	-	_PyNamespace_Type	-
Objects/object.c	-	_PyNone_Type	-
Objects/object.c	-	_PyNotImplemented_Type	-
Objects/object.c 	-	_PyAnextAwaitable_Type	-
Objects/odictobject.c	-	PyODictItems_Type	-
Objects/odictobject.c	-	PyODictIter_Type	-
Objects/odictobject.c	-	PyODictKeys_Type	-
Objects/odictobject.c	-	PyODictValues_Type	-
Objects/odictobject.c	-	PyODict_Type	-
Objects/picklebufobject.c	-	PyPickleBuffer_Type	-
Objects/rangeobject.c	-	PyLongRangeIter_Type	-
Objects/rangeobject.c	-	PyRangeIter_Type	-
Objects/rangeobject.c	-	PyRange_Type	-
Objects/setobject.c	-	PyFrozenSet_Type	-
Objects/setobject.c	-	PySetIter_Type	-
Objects/setobject.c	-	PySet_Type	-
Objects/sliceobject.c	-	PyEllipsis_Type	-
Objects/sliceobject.c	-	PySlice_Type	-
Objects/tupleobject.c	-	PyTupleIter_Type	-
Objects/tupleobject.c	-	PyTuple_Type	-
Objects/typeobject.c	-	_PyBufferWrapper_Type	-
Objects/typeobject.c	-	PyBaseObject_Type	-
Objects/typeobject.c	-	PySuper_Type	-
Objects/typeobject.c	-	PyType_Type	-
Objects/typevarobject.c	-	_PyTypeAlias_Type	-
Objects/typevarobject.c	-	_PyNoDefault_Type	-
Objects/unicodeobject.c	-	PyUnicodeIter_Type	-
Objects/unicodeobject.c	-	PyUnicode_Type	-
Objects/weakrefobject.c	-	_PyWeakref_CallableProxyType	-
Objects/weakrefobject.c	-	_PyWeakref_ProxyType	-
Objects/weakrefobject.c	-	_PyWeakref_RefType	-
Python/bltinmodule.c	-	PyFilter_Type	-
Python/bltinmodule.c	-	PyMap_Type	-
Python/bltinmodule.c	-	PyZip_Type	-
Python/context.c	-	PyContextToken_Type	-
Python/context.c	-	PyContextVar_Type	-
Python/context.c	-	PyContext_Type	-
Python/instruction_sequence.c	-	_PyInstructionSequence_Type	-
Python/instrumentation.c	-	_PyLegacyBranchEventHandler_Type	-
Python/instrumentation.c	-	_PyBranchesIterator	-
Python/traceback.c	-	PyTraceBack_Type	-

##-----------------------
## other exported builtin types

# Not in a .h file:
Objects/codeobject.c	-	_PyLineIterator	-
# Not in a .h file:
Objects/codeobject.c	-	_PyPositionsIterator	-
Objects/genericaliasobject.c	-	_Py_GenericAliasIterType	-
# Not in a .h file:
Objects/memoryobject.c	-	_PyMemoryIter_Type	-
Objects/unicodeobject.c	-	_PyUnicodeASCIIIter_Type	-
Objects/unionobject.c	-	_PyUnion_Type	-
Python/context.c	-	_PyContextTokenMissing_Type	-
Python/hamt.c	-	_PyHamtItems_Type	-
Python/hamt.c	-	_PyHamtKeys_Type	-
Python/hamt.c	-	_PyHamtValues_Type	-
Python/hamt.c	-	_PyHamt_ArrayNode_Type	-
Python/hamt.c	-	_PyHamt_BitmapNode_Type	-
Python/hamt.c	-	_PyHamt_CollisionNode_Type	-
Python/hamt.c	-	_PyHamt_Type	-
Python/symtable.c	-	PySTEntry_Type	-

##-----------------------
## private static builtin types

Objects/setobject.c	-	_PySetDummy_Type	-
Objects/stringlib/unicode_format.h	-	PyFormatterIter_Type	-
Objects/stringlib/unicode_format.h	-	PyFieldNameIter_Type	-
Objects/unicodeobject.c	-	EncodingMapType	-
#Objects/unicodeobject.c	-	PyFieldNameIter_Type	-
#Objects/unicodeobject.c	-	PyFormatterIter_Type	-
Python/legacy_tracing.c	-	_PyLegacyEventHandler_Type	-
Objects/object.c	-	_PyLegacyEventHandler_Type	-


##-----------------------
## static builtin structseq

Objects/floatobject.c	-	FloatInfoType	-
Objects/longobject.c	-	Int_InfoType	-
Python/errors.c	-	UnraisableHookArgsType	-
Python/sysmodule.c	-	AsyncGenHooksType	-
Python/sysmodule.c	-	FlagsType	-
Python/sysmodule.c	-	Hash_InfoType	-
Python/sysmodule.c	-	VersionInfoType	-
Python/thread.c	-	ThreadInfoType	-

##-----------------------
## builtin exception types

Objects/exceptions.c	-	_PyExc_BaseException	-
Objects/exceptions.c	-	_PyExc_BaseExceptionGroup	-
Objects/exceptions.c	-	_PyExc_UnicodeEncodeError	-
Objects/exceptions.c	-	_PyExc_UnicodeDecodeError	-
Objects/exceptions.c	-	_PyExc_UnicodeTranslateError	-
Objects/exceptions.c	-	_PyExc_MemoryError	-
Objects/exceptions.c	-	_PyExc_Exception	-
Objects/exceptions.c	-	_PyExc_TypeError	-
Objects/exceptions.c	-	_PyExc_StopAsyncIteration	-
Objects/exceptions.c	-	_PyExc_StopIteration	-
Objects/exceptions.c	-	_PyExc_GeneratorExit	-
Objects/exceptions.c	-	_PyExc_SystemExit	-
Objects/exceptions.c	-	_PyExc_KeyboardInterrupt	-
Objects/exceptions.c	-	_PyExc_ImportError	-
Objects/exceptions.c	-	_PyExc_ModuleNotFoundError	-
Objects/exceptions.c	-	_PyExc_OSError	-
Objects/exceptions.c	-	_PyExc_BlockingIOError	-
Objects/exceptions.c	-	_PyExc_ConnectionError	-
Objects/exceptions.c	-	_PyExc_ChildProcessError	-
Objects/exceptions.c	-	_PyExc_BrokenPipeError	-
Objects/exceptions.c	-	_PyExc_ConnectionAbortedError	-
Objects/exceptions.c	-	_PyExc_ConnectionRefusedError	-
Objects/exceptions.c	-	_PyExc_ConnectionResetError	-
Objects/exceptions.c	-	_PyExc_FileExistsError	-
Objects/exceptions.c	-	_PyExc_FileNotFoundError	-
Objects/exceptions.c	-	_PyExc_IsADirectoryError	-
Objects/exceptions.c	-	_PyExc_NotADirectoryError	-
Objects/exceptions.c	-	_PyExc_InterruptedError	-
Objects/exceptions.c	-	_PyExc_PermissionError	-
Objects/exceptions.c	-	_PyExc_ProcessLookupError	-
Objects/exceptions.c	-	_PyExc_TimeoutError	-
Objects/exceptions.c	-	_PyExc_EOFError	-
Objects/exceptions.c	-	_PyExc_RuntimeError	-
Objects/exceptions.c	-	_PyExc_PythonFinalizationError	-
Objects/exceptions.c	-	_PyExc_RecursionError	-
Objects/exceptions.c	-	_PyExc_NotImplementedError	-
Objects/exceptions.c	-	_PyExc_NameError	-
Objects/exceptions.c	-	_PyExc_UnboundLocalError	-
Objects/exceptions.c	-	_PyExc_AttributeError	-
Objects/exceptions.c	-	_PyExc_SyntaxError	-
Objects/exceptions.c	-	_PyExc_IndentationError	-
Objects/exceptions.c	-	_PyExc_TabError	-
Objects/exceptions.c	-	_PyExc_IncompleteInputError	-
Objects/exceptions.c	-	_PyExc_LookupError	-
Objects/exceptions.c	-	_PyExc_IndexError	-
Objects/exceptions.c	-	_PyExc_KeyError	-
Objects/exceptions.c	-	_PyExc_ValueError	-
Objects/exceptions.c	-	_PyExc_UnicodeError	-
Objects/exceptions.c	-	_PyExc_AssertionError	-
Objects/exceptions.c	-	_PyExc_ArithmeticError	-
Objects/exceptions.c	-	_PyExc_FloatingPointError	-
Objects/exceptions.c	-	_PyExc_OverflowError	-
Objects/exceptions.c	-	_PyExc_ZeroDivisionError	-
Objects/exceptions.c	-	_PyExc_SystemError	-
Objects/exceptions.c	-	_PyExc_ReferenceError	-
Objects/exceptions.c	-	_PyExc_BufferError	-
Objects/exceptions.c	-	_PyExc_Warning	-
Objects/exceptions.c	-	_PyExc_UserWarning	-
Objects/exceptions.c	-	_PyExc_DeprecationWarning	-
Objects/exceptions.c	-	_PyExc_PendingDeprecationWarning	-
Objects/exceptions.c	-	_PyExc_SyntaxWarning	-
Objects/exceptions.c	-	_PyExc_RuntimeWarning	-
Objects/exceptions.c	-	_PyExc_FutureWarning	-
Objects/exceptions.c	-	_PyExc_ImportWarning	-
Objects/exceptions.c	-	_PyExc_UnicodeWarning	-
Objects/exceptions.c	-	_PyExc_BytesWarning	-
Objects/exceptions.c	-	_PyExc_ResourceWarning	-
Objects/exceptions.c	-	_PyExc_EncodingWarning	-
Objects/exceptions.c	-	PyExc_EnvironmentError	-
Objects/exceptions.c	-	PyExc_IOError	-
Objects/exceptions.c	-	PyExc_BaseException	-
Objects/exceptions.c	-	PyExc_BaseExceptionGroup	-
Objects/exceptions.c	-	PyExc_Exception	-
Objects/exceptions.c	-	PyExc_TypeError	-
Objects/exceptions.c	-	PyExc_StopAsyncIteration	-
Objects/exceptions.c	-	PyExc_StopIteration	-
Objects/exceptions.c	-	PyExc_GeneratorExit	-
Objects/exceptions.c	-	PyExc_SystemExit	-
Objects/exceptions.c	-	PyExc_KeyboardInterrupt	-
Objects/exceptions.c	-	PyExc_ImportError	-
Objects/exceptions.c	-	PyExc_ModuleNotFoundError	-
Objects/exceptions.c	-	PyExc_OSError	-
Objects/exceptions.c	-	PyExc_BlockingIOError	-
Objects/exceptions.c	-	PyExc_ConnectionError	-
Objects/exceptions.c	-	PyExc_ChildProcessError	-
Objects/exceptions.c	-	PyExc_BrokenPipeError	-
Objects/exceptions.c	-	PyExc_ConnectionAbortedError	-
Objects/exceptions.c	-	PyExc_ConnectionRefusedError	-
Objects/exceptions.c	-	PyExc_ConnectionResetError	-
Objects/exceptions.c	-	PyExc_FileExistsError	-
Objects/exceptions.c	-	PyExc_FileNotFoundError	-
Objects/exceptions.c	-	PyExc_IsADirectoryError	-
Objects/exceptions.c	-	PyExc_NotADirectoryError	-
Objects/exceptions.c	-	PyExc_InterruptedError	-
Objects/exceptions.c	-	PyExc_PermissionError	-
Objects/exceptions.c	-	PyExc_ProcessLookupError	-
Objects/exceptions.c	-	PyExc_TimeoutError	-
Objects/exceptions.c	-	PyExc_EOFError	-
Objects/exceptions.c	-	PyExc_RuntimeError	-
Objects/exceptions.c	-	PyExc_PythonFinalizationError	-
Objects/exceptions.c	-	PyExc_RecursionError	-
Objects/exceptions.c	-	PyExc_NotImplementedError	-
Objects/exceptions.c	-	PyExc_NameError	-
Objects/exceptions.c	-	PyExc_UnboundLocalError	-
Objects/exceptions.c	-	PyExc_AttributeError	-
Objects/exceptions.c	-	PyExc_SyntaxError	-
Objects/exceptions.c	-	PyExc_IndentationError	-
Objects/exceptions.c	-	PyExc_TabError	-
Objects/exceptions.c	-	PyExc_IncompleteInputError	-
Objects/exceptions.c	-	PyExc_LookupError	-
Objects/exceptions.c	-	PyExc_IndexError	-
Objects/exceptions.c	-	PyExc_KeyError	-
Objects/exceptions.c	-	PyExc_ValueError	-
Objects/exceptions.c	-	PyExc_UnicodeError	-
Objects/exceptions.c	-	PyExc_UnicodeEncodeError	-
Objects/exceptions.c	-	PyExc_UnicodeDecodeError	-
Objects/exceptions.c	-	PyExc_UnicodeTranslateError	-
Objects/exceptions.c	-	PyExc_AssertionError	-
Objects/exceptions.c	-	PyExc_ArithmeticError	-
Objects/exceptions.c	-	PyExc_FloatingPointError	-
Objects/exceptions.c	-	PyExc_OverflowError	-
Objects/exceptions.c	-	PyExc_ZeroDivisionError	-
Objects/exceptions.c	-	PyExc_SystemError	-
Objects/exceptions.c	-	PyExc_ReferenceError	-
Objects/exceptions.c	-	PyExc_MemoryError	-
Objects/exceptions.c	-	PyExc_BufferError	-
Objects/exceptions.c	-	PyExc_Warning	-
Objects/exceptions.c	-	PyExc_UserWarning	-
Objects/exceptions.c	-	PyExc_DeprecationWarning	-
Objects/exceptions.c	-	PyExc_PendingDeprecationWarning	-
Objects/exceptions.c	-	PyExc_SyntaxWarning	-
Objects/exceptions.c	-	PyExc_RuntimeWarning	-
Objects/exceptions.c	-	PyExc_FutureWarning	-
Objects/exceptions.c	-	PyExc_ImportWarning	-
Objects/exceptions.c	-	PyExc_UnicodeWarning	-
Objects/exceptions.c	-	PyExc_BytesWarning	-
Objects/exceptions.c	-	PyExc_ResourceWarning	-
Objects/exceptions.c	-	PyExc_EncodingWarning	-
Python/crossinterp_exceptions.h	-	_PyExc_InterpreterError	-
Python/crossinterp_exceptions.h	-	_PyExc_InterpreterNotFoundError	-
Python/crossinterp_exceptions.h	-	PyExc_InterpreterError	-
Python/crossinterp_exceptions.h	-	PyExc_InterpreterNotFoundError	-

##-----------------------
## singletons

Modules/_datetimemodule.c	-	zero_delta	-
Modules/_datetimemodule.c	-	utc_timezone	-
Modules/_datetimemodule.c	-	capi	-
Modules/_datetimemodule.c	-	_globals	-
Objects/boolobject.c	-	_Py_FalseStruct	-
Objects/boolobject.c	-	_Py_TrueStruct	-
Objects/dictobject.c	-	empty_keys_struct	-
Objects/object.c	-	_Py_NoneStruct	-
Objects/object.c	-	_Py_NotImplementedStruct	-
Objects/setobject.c	-	_dummy_struct	-
Objects/setobject.c	-	_PySet_Dummy	-
Objects/sliceobject.c	-	_Py_EllipsisObject	-
Objects/typevarobject.c	-	_Py_NoDefaultStruct	-
Python/instrumentation.c	-	_PyInstrumentation_DISABLE	-
Python/instrumentation.c	-	_PyInstrumentation_MISSING	-


##################################
## global non-objects to fix in core code

# <none>


##################################
## global objects to fix in builtin modules

##-----------------------
## static types

Modules/_testcapi/vectorcall.c	-	MethodDescriptorBase_Type	-
Modules/_testcapi/vectorcall.c	-	MethodDescriptorDerived_Type	-
Modules/_testcapi/vectorcall.c	-	MethodDescriptorNopGet_Type	-
Modules/_testcapi/vectorcall.c	-	MethodDescriptor2_Type	-
Modules/_testclinic.c	-	DeprStarInit	-
Modules/_testclinic.c	-	DeprStarInitNoInline	-
Modules/_testclinic.c	-	DeprStarNew	-
Modules/_testclinic.c	-	DeprKwdInit	-
Modules/_testclinic.c	-	DeprKwdInitNoInline	-
Modules/_testclinic.c	-	DeprKwdNew	-
Modules/_testclinic.c	-	TestClass	-


##################################
## global non-objects to fix in builtin modules

# <none>


##################################
## global objects to fix in extension modules

##-----------------------
## static types

Modules/_datetimemodule.c	-	PyDateTime_DateTimeType	-
Modules/_datetimemodule.c	-	PyDateTime_DateType	-
Modules/_datetimemodule.c	-	PyDateTime_DeltaType	-
Modules/_datetimemodule.c	-	PyDateTime_IsoCalendarDateType	-
Modules/_datetimemodule.c	-	PyDateTime_TZInfoType	-
Modules/_datetimemodule.c	-	PyDateTime_TimeType	-
Modules/_datetimemodule.c	-	PyDateTime_TimeZoneType	-
Modules/xxmodule.c	-	Null_Type	-
Modules/xxmodule.c	-	Str_Type	-
Modules/xxmodule.c	-	Xxo_Type	-
Modules/xxsubtype.c	-	spamdict_type	-
Modules/xxsubtype.c	-	spamlist_type	-
Modules/_testcapi/monitoring.c	-	PyCodeLike_Type	-

##-----------------------
## non-static types - initialized once

## heap types
Modules/_tkinter.c	-	PyTclObject_Type	-
Modules/_tkinter.c	-	Tkapp_Type	-
Modules/_tkinter.c	-	Tktt_Type	-
Modules/xxlimited_35.c	-	Xxo_Type	-

## exception types
Modules/_tkinter.c	-	Tkinter_TclError	-
Modules/xxlimited_35.c	-	ErrorObject	-
Modules/xxmodule.c	-	ErrorObject	-

##-----------------------
## other

## state
Modules/_datetimemodule.c	-	_datetime_global_state	-
Modules/_tkinter.c	-	tcl_lock	-
Modules/_tkinter.c	-	excInCmd	-
Modules/_tkinter.c	-	valInCmd	-
Modules/_tkinter.c	-	trbInCmd	-


##################################
## global non-objects to fix in extension modules

##-----------------------
## initialized once

## other
Include/datetime.h	-	PyDateTimeAPI	-
Modules/_ctypes/cfield.c	_ctypes_get_fielddesc	initialized	-
Modules/_ctypes/malloc_closure.c	-	_pagesize	-
Modules/_cursesmodule.c	-	curses_module_loaded	-
Modules/_cursesmodule.c	-	curses_initscr_called	-
Modules/_cursesmodule.c	-	curses_setupterm_called	-
Modules/_cursesmodule.c	-	curses_start_color_called	-
Modules/_cursesmodule.c	-	curses_screen_encoding	-
Modules/_elementtree.c	-	expat_capi	-
Modules/readline.c	-	libedit_append_replace_history_offset	-
Modules/readline.c	-	using_libedit_emulation	-
Modules/readline.c	-	libedit_history_start	-

##-----------------------
## state

Modules/_ctypes/cfield.c	-	formattable	-
Modules/_ctypes/malloc_closure.c	-	free_list	-
Modules/_curses_panel.c	-	lop	-
Modules/_ssl/debughelpers.c	_PySSL_keylog_callback	lock	-
Modules/_tkinter.c	-	quitMainLoop	-
Modules/_tkinter.c	-	errorInCmd	-
Modules/_tkinter.c	-	Tkinter_busywaitinterval	-
Modules/_tkinter.c	-	call_mutex	-
Modules/_tkinter.c	-	var_mutex	-
Modules/_tkinter.c	-	command_mutex	-
Modules/_tkinter.c	-	HeadFHCD	-
Modules/_tkinter.c	-	stdin_ready	-
Modules/_tkinter.c	-	event_tstate	-
Modules/readline.c	-	completer_word_break_characters	-
Modules/readline.c	-	_history_length	-
Modules/readline.c	-	should_auto_add_history	-
Modules/readline.c	-	sigwinch_received	-
Modules/readline.c	-	sigwinch_ohandler	-
Modules/readline.c	-	completed_input_string	-
Modules/rotatingtree.c	-	random_stream	-
Modules/rotatingtree.c	-	random_value	-
Modules/rotatingtree.c	-	random_mutex	-
Modules/socketmodule.c	-	accept4_works	-
Modules/socketmodule.c	-	sock_cloexec_works	-


================================================
File: /Tools/c-analyzer/cpython/ignored.tsv
================================================
filename	funcname	name	reason
#???	-	somevar	???

# All globals here are technically mutable but known to be safe.


##################################
## process-global values - set once

# These will never re-initialize (but would be idempotent).
# These are effectively const.

##-----------------------
## process-global resources

## indicators for resource availability/capability
# (set during first init)
Python/bootstrap_hash.c	py_getrandom	getrandom_works	-
Python/fileutils.c	-	_Py_open_cloexec_works	-
Python/fileutils.c	set_inheritable	ioctl_works	-
# (set lazily, *after* first init)
# XXX Is this thread-safe?
Modules/posixmodule.c	os_dup2_impl	dup3_works	-

## guards around resource init
Python/thread_pthread.h	PyThread__init_thread	lib_initialized	-

##-----------------------
## other values (not Python-specific)

## cached computed data - set lazily (*after* first init)
# XXX Are these safe relative to write races?
Objects/longobject.c	long_from_non_binary_base	log_base_BASE	-
Objects/longobject.c	long_from_non_binary_base	convwidth_base	-
Objects/longobject.c	long_from_non_binary_base	convmultmax_base	-
Objects/unicodeobject.c	-	bloom_linebreak	-
# This is safe:
Objects/unicodeobject.c	_init_global_state	initialized	-

##-----------------------
## other values (Python-specific)

## internal state - set before/during first init
Modules/getbuildinfo.c	-	buildinfo	-
Modules/getbuildinfo.c	-	initialized	-
Python/getversion.c	-	initialized	-
Python/getversion.c	-	version	-

## public C-API - set during first init
Python/bootstrap_hash.c	-	_Py_HashSecret_Initialized	-
Python/pyhash.c	-	_Py_HashSecret	-

## thread-safe hashtable (internal locks)
Python/parking_lot.c	-	buckets	-


##################################
## state tied to Py_Main()
# (only in main thread)

##-----------------------
## handling C argv

Python/getopt.c	-	_PyOS_optarg	-
Python/getopt.c	-	_PyOS_opterr	-
Python/getopt.c	-	_PyOS_optind	-
Python/getopt.c	-	opt_ptr	-
Python/pathconfig.c	-	_Py_path_config	-

##-----------------------
## REPL

Parser/myreadline.c	-	_PyOS_ReadlineLock	-
Parser/myreadline.c	-	_PyOS_ReadlineTState	-
Parser/myreadline.c	-	PyOS_InputHook	-
Parser/myreadline.c	-	PyOS_ReadlineFunctionPointer	-


##################################
## runtime-global values - set once with each init

# These are effectively const.

##-----------------------
## set by embedders before init
# (whether directly or through a call)

Python/initconfig.c	-	_Py_StandardStreamEncoding	-
Python/initconfig.c	-	_Py_StandardStreamErrors	-

# Internal constant list
Python/initconfig.c	-	PYCONFIG_SPEC	-
Python/initconfig.c	-	PYPRECONFIG_SPEC	-


##-----------------------
## public C-API

## deprecated
Python/preconfig.c	-	Py_FileSystemDefaultEncoding	-
Python/preconfig.c	-	Py_HasFileSystemDefaultEncoding	-
Python/preconfig.c	-	Py_FileSystemDefaultEncodeErrors	-
Python/preconfig.c	-	_Py_HasFileSystemDefaultEncodeErrors	-

## legacy config flags
Python/initconfig.c	-	Py_UTF8Mode	-
Python/initconfig.c	-	Py_DebugFlag	-
Python/initconfig.c	-	Py_VerboseFlag	-
Python/initconfig.c	-	Py_QuietFlag	-
Python/initconfig.c	-	Py_InteractiveFlag	-
Python/initconfig.c	-	Py_InspectFlag	-
Python/initconfig.c	-	Py_OptimizeFlag	-
Python/initconfig.c	-	Py_NoSiteFlag	-
Python/initconfig.c	-	Py_BytesWarningFlag	-
Python/initconfig.c	-	Py_FrozenFlag	-
Python/initconfig.c	-	Py_IgnoreEnvironmentFlag	-
Python/initconfig.c	-	Py_DontWriteBytecodeFlag	-
Python/initconfig.c	-	Py_NoUserSiteDirectory	-
Python/initconfig.c	-	Py_UnbufferedStdioFlag	-
Python/initconfig.c	-	Py_HashRandomizationFlag	-
Python/initconfig.c	-	Py_IsolatedFlag	-
Python/initconfig.c	-	Py_LegacyWindowsFSEncodingFlag	-
Python/initconfig.c	-	Py_LegacyWindowsStdioFlag	-

##-----------------------
## initialized statically, may be customized by embedders

Python/frozen.c	-	PyImport_FrozenModules	-
Python/import.c	-	inittab_copy	-
Python/import.c	-	PyImport_Inittab	-


##################################
## runtime-global state

##-----------------------
## tied to each init/fini cycle

## the consolidated runtime state
Python/pylifecycle.c	-	_PyRuntime	-
Python/pylifecycle.c	-	runtime_initialized	-

# All cases of _PyArg_Parser are handled in c-analyzr/cpython/_analyzer.py.

## main interp state in stdlib modules
Modules/syslogmodule.c	-	S_ident_o	-
Modules/syslogmodule.c	-	S_log_open	-

##-----------------------
## kept for stable ABI compatibility

Objects/object.c	-	_Py_RefTotal	-

##-----------------------
## one-off temporary state

# used during runtime init
Python/sysmodule.c	-	_preinit_warnoptions	-
Python/sysmodule.c	-	_preinit_xoptions	-

# thread-safety
# XXX need race protection?
Modules/faulthandler.c	faulthandler_dump_traceback	reentrant	-
Python/pylifecycle.c	_Py_FatalErrorFormat	reentrant	-
Python/pylifecycle.c	fatal_error	reentrant	-

# explicitly protected, internal-only
Modules/_interpchannelsmodule.c	-	_globals	-
Modules/_interpqueuesmodule.c	-	_globals	-

# set once during module init
Modules/_decimal/_decimal.c	-	minalloc_is_set	-


##################################
## not significant

##-----------------------
## not used (kept for compatibility)

Python/pyfpe.c	-	PyFPE_counter	-

##-----------------------
## thread-local variables

Python/import.c	-	pkgcontext	-
Python/pystate.c	-	_Py_tss_tstate	-

##-----------------------
## should be const
# XXX Make them const.

# These are all variables that we will be leaving global.

# All module defs, type defs, etc. are handled in c-analyzr/cpython/_analyzer.py.
# All kwlist arrays are handled in c-analyzr/cpython/_analyzer.py.

# other vars that are actually constant

Include/internal/pycore_blocks_output_buffer.h	-	BUFFER_BLOCK_SIZE	-
Modules/_csv.c	-	quote_styles	-
Modules/_ctypes/_ctypes.c	-	_ctypesmodule	-
Modules/_ctypes/cfield.c	-	ffi_type_double	-
Modules/_ctypes/cfield.c	-	ffi_type_float	-
Modules/_ctypes/cfield.c	-	ffi_type_longdouble	-
Modules/_ctypes/cfield.c	-	ffi_type_pointer	-
Modules/_ctypes/cfield.c	-	ffi_type_sint16	-
Modules/_ctypes/cfield.c	-	ffi_type_sint32	-
Modules/_ctypes/cfield.c	-	ffi_type_sint64	-
Modules/_ctypes/cfield.c	-	ffi_type_sint8	-
Modules/_ctypes/cfield.c	-	ffi_type_uint16	-
Modules/_ctypes/cfield.c	-	ffi_type_uint32	-
Modules/_ctypes/cfield.c	-	ffi_type_uint64	-
Modules/_ctypes/cfield.c	-	ffi_type_uint8	-
Modules/_ctypes/cfield.c	-	ffi_type_void	-
Modules/_datetimemodule.c	-	epoch	-
Modules/_datetimemodule.c	-	max_fold_seconds	-
Modules/_datetimemodule.c	datetime_isoformat	specs	-
Modules/_datetimemodule.c	parse_hh_mm_ss_ff	correction	-
Modules/_datetimemodule.c	time_isoformat	specs	-
Modules/_datetimemodule.c	-	capi_types	-
Modules/_decimal/_decimal.c	-	cond_map_template	-
Modules/_decimal/_decimal.c	-	dec_signal_string	-
Modules/_decimal/_decimal.c	-	dflt_ctx	-
Modules/_decimal/_decimal.c	-	int_constants	-
Modules/_decimal/_decimal.c	-	invalid_rounding_err	-
Modules/_decimal/_decimal.c	-	invalid_signals_err	-
Modules/_decimal/_decimal.c	-	signal_map_template	-
Modules/_decimal/_decimal.c	-	ssize_constants	-
Modules/_decimal/_decimal.c	-	INVALID_SIGNALDICT_ERROR_MSG	-
Modules/_elementtree.c	-	ExpatMemoryHandler	-
Modules/_hashopenssl.c	-	py_hashes	-
Modules/_hacl/Hacl_Hash_SHA1.c	-	_h0	-
Modules/_hacl/Hacl_Hash_MD5.c	-	_h0	-
Modules/_hacl/Hacl_Hash_MD5.c	-	_t	-
Modules/_io/_iomodule.c	-	static_types	-
Modules/_io/textio.c	-	encodefuncs	-
Modules/_io/winconsoleio.c	-	_PyWindowsConsoleIO_Type	-
Modules/_localemodule.c	-	langinfo_constants	-
Modules/_lsprof.c	-	callback_table	-
Modules/_pickle.c	-	READ_WHOLE_LINE	-
Modules/_sqlite/module.c	-	error_codes	-
Modules/_sre/sre.c	pattern_repr	flag_names	-
# XXX I'm pretty sure this is actually constant:
Modules/_sre/sre_targets.h	-	sre_targets	-
Modules/_sre.c	pattern_repr	flag_names	-
Modules/_struct.c	-	bigendian_table	-
Modules/_struct.c	-	lilendian_table	-
Modules/_struct.c	-	native_table	-
Modules/_tkinter.c	-	state_key	-
Modules/_interpchannelsmodule.c	-	_channelid_end_recv	-
Modules/_interpchannelsmodule.c	-	_channelid_end_send	-
Modules/_zoneinfo.c	-	DAYS_BEFORE_MONTH	-
Modules/_zoneinfo.c	-	DAYS_IN_MONTH	-
Modules/_interpretersmodule.c	-	no_exception	-
Modules/arraymodule.c	-	descriptors	-
Modules/arraymodule.c	-	emptybuf	-
Modules/cjkcodecs/_codecs_cn.c	-	_mapping_list	-
Modules/cjkcodecs/_codecs_cn.c	-	mapping_list	-
Modules/cjkcodecs/_codecs_cn.c	-	_codec_list	-
Modules/cjkcodecs/_codecs_cn.c	-	codec_list	-
Modules/cjkcodecs/_codecs_hk.c	-	big5hkscs_pairenc_table	-
Modules/cjkcodecs/_codecs_hk.c	-	_mapping_list	-
Modules/cjkcodecs/_codecs_hk.c	-	mapping_list	-
Modules/cjkcodecs/_codecs_hk.c	-	_codec_list	-
Modules/cjkcodecs/_codecs_hk.c	-	codec_list	-
Modules/cjkcodecs/_codecs_iso2022.c	-	iso2022_kr_config	-
Modules/cjkcodecs/_codecs_iso2022.c	-	iso2022_jp_config	-
Modules/cjkcodecs/_codecs_iso2022.c	-	iso2022_jp_1_config	-
Modules/cjkcodecs/_codecs_iso2022.c	-	iso2022_jp_2_config	-
Modules/cjkcodecs/_codecs_iso2022.c	-	iso2022_jp_2004_config	-
Modules/cjkcodecs/_codecs_iso2022.c	-	iso2022_jp_3_config	-
Modules/cjkcodecs/_codecs_iso2022.c	-	iso2022_jp_ext_config	-
Modules/cjkcodecs/_codecs_iso2022.c	-	_mapping_list	-
Modules/cjkcodecs/_codecs_iso2022.c	-	mapping_list	-
Modules/cjkcodecs/_codecs_iso2022.c	-	_codec_list	-
Modules/cjkcodecs/_codecs_iso2022.c	-	codec_list	-
Modules/cjkcodecs/_codecs_jp.c	-	_mapping_list	-
Modules/cjkcodecs/_codecs_jp.c	-	mapping_list	-
Modules/cjkcodecs/_codecs_jp.c	-	_codec_list	-
Modules/cjkcodecs/_codecs_jp.c	-	codec_list	-
Modules/cjkcodecs/_codecs_kr.c	-	u2johabjamo	-
Modules/cjkcodecs/_codecs_kr.c	-	_mapping_list	-
Modules/cjkcodecs/_codecs_kr.c	-	mapping_list	-
Modules/cjkcodecs/_codecs_kr.c	-	_codec_list	-
Modules/cjkcodecs/_codecs_kr.c	-	codec_list	-
Modules/cjkcodecs/_codecs_tw.c	-	_mapping_list	-
Modules/cjkcodecs/_codecs_tw.c	-	mapping_list	-
Modules/cjkcodecs/_codecs_tw.c	-	_codec_list	-
Modules/cjkcodecs/_codecs_tw.c	-	codec_list	-
Modules/cjkcodecs/cjkcodecs.h	-	__methods	-
Modules/cmathmodule.c	-	acos_special_values	-
Modules/cmathmodule.c	-	acosh_special_values	-
Modules/cmathmodule.c	-	asinh_special_values	-
Modules/cmathmodule.c	-	atanh_special_values	-
Modules/cmathmodule.c	-	cosh_special_values	-
Modules/cmathmodule.c	-	exp_special_values	-
Modules/cmathmodule.c	-	log_special_values	-
Modules/cmathmodule.c	-	rect_special_values	-
Modules/cmathmodule.c	-	sinh_special_values	-
Modules/cmathmodule.c	-	sqrt_special_values	-
Modules/cmathmodule.c	-	tanh_special_values	-
Modules/config.c	-	_PyImport_Inittab	-
Modules/faulthandler.c	-	faulthandler_handlers	-
Modules/getnameinfo.c	-	gni_afdl	-
Modules/posixmodule.c	os_getxattr_impl	buffer_sizes	-
Modules/posixmodule.c	os_listxattr_impl	buffer_sizes	-
Modules/posixmodule.c	-	posix_constants_confstr	-
Modules/posixmodule.c	-	posix_constants_pathconf	-
Modules/posixmodule.c	-	posix_constants_sysconf	-
Modules/pyexpat.c	-	ExpatMemoryHandler	-
Modules/pyexpat.c	-	error_info_of	-
Modules/pyexpat.c	-	handler_info	-
Modules/termios.c	-	termios_constants	-
Modules/timemodule.c	init_timezone	YEAR	-
Objects/bytearrayobject.c	-	_PyByteArray_empty_string	-
Objects/complexobject.c	-	c_1	-
Objects/exceptions.c	-	static_exceptions	-
Objects/genobject.c	-	ASYNC_GEN_IGNORED_EXIT_MSG	-
Objects/genobject.c	-	NON_INIT_CORO_MSG	-
Objects/longobject.c	-	_PyLong_DigitValue	-
Objects/longobject.c	-	PyLong_LAYOUT	-
Objects/object.c	-	_Py_SwappedOp	-
Objects/object.c	-	_Py_abstract_hack	-
Objects/object.c	-	last_final_reftotal	-
Objects/object.c	-	static_types	-
Objects/obmalloc.c	-	_PyMem	-
Objects/obmalloc.c	-	_PyMem_Debug	-
Objects/obmalloc.c	-	_PyMem_Raw	-
Objects/obmalloc.c	-	_PyObject	-
Objects/obmalloc.c	-	last_final_leaks	-
Objects/obmalloc.c	-	obmalloc_state_main	-
Objects/obmalloc.c	-	obmalloc_state_initialized	-
Objects/typeobject.c	-	name_op	-
Objects/typeobject.c	-	slotdefs	-
Objects/unicodeobject.c	-	stripfuncnames	-
Objects/unicodeobject.c	-	utf7_category	-
Objects/unicodeobject.c	unicode_decode_call_errorhandler_wchar	argparse	-
Objects/unicodeobject.c	unicode_decode_call_errorhandler_writer	argparse	-
Objects/unicodeobject.c	unicode_encode_call_errorhandler	argparse	-
Objects/unicodeobject.c	unicode_translate_call_errorhandler	argparse	-
Parser/parser.c	-	reserved_keywords	-
Parser/parser.c	-	soft_keywords	-
Parser/lexer/lexer.c	-	type_comment_prefix	-
Python/ast_opt.c	fold_unaryop	ops	-
Python/ceval.c	-	_PyEval_BinaryOps	-
Python/ceval.c	-	_Py_INTERPRETER_TRAMPOLINE_INSTRUCTIONS	-
Python/codecs.c	-	Py_hexdigits	-
Python/codecs.c	-	codecs_builtin_error_handlers	-
Python/codecs.c	-	ucnhash_capi	-
Python/codecs.c	_PyCodec_InitRegistry	methods	-
Python/compile.c	-	NO_LOCATION	-
Python/dynload_shlib.c	-	_PyImport_DynLoadFiletab	-
Python/dynload_stub.c	-	_PyImport_DynLoadFiletab	-
Python/frozen.c	-	aliases	-
Python/frozen.c	-	bootstrap_modules	-
Python/frozen.c	-	stdlib_modules	-
Python/frozen.c	-	test_modules	-
Python/frozen.c	-	_PyImport_FrozenAliases	-
Python/frozen.c	-	_PyImport_FrozenBootstrap	-
Python/frozen.c	-	_PyImport_FrozenStdlib	-
Python/frozen.c	-	_PyImport_FrozenTest	-
Python/getopt.c	-	longopts	-
Python/import.c	-	_PyImport_Inittab	-
Python/import.c	-	_PySys_ImplCacheTag	-
Python/intrinsics.c	-	_PyIntrinsics_UnaryFunctions	-
Python/intrinsics.c	-	_PyIntrinsics_BinaryFunctions	-
Python/lock.c	-	TIME_TO_BE_FAIR_NS	-
Python/opcode_targets.h	-	opcode_targets	-
Python/perf_trampoline.c	-	_Py_perfmap_callbacks	-
Python/perf_jit_trampoline.c	-	_Py_perfmap_jit_callbacks	-
Python/perf_jit_trampoline.c	-	perf_jit_map_state	-
Python/pyhash.c	-	PyHash_Func	-
Python/pylifecycle.c	-	_C_LOCALE_WARNING	-
Python/pylifecycle.c	-	_PyOS_mystrnicmp_hack	-
Python/pylifecycle.c	-	_TARGET_LOCALES	-
Python/pylifecycle.c	-	INTERPRETER_TRAMPOLINE_CODEDEF	-
Python/pystate.c	-	initial	-
Python/specialize.c	-	adaptive_opcodes	-
Python/specialize.c	-	cache_requirements	-
Python/stdlib_module_names.h	-	_Py_stdlib_module_names	-
Python/sysmodule.c	-	perf_map_state	-
Python/sysmodule.c	-	_PySys_ImplCacheTag	-
Python/sysmodule.c	-	_PySys_ImplName	-
Python/sysmodule.c	-	whatstrings	-
Python/optimizer.c	-	_PyDefaultOptimizer_Type	-
Python/optimizer.c	-	_PyCounterExecutor_Type	-
Python/optimizer.c	-	_PyCounterOptimizer_Type	-
Python/optimizer.c	-	_PyUOpExecutor_Type	-
Python/optimizer.c	-	_PyUOpOptimizer_Type	-
Python/optimizer.c	-	_PyOptimizer_Default	-
Python/optimizer.c	-	_ColdExit_Type	-
Python/optimizer.c	-	Py_FatalErrorExecutor	-
Python/optimizer.c	-	EMPTY_FILTER	-

##-----------------------
## test code

Modules/_ctypes/_ctypes_test.c	-	_xxx_lib	-
Modules/_ctypes/_ctypes_test.c	-	an_integer	-
Modules/_ctypes/_ctypes_test.c	-	bottom	-
Modules/_ctypes/_ctypes_test.c	-	last_tf_arg_s	-
Modules/_ctypes/_ctypes_test.c	-	last_tf_arg_u	-
Modules/_ctypes/_ctypes_test.c	-	last_tfrsuv_arg	-
Modules/_ctypes/_ctypes_test.c	-	left	-
Modules/_ctypes/_ctypes_test.c	-	my_eggs	-
Modules/_ctypes/_ctypes_test.c	-	my_spams	-
Modules/_ctypes/_ctypes_test.c	-	right	-
Modules/_ctypes/_ctypes_test.c	-	top	-
Modules/_testbuffer.c	-	NDArray_Type	-
Modules/_testbuffer.c	-	StaticArray_Type	-
Modules/_testbuffer.c	-	Struct	-
Modules/_testbuffer.c	-	_testbuffer_functions	-
Modules/_testbuffer.c	-	_testbuffermodule	-
Modules/_testbuffer.c	-	calcsize	-
Modules/_testbuffer.c	-	infobuf	-
Modules/_testbuffer.c	-	ndarray_as_buffer	-
Modules/_testbuffer.c	-	ndarray_as_mapping	-
Modules/_testbuffer.c	-	ndarray_as_sequence	-
Modules/_testbuffer.c	-	ndarray_getset	-
Modules/_testbuffer.c	-	ndarray_methods	-
Modules/_testbuffer.c	-	simple_fmt	-
Modules/_testbuffer.c	-	simple_format	-
Modules/_testbuffer.c	-	static_buffer	-
Modules/_testbuffer.c	-	static_mem	-
Modules/_testbuffer.c	-	static_shape	-
Modules/_testbuffer.c	-	static_strides	-
Modules/_testbuffer.c	-	staticarray_as_buffer	-
Modules/_testbuffer.c	-	structmodule	-
Modules/_testbuffer.c	ndarray_init	kwlist	-
Modules/_testbuffer.c	ndarray_memoryview_from_buffer	format	-
Modules/_testbuffer.c	ndarray_memoryview_from_buffer	info	-
Modules/_testbuffer.c	ndarray_memoryview_from_buffer	shape	-
Modules/_testbuffer.c	ndarray_memoryview_from_buffer	strides	-
Modules/_testbuffer.c	ndarray_memoryview_from_buffer	suboffsets	-
Modules/_testbuffer.c	ndarray_push	kwlist	-
Modules/_testbuffer.c	staticarray_init	kwlist	-
Modules/_testcapi/buffer.c	-	testBufType	-
Modules/_testcapi/code.c	get_code_extra_index	key	-
Modules/_testcapi/datetime.c	-	test_run_counter	-
Modules/_testcapi/docstring.c	-	DocStringNoSignatureTest	-
Modules/_testcapi/docstring.c	-	DocStringUnrepresentableSignatureTest	-
Modules/_testcapi/exceptions.c	-	PyRecursingInfinitelyError_Type	-
Modules/_testcapi/heaptype.c	-	_testcapimodule	-
Modules/_testcapi/mem.c	-	FmData	-
Modules/_testcapi/mem.c	-	FmHook	-
Modules/_testcapi/structmember.c	-	test_structmembersType_OldAPI	-
Modules/_testcapi/watchers.c	-	g_dict_watch_events	-
Modules/_testcapi/watchers.c	-	g_dict_watchers_installed	-
Modules/_testcapi/watchers.c	-	g_type_modified_events	-
Modules/_testcapi/watchers.c	-	g_type_watchers_installed	-
Modules/_testcapi/watchers.c	-	code_watcher_ids	-
Modules/_testcapi/watchers.c	-	num_code_object_created_events	-
Modules/_testcapi/watchers.c	-	num_code_object_destroyed_events	-
Modules/_testcapi/watchers.c	-	pyfunc_watchers	-
Modules/_testcapi/watchers.c	-	func_watcher_ids	-
Modules/_testcapi/watchers.c	-	func_watcher_callbacks	-
Modules/_testcapi/watchers.c	-	context_watcher_ids	-
Modules/_testcapi/watchers.c	-	context_switches	-
Modules/_testcapi/watchers.c	add_context_watcher	callbacks	-
Modules/_testcapimodule.c	-	BasicStaticTypes	-
Modules/_testcapimodule.c	-	num_basic_static_types_used	-
Modules/_testcapimodule.c	-	ContainerNoGC_members	-
Modules/_testcapimodule.c	-	ContainerNoGC_type	-
Modules/_testcapimodule.c	-	FmData	-
Modules/_testcapimodule.c	-	FmHook	-
Modules/_testcapimodule.c	-	GenericAlias_Type	-
Modules/_testcapimodule.c	-	Generic_Type	-
Modules/_testcapimodule.c	-	HeapCTypeSetattr_slots	-
Modules/_testcapimodule.c	-	HeapCTypeSetattr_spec	-
Modules/_testcapimodule.c	-	HeapCTypeSubclassWithFinalizer_slots	-
Modules/_testcapimodule.c	-	HeapCTypeSubclassWithFinalizer_spec	-
Modules/_testcapimodule.c	-	HeapCTypeSubclass_slots	-
Modules/_testcapimodule.c	-	HeapCTypeSubclass_spec	-
Modules/_testcapimodule.c	-	HeapCTypeWithBuffer_slots	-
Modules/_testcapimodule.c	-	HeapCTypeWithBuffer_spec	-
Modules/_testcapimodule.c	-	HeapCTypeWithDict_slots	-
Modules/_testcapimodule.c	-	HeapCTypeWithDict_spec	-
Modules/_testcapimodule.c	-	HeapCTypeWithNegativeDict_slots	-
Modules/_testcapimodule.c	-	HeapCTypeWithNegativeDict_spec	-
Modules/_testcapimodule.c	-	HeapCTypeWithWeakref_slots	-
Modules/_testcapimodule.c	-	HeapCTypeWithWeakref_spec	-
Modules/_testcapimodule.c	-	HeapCType_slots	-
Modules/_testcapimodule.c	-	HeapCType_spec	-
Modules/_testcapimodule.c	-	HeapDocCType_slots	-
Modules/_testcapimodule.c	-	HeapDocCType_spec	-
Modules/_testcapimodule.c	-	HeapGcCType_slots	-
Modules/_testcapimodule.c	-	HeapGcCType_spec	-
Modules/_testcapimodule.c	-	MethClass_Type	-
Modules/_testcapimodule.c	-	MethInstance_Type	-
Modules/_testcapimodule.c	-	MethStatic_Type	-
Modules/_testcapimodule.c	-	MethodDescriptor2_Type	-
Modules/_testcapimodule.c	-	MethodDescriptorBase_Type	-
Modules/_testcapimodule.c	-	MethodDescriptorDerived_Type	-
Modules/_testcapimodule.c	-	MethodDescriptorNopGet_Type	-
Modules/_testcapimodule.c	-	MyList_Type	-
Modules/_testcapimodule.c	-	PyRecursingInfinitelyError_Type	-
Modules/_testcapimodule.c	-	TestError	-
Modules/_testcapimodule.c	-	TestMethods	-
Modules/_testcapimodule.c	-	_HashInheritanceTester_Type	-
Modules/_testcapimodule.c	-	_testcapimodule	-
Modules/_testcapimodule.c	-	awaitType	-
Modules/_testcapimodule.c	-	awaitType_as_async	-
Modules/_testcapimodule.c	-	capsule_context	-
Modules/_testcapimodule.c	-	capsule_destructor_call_count	-
Modules/_testcapimodule.c	-	capsule_error	-
Modules/_testcapimodule.c	-	capsule_name	-
Modules/_testcapimodule.c	-	capsule_pointer	-
Modules/_testcapimodule.c	-	decimal_initialized	-
Modules/_testcapimodule.c	-	generic_alias_methods	-
Modules/_testcapimodule.c	-	generic_methods	-
Modules/_testcapimodule.c	-	heapctype_members	-
Modules/_testcapimodule.c	-	heapctypesetattr_members	-
Modules/_testcapimodule.c	-	heapctypesubclass_members	-
Modules/_testcapimodule.c	-	heapctypewithdict_getsetlist	-
Modules/_testcapimodule.c	-	heapctypewithdict_members	-
Modules/_testcapimodule.c	-	heapctypewithnegativedict_members	-
Modules/_testcapimodule.c	-	heapctypewithweakref_members	-
Modules/_testcapimodule.c	-	ipowType	-
Modules/_testcapimodule.c	-	ipowType_as_number	-
Modules/_testcapimodule.c	-	matmulType	-
Modules/_testcapimodule.c	-	matmulType_as_number	-
Modules/_testcapimodule.c	-	meth_class_methods	-
Modules/_testcapimodule.c	-	meth_instance_methods	-
Modules/_testcapimodule.c	-	meth_static_methods	-
Modules/_testcapimodule.c	-	ml	-
Modules/_testcapimodule.c	-	str1	-
Modules/_testcapimodule.c	-	str2	-
Modules/_testcapimodule.c	-	test_c_thread	-
Modules/_testcapimodule.c	-	test_members	-
Modules/_testcapimodule.c	-	test_run_counter	-
Modules/_testcapimodule.c	-	test_structmembersType	-
Modules/_testcapimodule.c	-	thread_done	-
Modules/_testcapimodule.c	-	x	-
Modules/_testcapimodule.c	-	wait_done	-
Modules/_testcapimodule.c	getargs_keyword_only	keywords	-
Modules/_testcapimodule.c	getargs_keywords	keywords	-
Modules/_testcapimodule.c	getargs_positional_only_and_keywords	keywords	-
Modules/_testcapimodule.c	getargs_s_hash_int2	keywords	static char*[]
Modules/_testcapimodule.c	make_exception_with_doc	kwlist	-
Modules/_testcapimodule.c	raise_SIGINT_then_send_None	PyId_send	-
Modules/_testcapimodule.c	slot_tp_del	PyId___tp_del__	-
Modules/_testcapimodule.c	test_capsule	buffer	-
Modules/_testcapimodule.c	getargs_empty	kwlist	-
Modules/_testcapimodule.c	test_structmembers_new	keywords	-
Modules/_testcapimodule.c	getargs_s_hash_int	keywords	-
Modules/_testcapimodule.c	-	g_dict_watch_events	-
Modules/_testcapimodule.c	-	g_dict_watchers_installed	-
Modules/_testcapimodule.c	-	g_type_modified_events	-
Modules/_testcapimodule.c	-	g_type_watchers_installed	-
Modules/_testimportmultiple.c	-	_barmodule	-
Modules/_testimportmultiple.c	-	_foomodule	-
Modules/_testimportmultiple.c	-	_testimportmultiple	-
Modules/_testinternalcapi.c	-	pending_identify_result	-
Modules/_testmultiphase.c	-	Example_Type_slots	-
Modules/_testmultiphase.c	-	Example_Type_spec	-
Modules/_testmultiphase.c	-	Example_methods	-
Modules/_testmultiphase.c	-	StateAccessType_Type_slots	-
Modules/_testmultiphase.c	-	StateAccessType_methods	-
Modules/_testmultiphase.c	-	StateAccessType_spec	-
Modules/_testmultiphase.c	-	Str_Type_slots	-
Modules/_testmultiphase.c	-	Str_Type_spec	-
Modules/_testmultiphase.c	-	def_bad_large	-
Modules/_testmultiphase.c	-	def_bad_negative	-
Modules/_testmultiphase.c	-	def_create_int_with_state	-
Modules/_testmultiphase.c	-	def_create_null	-
Modules/_testmultiphase.c	-	def_create_raise	-
Modules/_testmultiphase.c	-	def_create_unreported_exception	-
Modules/_testmultiphase.c	-	def_exec_err	-
Modules/_testmultiphase.c	-	def_exec_raise	-
Modules/_testmultiphase.c	-	def_exec_unreported_exception	-
Modules/_testmultiphase.c	-	def_meth_state_access	-
Modules/_testmultiphase.c	-	def_negative_size	-
Modules/_testmultiphase.c	-	def_nonascii_kana	-
Modules/_testmultiphase.c	-	def_nonascii_latin	-
Modules/_testmultiphase.c	-	def_nonmodule	-
Modules/_testmultiphase.c	-	def_nonmodule_with_exec_slots	-
Modules/_testmultiphase.c	-	def_nonmodule_with_methods	-
Modules/_testmultiphase.c	-	main_def	-
Modules/_testmultiphase.c	-	main_slots	-
Modules/_testmultiphase.c	-	meth_state_access_slots	-
Modules/_testmultiphase.c	-	nonmodule_methods	-
Modules/_testmultiphase.c	-	null_slots_def	-
Modules/_testmultiphase.c	-	slots_bad_large	-
Modules/_testmultiphase.c	-	slots_bad_negative	-
Modules/_testmultiphase.c	-	slots_create_nonmodule	-
Modules/_testmultiphase.c	-	slots_create_nonmodule	-
Modules/_testmultiphase.c	-	slots_create_null	-
Modules/_testmultiphase.c	-	slots_create_raise	-
Modules/_testmultiphase.c	-	slots_create_unreported_exception	-
Modules/_testmultiphase.c	-	slots_exec_err	-
Modules/_testmultiphase.c	-	slots_exec_raise	-
Modules/_testmultiphase.c	-	slots_exec_unreported_exception	-
Modules/_testmultiphase.c	-	slots_nonmodule_with_exec_slots	-
Modules/_testmultiphase.c	-	testexport_methods	-
Modules/_testmultiphase.c	-	uninitialized_def	-
Modules/_testsinglephase.c	-	global_state	-
Modules/_testsinglephase.c	-	static_module_circular	-
Modules/_xxtestfuzz/_xxtestfuzz.c	-	_fuzzmodule	-
Modules/_xxtestfuzz/_xxtestfuzz.c	-	module_methods	-
Modules/_xxtestfuzz/fuzzer.c	-	RE_FLAG_DEBUG	-
Modules/_xxtestfuzz/fuzzer.c	-	ast_literal_eval_method	-
Modules/_xxtestfuzz/fuzzer.c	-	bytesio_type	-
Modules/_xxtestfuzz/fuzzer.c	-	compiled_patterns	-
Modules/_xxtestfuzz/fuzzer.c	-	csv_error	-
Modules/_xxtestfuzz/fuzzer.c	-	csv_module	-
Modules/_xxtestfuzz/fuzzer.c	-	json_loads_method	-
Modules/_xxtestfuzz/fuzzer.c	-	regex_patterns	-
Modules/_xxtestfuzz/fuzzer.c	-	re_compile_method	-
Modules/_xxtestfuzz/fuzzer.c	-	re_error_exception	-
Modules/_xxtestfuzz/fuzzer.c	-	struct_error	-
Modules/_xxtestfuzz/fuzzer.c	-	struct_unpack_method	-
Modules/_xxtestfuzz/fuzzer.c	-	xmlparser_type	-
Modules/_xxtestfuzz/fuzzer.c	-	pycompile_scratch	-
Modules/_xxtestfuzz/fuzzer.c	-	start_vals	-
Modules/_xxtestfuzz/fuzzer.c	-	optimize_vals	-
Modules/_xxtestfuzz/fuzzer.c	LLVMFuzzerTestOneInput	CSV_READER_INITIALIZED	-
Modules/_xxtestfuzz/fuzzer.c	LLVMFuzzerTestOneInput	JSON_LOADS_INITIALIZED	-
Modules/_xxtestfuzz/fuzzer.c	LLVMFuzzerTestOneInput	SRE_COMPILE_INITIALIZED	-
Modules/_xxtestfuzz/fuzzer.c	LLVMFuzzerTestOneInput	SRE_MATCH_INITIALIZED	-
Modules/_xxtestfuzz/fuzzer.c	LLVMFuzzerTestOneInput	STRUCT_UNPACK_INITIALIZED	-
Modules/_xxtestfuzz/fuzzer.c	LLVMFuzzerTestOneInput	AST_LITERAL_EVAL_INITIALIZED	-
Modules/_xxtestfuzz/fuzzer.c	LLVMFuzzerTestOneInput	ELEMENTTREE_PARSEWHOLE_INITIALIZED	-

##-----------------------
## the analyzer should have ignored these
# XXX Fix the analyzer.

## forward/extern references
Include/internal/pycore_importdl.h	-	_PyImport_DynLoadFiletab	-
Include/py_curses.h	-	PyCurses_API	-
Include/pydecimal.h	-	_decimal_api	-
Modules/_io/fileio.c	-	_Py_open_cloexec_works	-
Modules/_io/_iomodule.h	-	PyIOBase_Type	-
Modules/_io/_iomodule.h	-	PyRawIOBase_Type	-
Modules/_io/_iomodule.h	-	PyBufferedIOBase_Type	-
Modules/_io/_iomodule.h	-	PyTextIOBase_Type	-
Modules/_io/_iomodule.h	-	PyFileIO_Type	-
Modules/_io/_iomodule.h	-	PyBytesIO_Type	-
Modules/_io/_iomodule.h	-	PyStringIO_Type	-
Modules/_io/_iomodule.h	-	PyBufferedReader_Type	-
Modules/_io/_iomodule.h	-	PyBufferedWriter_Type	-
Modules/_io/_iomodule.h	-	PyBufferedRWPair_Type	-
Modules/_io/_iomodule.h	-	PyBufferedRandom_Type	-
Modules/_io/_iomodule.h	-	PyTextIOWrapper_Type	-
Modules/_io/_iomodule.h	-	PyIncrementalNewlineDecoder_Type	-
Modules/_io/_iomodule.h	-	_PyBytesIOBuffer_Type	-
Modules/_io/_iomodule.h	-	_PyIO_Module	-
Modules/_io/_iomodule.h	-	_PyIO_str_close	-
Modules/_io/_iomodule.h	-	_PyIO_str_closed	-
Modules/_io/_iomodule.h	-	_PyIO_str_decode	-
Modules/_io/_iomodule.h	-	_PyIO_str_encode	-
Modules/_io/_iomodule.h	-	_PyIO_str_fileno	-
Modules/_io/_iomodule.h	-	_PyIO_str_flush	-
Modules/_io/_iomodule.h	-	_PyIO_str_getstate	-
Modules/_io/_iomodule.h	-	_PyIO_str_isatty	-
Modules/_io/_iomodule.h	-	_PyIO_str_newlines	-
Modules/_io/_iomodule.h	-	_PyIO_str_nl	-
Modules/_io/_iomodule.h	-	_PyIO_str_peek	-
Modules/_io/_iomodule.h	-	_PyIO_str_read	-
Modules/_io/_iomodule.h	-	_PyIO_str_read1	-
Modules/_io/_iomodule.h	-	_PyIO_str_readable	-
Modules/_io/_iomodule.h	-	_PyIO_str_readall	-
Modules/_io/_iomodule.h	-	_PyIO_str_readinto	-
Modules/_io/_iomodule.h	-	_PyIO_str_readline	-
Modules/_io/_iomodule.h	-	_PyIO_str_reset	-
Modules/_io/_iomodule.h	-	_PyIO_str_seek	-
Modules/_io/_iomodule.h	-	_PyIO_str_seekable	-
Modules/_io/_iomodule.h	-	_PyIO_str_setstate	-
Modules/_io/_iomodule.h	-	_PyIO_str_tell	-
Modules/_io/_iomodule.h	-	_PyIO_str_truncate	-
Modules/_io/_iomodule.h	-	_PyIO_str_writable	-
Modules/_io/_iomodule.h	-	_PyIO_str_write	-
Modules/_io/_iomodule.h	-	_PyIO_empty_str	-
Modules/_io/_iomodule.h	-	_PyIO_empty_bytes	-
Modules/_multiprocessing/multiprocessing.h	-	_PyMp_SemLockType	-
Modules/_sqlite/module.c	-	_pysqlite_converters	-
Modules/_sqlite/module.c	-	_pysqlite_enable_callback_tracebacks	-
Modules/_sqlite/module.c	-	pysqlite_BaseTypeAdapted	-
Modules/_sqlite/module.h	-	pysqlite_global_state	-
Modules/_testcapimodule.c	-	_PyBytesIOBuffer_Type	-
Modules/posixmodule.c	-	_Py_open_cloexec_works	-
Modules/posixmodule.c	-	environ	-
Objects/object.c	-	_Py_GenericAliasIterType	-
Objects/object.c	-	_PyMemoryIter_Type	-
Objects/object.c	-	_PyLineIterator	-
Objects/object.c	-	_PyPositionsIterator	-
Python/perf_trampoline.c	-	_Py_trampoline_func_start	-
Python/perf_trampoline.c	-	_Py_trampoline_func_end	-
Modules/expat/xmlrole.c	-	prolog0	-
Modules/expat/xmlrole.c	-	prolog1	-
Modules/expat/xmlrole.c	-	prolog2	-
Modules/expat/xmlrole.c	-	doctype0	-
Modules/expat/xmlrole.c	-	doctype1	-
Modules/expat/xmlrole.c	-	doctype2	-
Modules/expat/xmlrole.c	-	doctype3	-
Modules/expat/xmlrole.c	-	doctype4	-
Modules/expat/xmlrole.c	-	doctype5	-
Modules/expat/xmlrole.c	-	internalSubset	-
Modules/expat/xmlrole.c	-	entity0	-
Modules/expat/xmlrole.c	-	entity1	-
Modules/expat/xmlrole.c	-	entity2	-
Modules/expat/xmlrole.c	-	entity3	-
Modules/expat/xmlrole.c	-	entity4	-
Modules/expat/xmlrole.c	-	entity5	-
Modules/expat/xmlrole.c	-	entity6	-
Modules/expat/xmlrole.c	-	entity7	-
Modules/expat/xmlrole.c	-	entity8	-
Modules/expat/xmlrole.c	-	entity9	-
Modules/expat/xmlrole.c	-	entity10	-
Modules/expat/xmlrole.c	-	notation0	-
Modules/expat/xmlrole.c	-	notation1	-
Modules/expat/xmlrole.c	-	notation2	-
Modules/expat/xmlrole.c	-	notation3	-
Modules/expat/xmlrole.c	-	notation4	-
Modules/expat/xmlrole.c	-	attlist0	-
Modules/expat/xmlrole.c	-	attlist1	-
Modules/expat/xmlrole.c	-	attlist2	-
Modules/expat/xmlrole.c	-	attlist3	-
Modules/expat/xmlrole.c	-	attlist4	-
Modules/expat/xmlrole.c	-	attlist5	-
Modules/expat/xmlrole.c	-	attlist6	-
Modules/expat/xmlrole.c	-	attlist7	-
Modules/expat/xmlrole.c	-	attlist8	-
Modules/expat/xmlrole.c	-	attlist9	-
Modules/expat/xmlrole.c	-	element0	-
Modules/expat/xmlrole.c	-	element1	-
Modules/expat/xmlrole.c	-	element2	-
Modules/expat/xmlrole.c	-	element3	-
Modules/expat/xmlrole.c	-	element4	-
Modules/expat/xmlrole.c	-	element5	-
Modules/expat/xmlrole.c	-	element6	-
Modules/expat/xmlrole.c	-	element7	-
Modules/expat/xmlrole.c	-	externalSubset0	-
Modules/expat/xmlrole.c	-	externalSubset1	-
Modules/expat/xmlrole.c	-	condSect0	-
Modules/expat/xmlrole.c	-	condSect1	-
Modules/expat/xmlrole.c	-	condSect2	-
Modules/expat/xmlrole.c	-	declClose	-
Modules/expat/xmlrole.c	-	error	-

## other
Modules/_io/_iomodule.c	-	_PyIO_Module	-
Modules/_sqlite/module.c	-	_sqlite3module	-
Modules/clinic/md5module.c.h	_md5_md5	_keywords	-
Modules/clinic/grpmodule.c.h	grp_getgrgid	_keywords	-
Modules/clinic/grpmodule.c.h	grp_getgrnam	_keywords	-
Objects/object.c	-	constants	static PyObject*[]


## False positives
Python/specialize.c	-	_Py_InitCleanup	-


================================================
File: /Tools/c-analyzer/cpython/known.tsv
================================================
filename	funcname	name	kind	declaration
#filename	funcname	name	kind	is_supported	declaration
#???	-	PyWideStringList	typedef	???


================================================
File: /Tools/c-analyzer/distutils/README
================================================
This is a partial copy of distutils as it was removed in 0faa0ba240e.
It only includes the parts needed by the C parser.


================================================
File: /Tools/c-analyzer/distutils/_msvccompiler.py
================================================
"""distutils._msvccompiler

Contains MSVCCompiler, an implementation of the abstract CCompiler class
for Microsoft Visual Studio 2015.

The module is compatible with VS 2015 and later. You can find legacy support
for older versions in distutils.msvc9compiler and distutils.msvccompiler.
"""

# Written by Perry Stoll
# hacked by Robin Becker and Thomas Heller to do a better job of
#   finding DevStudio (through the registry)
# ported to VS 2005 and VS 2008 by Christian Heimes
# ported to VS 2015 by Steve Dower

import os
import subprocess
import winreg

from distutils.errors import DistutilsPlatformError
from distutils.ccompiler import CCompiler
from distutils import log

from itertools import count

def _find_vc2015():
    try:
        key = winreg.OpenKeyEx(
            winreg.HKEY_LOCAL_MACHINE,
            r"Software\Microsoft\VisualStudio\SxS\VC7",
            access=winreg.KEY_READ | winreg.KEY_WOW64_32KEY
        )
    except OSError:
        log.debug("Visual C++ is not registered")
        return None, None

    best_version = 0
    best_dir = None
    with key:
        for i in count():
            try:
                v, vc_dir, vt = winreg.EnumValue(key, i)
            except OSError:
                break
            if v and vt == winreg.REG_SZ and os.path.isdir(vc_dir):
                try:
                    version = int(float(v))
                except (ValueError, TypeError):
                    continue
                if version >= 14 and version > best_version:
                    best_version, best_dir = version, vc_dir
    return best_version, best_dir

def _find_vc2017():
    """Returns "15, path" based on the result of invoking vswhere.exe
    If no install is found, returns "None, None"

    The version is returned to avoid unnecessarily changing the function
    result. It may be ignored when the path is not None.

    If vswhere.exe is not available, by definition, VS 2017 is not
    installed.
    """
    root = os.environ.get("ProgramFiles(x86)") or os.environ.get("ProgramFiles")
    if not root:
        return None, None

    try:
        path = subprocess.check_output([
            os.path.join(root, "Microsoft Visual Studio", "Installer", "vswhere.exe"),
            "-latest",
            "-prerelease",
            "-requires", "Microsoft.VisualStudio.Component.VC.Tools.x86.x64",
            "-property", "installationPath",
            "-products", "*",
        ], encoding="mbcs", errors="strict").strip()
    except (subprocess.CalledProcessError, OSError, UnicodeDecodeError):
        return None, None

    path = os.path.join(path, "VC", "Auxiliary", "Build")
    if os.path.isdir(path):
        return 15, path

    return None, None

PLAT_SPEC_TO_RUNTIME = {
    'x86' : 'x86',
    'x86_amd64' : 'x64',
    'x86_arm' : 'arm',
    'x86_arm64' : 'arm64'
}

def _find_vcvarsall(plat_spec):
    # bpo-38597: Removed vcruntime return value
    _, best_dir = _find_vc2017()

    if not best_dir:
        best_version, best_dir = _find_vc2015()

    if not best_dir:
        log.debug("No suitable Visual C++ version found")
        return None, None

    vcvarsall = os.path.join(best_dir, "vcvarsall.bat")
    if not os.path.isfile(vcvarsall):
        log.debug("%s cannot be found", vcvarsall)
        return None, None

    return vcvarsall, None

def _get_vc_env(plat_spec):
    if os.getenv("DISTUTILS_USE_SDK"):
        return {
            key.lower(): value
            for key, value in os.environ.items()
        }

    vcvarsall, _ = _find_vcvarsall(plat_spec)
    if not vcvarsall:
        raise DistutilsPlatformError("Unable to find vcvarsall.bat")

    try:
        out = subprocess.check_output(
            'cmd /u /c "{}" {} && set'.format(vcvarsall, plat_spec),
            stderr=subprocess.STDOUT,
        ).decode('utf-16le', errors='replace')
    except subprocess.CalledProcessError as exc:
        log.error(exc.output)
        raise DistutilsPlatformError("Error executing {}"
                .format(exc.cmd))

    env = {
        key.lower(): value
        for key, _, value in
        (line.partition('=') for line in out.splitlines())
        if key and value
    }

    return env

def _find_exe(exe, paths=None):
    """Return path to an MSVC executable program.

    Tries to find the program in several places: first, one of the
    MSVC program search paths from the registry; next, the directories
    in the PATH environment variable.  If any of those work, return an
    absolute path that is known to exist.  If none of them work, just
    return the original program name, 'exe'.
    """
    if not paths:
        paths = os.getenv('path').split(os.pathsep)
    for p in paths:
        fn = os.path.join(os.path.abspath(p), exe)
        if os.path.isfile(fn):
            return fn
    return exe

# A map keyed by get_platform() return values to values accepted by
# 'vcvarsall.bat'. Always cross-compile from x86 to work with the
# lighter-weight MSVC installs that do not include native 64-bit tools.
PLAT_TO_VCVARS = {
    'win32' : 'x86',
    'win-amd64' : 'x86_amd64',
    'win-arm32' : 'x86_arm',
    'win-arm64' : 'x86_arm64'
}

class MSVCCompiler(CCompiler) :
    """Concrete class that implements an interface to Microsoft Visual C++,
       as defined by the CCompiler abstract class."""

    compiler_type = 'msvc'

    # Just set this so CCompiler's constructor doesn't barf.  We currently
    # don't use the 'set_executables()' bureaucracy provided by CCompiler,
    # as it really isn't necessary for this sort of single-compiler class.
    # Would be nice to have a consistent interface with UnixCCompiler,
    # though, so it's worth thinking about.
    executables = {}

    # Private class data (need to distinguish C from C++ source for compiler)
    _c_extensions = ['.c']
    _cpp_extensions = ['.cc', '.cpp', '.cxx']
    _rc_extensions = ['.rc']
    _mc_extensions = ['.mc']

    # Needed for the filename generation methods provided by the
    # base class, CCompiler.
    src_extensions = (_c_extensions + _cpp_extensions +
                      _rc_extensions + _mc_extensions)
    res_extension = '.res'
    obj_extension = '.obj'
    static_lib_extension = '.lib'
    shared_lib_extension = '.dll'
    static_lib_format = shared_lib_format = '%s%s'
    exe_extension = '.exe'


    def __init__(self, verbose=0, dry_run=0, force=0):
        CCompiler.__init__ (self, verbose, dry_run, force)
        # target platform (.plat_name is consistent with 'bdist')
        self.plat_name = None
        self.initialized = False


================================================
File: /Tools/c-analyzer/distutils/bcppcompiler.py
================================================
"""distutils.bcppcompiler

Contains BorlandCCompiler, an implementation of the abstract CCompiler class
for the Borland C++ compiler.
"""

# This implementation by Lyle Johnson, based on the original msvccompiler.py
# module and using the directions originally published by Gordon Williams.

# XXX looks like there's a LOT of overlap between these two classes:
# someone should sit down and factor out the common code as
# WindowsCCompiler!  --GPW


import os
from distutils.errors import DistutilsExecError, CompileError
from distutils.ccompiler import \
     CCompiler, gen_preprocess_options
from distutils.dep_util import newer

class BCPPCompiler(CCompiler) :
    """Concrete class that implements an interface to the Borland C/C++
    compiler, as defined by the CCompiler abstract class.
    """

    compiler_type = 'bcpp'

    # Just set this so CCompiler's constructor doesn't barf.  We currently
    # don't use the 'set_executables()' bureaucracy provided by CCompiler,
    # as it really isn't necessary for this sort of single-compiler class.
    # Would be nice to have a consistent interface with UnixCCompiler,
    # though, so it's worth thinking about.
    executables = {}

    # Private class data (need to distinguish C from C++ source for compiler)
    _c_extensions = ['.c']
    _cpp_extensions = ['.cc', '.cpp', '.cxx']

    # Needed for the filename generation methods provided by the
    # base class, CCompiler.
    src_extensions = _c_extensions + _cpp_extensions
    obj_extension = '.obj'
    static_lib_extension = '.lib'
    shared_lib_extension = '.dll'
    static_lib_format = shared_lib_format = '%s%s'
    exe_extension = '.exe'


    def __init__ (self,
                  verbose=0,
                  dry_run=0,
                  force=0):

        CCompiler.__init__ (self, verbose, dry_run, force)

        # These executables are assumed to all be in the path.
        # Borland doesn't seem to use any special registry settings to
        # indicate their installation locations.

        self.cc = "bcc32.exe"
        self.linker = "ilink32.exe"
        self.lib = "tlib.exe"

        self.preprocess_options = None
        self.compile_options = ['/tWM', '/O2', '/q', '/g0']
        self.compile_options_debug = ['/tWM', '/Od', '/q', '/g0']

        self.ldflags_shared = ['/Tpd', '/Gn', '/q', '/x']
        self.ldflags_shared_debug = ['/Tpd', '/Gn', '/q', '/x']
        self.ldflags_static = []
        self.ldflags_exe = ['/Gn', '/q', '/x']
        self.ldflags_exe_debug = ['/Gn', '/q', '/x','/r']


    # -- Worker methods ------------------------------------------------

    def preprocess (self,
                    source,
                    output_file=None,
                    macros=None,
                    include_dirs=None,
                    extra_preargs=None,
                    extra_postargs=None):

        (_, macros, include_dirs) = \
            self._fix_compile_args(None, macros, include_dirs)
        pp_opts = gen_preprocess_options(macros, include_dirs)
        pp_args = ['cpp32.exe'] + pp_opts
        if output_file is not None:
            pp_args.append('-o' + output_file)
        if extra_preargs:
            pp_args[:0] = extra_preargs
        if extra_postargs:
            pp_args.extend(extra_postargs)
        pp_args.append(source)

        # We need to preprocess: either we're being forced to, or the
        # source file is newer than the target (or the target doesn't
        # exist).
        if self.force or output_file is None or newer(source, output_file):
            if output_file:
                self.mkpath(os.path.dirname(output_file))
            try:
                self.spawn(pp_args)
            except DistutilsExecError as msg:
                print(msg)
                raise CompileError(msg)

    # preprocess()


================================================
File: /Tools/c-analyzer/distutils/ccompiler.py
================================================
"""distutils.ccompiler

Contains CCompiler, an abstract base class that defines the interface
for the Distutils compiler abstraction model."""

import sys, os, re
from distutils.errors import (
    DistutilsModuleError, DistutilsPlatformError,
)
from distutils.util import split_quoted

class CCompiler:
    """Abstract base class to define the interface that must be implemented
    by real compiler classes.  Also has some utility methods used by
    several compiler classes.

    The basic idea behind a compiler abstraction class is that each
    instance can be used for all the compile/link steps in building a
    single project.  Thus, attributes common to all of those compile and
    link steps -- include directories, macros to define, libraries to link
    against, etc. -- are attributes of the compiler instance.  To allow for
    variability in how individual files are treated, most of those
    attributes may be varied on a per-compilation or per-link basis.
    """

    # 'compiler_type' is a class attribute that identifies this class.  It
    # keeps code that wants to know what kind of compiler it's dealing with
    # from having to import all possible compiler classes just to do an
    # 'isinstance'.  In concrete CCompiler subclasses, 'compiler_type'
    # should really, really be one of the keys of the 'compiler_class'
    # dictionary (see below -- used by the 'new_compiler()' factory
    # function) -- authors of new compiler interface classes are
    # responsible for updating 'compiler_class'!
    compiler_type = None

    # XXX things not handled by this compiler abstraction model:
    #   * client can't provide additional options for a compiler,
    #     e.g. warning, optimization, debugging flags.  Perhaps this
    #     should be the domain of concrete compiler abstraction classes
    #     (UnixCCompiler, MSVCCompiler, etc.) -- or perhaps the base
    #     class should have methods for the common ones.
    #   * can't completely override the include or library searchg
    #     path, ie. no "cc -I -Idir1 -Idir2" or "cc -L -Ldir1 -Ldir2".
    #     I'm not sure how widely supported this is even by Unix
    #     compilers, much less on other platforms.  And I'm even less
    #     sure how useful it is; maybe for cross-compiling, but
    #     support for that is a ways off.  (And anyways, cross
    #     compilers probably have a dedicated binary with the
    #     right paths compiled in.  I hope.)
    #   * can't do really freaky things with the library list/library
    #     dirs, e.g. "-Ldir1 -lfoo -Ldir2 -lfoo" to link against
    #     different versions of libfoo.a in different locations.  I
    #     think this is useless without the ability to null out the
    #     library search path anyways.


    # Subclasses that rely on the standard filename generation methods
    # implemented below should override these; see the comment near
    # those methods ('object_filenames()' et. al.) for details:
    src_extensions = None               # list of strings
    obj_extension = None                # string
    static_lib_extension = None
    shared_lib_extension = None         # string
    static_lib_format = None            # format string
    shared_lib_format = None            # prob. same as static_lib_format
    exe_extension = None                # string

    # Default language settings. language_map is used to detect a source
    # file or Extension target language, checking source filenames.
    # language_order is used to detect the language precedence, when deciding
    # what language to use when mixing source types. For example, if some
    # extension has two files with ".c" extension, and one with ".cpp", it
    # is still linked as c++.
    language_map = {".c"   : "c",
                    ".cc"  : "c++",
                    ".cpp" : "c++",
                    ".cxx" : "c++",
                    ".m"   : "objc",
                   }
    language_order = ["c++", "objc", "c"]

    def __init__(self, verbose=0, dry_run=0, force=0):
        self.dry_run = dry_run
        self.force = force
        self.verbose = verbose

        # 'output_dir': a common output directory for object, library,
        # shared object, and shared library files
        self.output_dir = None

        # 'macros': a list of macro definitions (or undefinitions).  A
        # macro definition is a 2-tuple (name, value), where the value is
        # either a string or None (no explicit value).  A macro
        # undefinition is a 1-tuple (name,).
        self.macros = []

        # 'include_dirs': a list of directories to search for include files
        self.include_dirs = []

        # 'libraries': a list of libraries to include in any link
        # (library names, not filenames: eg. "foo" not "libfoo.a")
        self.libraries = []

        # 'library_dirs': a list of directories to search for libraries
        self.library_dirs = []

        # 'runtime_library_dirs': a list of directories to search for
        # shared libraries/objects at runtime
        self.runtime_library_dirs = []

        # 'objects': a list of object files (or similar, such as explicitly
        # named library files) to include on any link
        self.objects = []

        for key in self.executables.keys():
            self.set_executable(key, self.executables[key])

    def set_executables(self, **kwargs):
        """Define the executables (and options for them) that will be run
        to perform the various stages of compilation.  The exact set of
        executables that may be specified here depends on the compiler
        class (via the 'executables' class attribute), but most will have:
          compiler      the C/C++ compiler
          linker_so     linker used to create shared objects and libraries
          linker_exe    linker used to create binary executables
          archiver      static library creator

        On platforms with a command-line (Unix, DOS/Windows), each of these
        is a string that will be split into executable name and (optional)
        list of arguments.  (Splitting the string is done similarly to how
        Unix shells operate: words are delimited by spaces, but quotes and
        backslashes can override this.  See
        'distutils.util.split_quoted()'.)
        """

        # Note that some CCompiler implementation classes will define class
        # attributes 'cpp', 'cc', etc. with hard-coded executable names;
        # this is appropriate when a compiler class is for exactly one
        # compiler/OS combination (eg. MSVCCompiler).  Other compiler
        # classes (UnixCCompiler, in particular) are driven by information
        # discovered at run-time, since there are many different ways to do
        # basically the same things with Unix C compilers.

        for key in kwargs:
            if key not in self.executables:
                raise ValueError("unknown executable '%s' for class %s" %
                      (key, self.__class__.__name__))
            self.set_executable(key, kwargs[key])

    def set_executable(self, key, value):
        if isinstance(value, str):
            setattr(self, key, split_quoted(value))
        else:
            setattr(self, key, value)

    def _find_macro(self, name):
        i = 0
        for defn in self.macros:
            if defn[0] == name:
                return i
            i += 1
        return None

    def _check_macro_definitions(self, definitions):
        """Ensures that every element of 'definitions' is a valid macro
        definition, ie. either (name,value) 2-tuple or a (name,) tuple.  Do
        nothing if all definitions are OK, raise TypeError otherwise.
        """
        for defn in definitions:
            if not (isinstance(defn, tuple) and
                    (len(defn) in (1, 2) and
                      (isinstance (defn[1], str) or defn[1] is None)) and
                    isinstance (defn[0], str)):
                raise TypeError(("invalid macro definition '%s': " % defn) + \
                      "must be tuple (string,), (string, string), or " + \
                      "(string, None)")


    # -- Bookkeeping methods -------------------------------------------

    def define_macro(self, name, value=None):
        """Define a preprocessor macro for all compilations driven by this
        compiler object.  The optional parameter 'value' should be a
        string; if it is not supplied, then the macro will be defined
        without an explicit value and the exact outcome depends on the
        compiler used (XXX true? does ANSI say anything about this?)
        """
        # Delete from the list of macro definitions/undefinitions if
        # already there (so that this one will take precedence).
        i = self._find_macro (name)
        if i is not None:
            del self.macros[i]

        self.macros.append((name, value))

    def undefine_macro(self, name):
        """Undefine a preprocessor macro for all compilations driven by
        this compiler object.  If the same macro is defined by
        'define_macro()' and undefined by 'undefine_macro()' the last call
        takes precedence (including multiple redefinitions or
        undefinitions).  If the macro is redefined/undefined on a
        per-compilation basis (ie. in the call to 'compile()'), then that
        takes precedence.
        """
        # Delete from the list of macro definitions/undefinitions if
        # already there (so that this one will take precedence).
        i = self._find_macro (name)
        if i is not None:
            del self.macros[i]

        undefn = (name,)
        self.macros.append(undefn)

    def add_include_dir(self, dir):
        """Add 'dir' to the list of directories that will be searched for
        header files.  The compiler is instructed to search directories in
        the order in which they are supplied by successive calls to
        'add_include_dir()'.
        """
        self.include_dirs.append(dir)

    def set_include_dirs(self, dirs):
        """Set the list of directories that will be searched to 'dirs' (a
        list of strings).  Overrides any preceding calls to
        'add_include_dir()'; subsequence calls to 'add_include_dir()' add
        to the list passed to 'set_include_dirs()'.  This does not affect
        any list of standard include directories that the compiler may
        search by default.
        """
        self.include_dirs = dirs[:]


    # -- Private utility methods --------------------------------------
    # (here for the convenience of subclasses)

    # Helper method to prep compiler in subclass compile() methods

    def _fix_compile_args(self, output_dir, macros, include_dirs):
        """Typecheck and fix-up some of the arguments to the 'compile()'
        method, and return fixed-up values.  Specifically: if 'output_dir'
        is None, replaces it with 'self.output_dir'; ensures that 'macros'
        is a list, and augments it with 'self.macros'; ensures that
        'include_dirs' is a list, and augments it with 'self.include_dirs'.
        Guarantees that the returned values are of the correct type,
        i.e. for 'output_dir' either string or None, and for 'macros' and
        'include_dirs' either list or None.
        """
        if output_dir is None:
            output_dir = self.output_dir
        elif not isinstance(output_dir, str):
            raise TypeError("'output_dir' must be a string or None")

        if macros is None:
            macros = self.macros
        elif isinstance(macros, list):
            macros = macros + (self.macros or [])
        else:
            raise TypeError("'macros' (if supplied) must be a list of tuples")

        if include_dirs is None:
            include_dirs = self.include_dirs
        elif isinstance(include_dirs, (list, tuple)):
            include_dirs = list(include_dirs) + (self.include_dirs or [])
        else:
            raise TypeError(
                  "'include_dirs' (if supplied) must be a list of strings")

        return output_dir, macros, include_dirs


    # -- Worker methods ------------------------------------------------
    # (must be implemented by subclasses)

    def preprocess(self, source, output_file=None, macros=None,
                   include_dirs=None, extra_preargs=None, extra_postargs=None):
        """Preprocess a single C/C++ source file, named in 'source'.
        Output will be written to file named 'output_file', or stdout if
        'output_file' not supplied.  'macros' is a list of macro
        definitions as for 'compile()', which will augment the macros set
        with 'define_macro()' and 'undefine_macro()'.  'include_dirs' is a
        list of directory names that will be added to the default list.

        Raises PreprocessError on failure.
        """
        pass


    # -- Miscellaneous methods -----------------------------------------
    # These are all used by the 'gen_lib_options() function; there is
    # no appropriate default implementation so subclasses should
    # implement all of these.

#    def library_dir_option(self, dir):
#        """Return the compiler option to add 'dir' to the list of
#        directories searched for libraries.
#        """
#        raise NotImplementedError
#
#    def runtime_library_dir_option(self, dir):
#        """Return the compiler option to add 'dir' to the list of
#        directories searched for runtime libraries.
#        """
#        raise NotImplementedError
#
#    def library_option(self, lib):
#        """Return the compiler option to add 'lib' to the list of libraries
#        linked into the shared library or executable.
#        """
#        raise NotImplementedError
#
#    def find_library_file (self, dirs, lib, debug=0):
#        """Search the specified list of directories for a static or shared
#        library file 'lib' and return the full path to that file.  If
#        'debug' true, look for a debugging version (if that makes sense on
#        the current platform).  Return None if 'lib' wasn't found in any of
#        the specified directories.
#        """
#        raise NotImplementedError


    # -- Utility methods -----------------------------------------------

    def spawn(self, cmd):
        raise NotImplementedError


# Map a sys.platform/os.name ('posix', 'nt') to the default compiler
# type for that platform. Keys are interpreted as re match
# patterns. Order is important; platform mappings are preferred over
# OS names.
_default_compilers = (

    # Platform string mappings

    # on a cygwin built python we can use gcc like an ordinary UNIXish
    # compiler
    ('cygwin.*', 'unix'),

    # OS name mappings
    ('posix', 'unix'),
    ('nt', 'msvc'),

    )

def get_default_compiler(osname=None, platform=None):
    """Determine the default compiler to use for the given platform.

       osname should be one of the standard Python OS names (i.e. the
       ones returned by os.name) and platform the common value
       returned by sys.platform for the platform in question.

       The default values are os.name and sys.platform in case the
       parameters are not given.
    """
    if osname is None:
        osname = os.name
    if platform is None:
        platform = sys.platform
    for pattern, compiler in _default_compilers:
        if re.match(pattern, platform) is not None or \
           re.match(pattern, osname) is not None:
            return compiler
    # Default to Unix compiler
    return 'unix'

# Map compiler types to (module_name, class_name) pairs -- ie. where to
# find the code that implements an interface to this compiler.  (The module
# is assumed to be in the 'distutils' package.)
compiler_class = { 'unix':    ('unixccompiler', 'UnixCCompiler',
                               "standard UNIX-style compiler"),
                   'msvc':    ('_msvccompiler', 'MSVCCompiler',
                               "Microsoft Visual C++"),
                   'cygwin':  ('cygwinccompiler', 'CygwinCCompiler',
                               "Cygwin port of GNU C Compiler for Win32"),
                   'mingw32': ('cygwinccompiler', 'Mingw32CCompiler',
                               "Mingw32 port of GNU C Compiler for Win32"),
                   'bcpp':    ('bcppcompiler', 'BCPPCompiler',
                               "Borland C++ Compiler"),
                 }


def new_compiler(plat=None, compiler=None, verbose=0, dry_run=0, force=0):
    """Generate an instance of some CCompiler subclass for the supplied
    platform/compiler combination.  'plat' defaults to 'os.name'
    (eg. 'posix', 'nt'), and 'compiler' defaults to the default compiler
    for that platform.  Currently only 'posix' and 'nt' are supported, and
    the default compilers are "traditional Unix interface" (UnixCCompiler
    class) and Visual C++ (MSVCCompiler class).  Note that it's perfectly
    possible to ask for a Unix compiler object under Windows, and a
    Microsoft compiler object under Unix -- if you supply a value for
    'compiler', 'plat' is ignored.
    """
    if plat is None:
        plat = os.name

    try:
        if compiler is None:
            compiler = get_default_compiler(plat)

        (module_name, class_name, long_description) = compiler_class[compiler]
    except KeyError:
        msg = "don't know how to compile C/C++ code on platform '%s'" % plat
        if compiler is not None:
            msg = msg + " with '%s' compiler" % compiler
        raise DistutilsPlatformError(msg)

    try:
        module_name = "distutils." + module_name
        __import__ (module_name)
        module = sys.modules[module_name]
        klass = vars(module)[class_name]
    except ImportError:
        raise
        raise DistutilsModuleError(
              "can't compile C/C++ code: unable to load module '%s'" % \
              module_name)
    except KeyError:
        raise DistutilsModuleError(
               "can't compile C/C++ code: unable to find class '%s' "
               "in module '%s'" % (class_name, module_name))

    # XXX The None is necessary to preserve backwards compatibility
    # with classes that expect verbose to be the first positional
    # argument.
    return klass(None, dry_run, force)


def gen_preprocess_options(macros, include_dirs):
    """Generate C pre-processor options (-D, -U, -I) as used by at least
    two types of compilers: the typical Unix compiler and Visual C++.
    'macros' is the usual thing, a list of 1- or 2-tuples, where (name,)
    means undefine (-U) macro 'name', and (name,value) means define (-D)
    macro 'name' to 'value'.  'include_dirs' is just a list of directory
    names to be added to the header file search path (-I).  Returns a list
    of command-line options suitable for either Unix compilers or Visual
    C++.
    """
    # XXX it would be nice (mainly aesthetic, and so we don't generate
    # stupid-looking command lines) to go over 'macros' and eliminate
    # redundant definitions/undefinitions (ie. ensure that only the
    # latest mention of a particular macro winds up on the command
    # line).  I don't think it's essential, though, since most (all?)
    # Unix C compilers only pay attention to the latest -D or -U
    # mention of a macro on their command line.  Similar situation for
    # 'include_dirs'.  I'm punting on both for now.  Anyways, weeding out
    # redundancies like this should probably be the province of
    # CCompiler, since the data structures used are inherited from it
    # and therefore common to all CCompiler classes.
    pp_opts = []
    for macro in macros:
        if not (isinstance(macro, tuple) and 1 <= len(macro) <= 2):
            raise TypeError(
                  "bad macro definition '%s': "
                  "each element of 'macros' list must be a 1- or 2-tuple"
                  % macro)

        if len(macro) == 1:        # undefine this macro
            pp_opts.append("-U%s" % macro[0])
        elif len(macro) == 2:
            if macro[1] is None:    # define with no explicit value
                pp_opts.append("-D%s" % macro[0])
            else:
                # XXX *don't* need to be clever about quoting the
                # macro value here, because we're going to avoid the
                # shell at all costs when we spawn the command!
                pp_opts.append("-D%s=%s" % macro)

    for dir in include_dirs:
        pp_opts.append("-I%s" % dir)
    return pp_opts


================================================
File: /Tools/c-analyzer/distutils/cygwinccompiler.py
================================================
"""distutils.cygwinccompiler

Provides the CygwinCCompiler class, a subclass of UnixCCompiler that
handles the Cygwin port of the GNU C compiler to Windows.  It also contains
the Mingw32CCompiler class which handles the mingw32 port of GCC (same as
cygwin in no-cygwin mode).
"""

# problems:
#
# * if you use a msvc compiled python version (1.5.2)
#   1. you have to insert a __GNUC__ section in its config.h
#   2. you have to generate an import library for its dll
#      - create a def-file for python??.dll
#      - create an import library using
#             dlltool --dllname python15.dll --def python15.def \
#                       --output-lib libpython15.a
#
#   see also http://starship.python.net/crew/kernr/mingw32/Notes.html
#
# * We put export_symbols in a def-file, and don't use
#   --export-all-symbols because it doesn't worked reliable in some
#   tested configurations. And because other windows compilers also
#   need their symbols specified this no serious problem.
#
# tested configurations:
#
# * cygwin gcc 2.91.57/ld 2.9.4/dllwrap 0.2.4 works
#   (after patching python's config.h and for C++ some other include files)
#   see also http://starship.python.net/crew/kernr/mingw32/Notes.html
# * mingw32 gcc 2.95.2/ld 2.9.4/dllwrap 0.2.4 works
#   (ld doesn't support -shared, so we use dllwrap)
# * cygwin gcc 2.95.2/ld 2.10.90/dllwrap 2.10.90 works now
#   - its dllwrap doesn't work, there is a bug in binutils 2.10.90
#     see also http://sources.redhat.com/ml/cygwin/2000-06/msg01274.html
#   - using gcc -mdll instead dllwrap doesn't work without -static because
#     it tries to link against dlls instead their import libraries. (If
#     it finds the dll first.)
#     By specifying -static we force ld to link against the import libraries,
#     this is windows standard and there are normally not the necessary symbols
#     in the dlls.
#   *** only the version of June 2000 shows these problems
# * cygwin gcc 3.2/ld 2.13.90 works
#   (ld supports -shared)
# * mingw gcc 3.2/ld 2.13 works
#   (ld supports -shared)

import sys
from subprocess import Popen, PIPE, check_output
import re

from distutils.unixccompiler import UnixCCompiler
from distutils.errors import CCompilerError
from distutils.version import LooseVersion
from distutils.spawn import find_executable

def get_msvcr():
    """Include the appropriate MSVC runtime library if Python was built
    with MSVC 7.0 or later.
    """
    msc_pos = sys.version.find('MSC v.')
    if msc_pos != -1:
        msc_ver = sys.version[msc_pos+6:msc_pos+10]
        if msc_ver == '1300':
            # MSVC 7.0
            return ['msvcr70']
        elif msc_ver == '1310':
            # MSVC 7.1
            return ['msvcr71']
        elif msc_ver == '1400':
            # VS2005 / MSVC 8.0
            return ['msvcr80']
        elif msc_ver == '1500':
            # VS2008 / MSVC 9.0
            return ['msvcr90']
        elif msc_ver == '1600':
            # VS2010 / MSVC 10.0
            return ['msvcr100']
        else:
            raise ValueError("Unknown MS Compiler version %s " % msc_ver)


class CygwinCCompiler(UnixCCompiler):
    """ Handles the Cygwin port of the GNU C compiler to Windows.
    """
    compiler_type = 'cygwin'
    obj_extension = ".o"
    static_lib_extension = ".a"
    shared_lib_extension = ".dll"
    static_lib_format = "lib%s%s"
    shared_lib_format = "%s%s"
    exe_extension = ".exe"

    def __init__(self, verbose=0, dry_run=0, force=0):

        UnixCCompiler.__init__(self, verbose, dry_run, force)

        status, details = check_config_h()
        self.debug_print("Python's GCC status: %s (details: %s)" %
                         (status, details))
        if status is not CONFIG_H_OK:
            self.warn(
                "Python's pyconfig.h doesn't seem to support your compiler. "
                "Reason: %s. "
                "Compiling may fail because of undefined preprocessor macros."
                % details)

        self.gcc_version, self.ld_version, self.dllwrap_version = \
            get_versions()
        self.debug_print(self.compiler_type + ": gcc %s, ld %s, dllwrap %s\n" %
                         (self.gcc_version,
                          self.ld_version,
                          self.dllwrap_version) )

        # ld_version >= "2.10.90" and < "2.13" should also be able to use
        # gcc -mdll instead of dllwrap
        # Older dllwraps had own version numbers, newer ones use the
        # same as the rest of binutils ( also ld )
        # dllwrap 2.10.90 is buggy
        if self.ld_version >= "2.10.90":
            self.linker_dll = "gcc"
        else:
            self.linker_dll = "dllwrap"

        # ld_version >= "2.13" support -shared so use it instead of
        # -mdll -static
        if self.ld_version >= "2.13":
            shared_option = "-shared"
        else:
            shared_option = "-mdll -static"

        # Hard-code GCC because that's what this is all about.
        # XXX optimization, warnings etc. should be customizable.
        self.set_executables(compiler='gcc -mcygwin -O -Wall',
                             compiler_so='gcc -mcygwin -mdll -O -Wall',
                             compiler_cxx='g++ -mcygwin -O -Wall',
                             linker_exe='gcc -mcygwin',
                             linker_so=('%s -mcygwin %s' %
                                        (self.linker_dll, shared_option)))

        # cygwin and mingw32 need different sets of libraries
        if self.gcc_version == "2.91.57":
            # cygwin shouldn't need msvcrt, but without the dlls will crash
            # (gcc version 2.91.57) -- perhaps something about initialization
            self.dll_libraries=["msvcrt"]
            self.warn(
                "Consider upgrading to a newer version of gcc")
        else:
            # Include the appropriate MSVC runtime library if Python was built
            # with MSVC 7.0 or later.
            self.dll_libraries = get_msvcr()


# the same as cygwin plus some additional parameters
class Mingw32CCompiler(CygwinCCompiler):
    """ Handles the Mingw32 port of the GNU C compiler to Windows.
    """
    compiler_type = 'mingw32'

    def __init__(self, verbose=0, dry_run=0, force=0):

        CygwinCCompiler.__init__ (self, verbose, dry_run, force)

        # ld_version >= "2.13" support -shared so use it instead of
        # -mdll -static
        if self.ld_version >= "2.13":
            shared_option = "-shared"
        else:
            shared_option = "-mdll -static"

        # A real mingw32 doesn't need to specify a different entry point,
        # but cygwin 2.91.57 in no-cygwin-mode needs it.
        if self.gcc_version <= "2.91.57":
            entry_point = '--entry _DllMain@12'
        else:
            entry_point = ''

        if is_cygwingcc():
            raise CCompilerError(
                'Cygwin gcc cannot be used with --compiler=mingw32')

        self.set_executables(compiler='gcc -O -Wall',
                             compiler_so='gcc -mdll -O -Wall',
                             compiler_cxx='g++ -O -Wall',
                             linker_exe='gcc',
                             linker_so='%s %s %s'
                                        % (self.linker_dll, shared_option,
                                           entry_point))
        # Maybe we should also append -mthreads, but then the finished
        # dlls need another dll (mingwm10.dll see Mingw32 docs)
        # (-mthreads: Support thread-safe exception handling on `Mingw32')

        # no additional libraries needed
        self.dll_libraries=[]

        # Include the appropriate MSVC runtime library if Python was built
        # with MSVC 7.0 or later.
        self.dll_libraries = get_msvcr()

# Because these compilers aren't configured in Python's pyconfig.h file by
# default, we should at least warn the user if he is using an unmodified
# version.

CONFIG_H_OK = "ok"
CONFIG_H_NOTOK = "not ok"
CONFIG_H_UNCERTAIN = "uncertain"

def check_config_h():
    """Check if the current Python installation appears amenable to building
    extensions with GCC.

    Returns a tuple (status, details), where 'status' is one of the following
    constants:

    - CONFIG_H_OK: all is well, go ahead and compile
    - CONFIG_H_NOTOK: doesn't look good
    - CONFIG_H_UNCERTAIN: not sure -- unable to read pyconfig.h

    'details' is a human-readable string explaining the situation.

    Note there are two ways to conclude "OK": either 'sys.version' contains
    the string "GCC" (implying that this Python was built with GCC), or the
    installed "pyconfig.h" contains the string "__GNUC__".
    """

    # XXX since this function also checks sys.version, it's not strictly a
    # "pyconfig.h" check -- should probably be renamed...

    import sysconfig

    # if sys.version contains GCC then python was compiled with GCC, and the
    # pyconfig.h file should be OK
    if "GCC" in sys.version:
        return CONFIG_H_OK, "sys.version mentions 'GCC'"

    # let's see if __GNUC__ is mentioned in python.h
    fn = sysconfig.get_config_h_filename()
    try:
        config_h = open(fn)
        try:
            if "__GNUC__" in config_h.read():
                return CONFIG_H_OK, "'%s' mentions '__GNUC__'" % fn
            else:
                return CONFIG_H_NOTOK, "'%s' does not mention '__GNUC__'" % fn
        finally:
            config_h.close()
    except OSError as exc:
        return (CONFIG_H_UNCERTAIN,
                "couldn't read '%s': %s" % (fn, exc.strerror))

RE_VERSION = re.compile(br'(\d+\.\d+(\.\d+)*)')

def _find_exe_version(cmd):
    """Find the version of an executable by running `cmd` in the shell.

    If the command is not found, or the output does not match
    `RE_VERSION`, returns None.
    """
    executable = cmd.split()[0]
    if find_executable(executable) is None:
        return None
    out = Popen(cmd, shell=True, stdout=PIPE).stdout
    try:
        out_string = out.read()
    finally:
        out.close()
    result = RE_VERSION.search(out_string)
    if result is None:
        return None
    # LooseVersion works with strings
    # so we need to decode our bytes
    return LooseVersion(result.group(1).decode())

def get_versions():
    """ Try to find out the versions of gcc, ld and dllwrap.

    If not possible it returns None for it.
    """
    commands = ['gcc -dumpversion', 'ld -v', 'dllwrap --version']
    return tuple([_find_exe_version(cmd) for cmd in commands])

def is_cygwingcc():
    '''Try to determine if the gcc that would be used is from cygwin.'''
    out_string = check_output(['gcc', '-dumpmachine'])
    return out_string.strip().endswith(b'cygwin')


================================================
File: /Tools/c-analyzer/distutils/debug.py
================================================
import os

# If DISTUTILS_DEBUG is anything other than the empty string, we run in
# debug mode.
DEBUG = os.environ.get('DISTUTILS_DEBUG')


================================================
File: /Tools/c-analyzer/distutils/dep_util.py
================================================
"""distutils.dep_util

Utility functions for simple, timestamp-based dependency of files
and groups of files; also, function based entirely on such
timestamp dependency analysis."""

import os
from distutils.errors import DistutilsFileError


def newer (source, target):
    """Return true if 'source' exists and is more recently modified than
    'target', or if 'source' exists and 'target' doesn't.  Return false if
    both exist and 'target' is the same age or younger than 'source'.
    Raise DistutilsFileError if 'source' does not exist.
    """
    if not os.path.exists(source):
        raise DistutilsFileError("file '%s' does not exist" %
                                 os.path.abspath(source))
    if not os.path.exists(target):
        return 1

    from stat import ST_MTIME
    mtime1 = os.stat(source)[ST_MTIME]
    mtime2 = os.stat(target)[ST_MTIME]

    return mtime1 > mtime2

# newer ()


================================================
File: /Tools/c-analyzer/distutils/errors.py
================================================
"""distutils.errors

Provides exceptions used by the Distutils modules.  Note that Distutils
modules may raise standard exceptions; in particular, SystemExit is
usually raised for errors that are obviously the end-user's fault
(eg. bad command-line arguments).

This module is safe to use in "from ... import *" mode; it only exports
symbols whose names start with "Distutils" and end with "Error"."""

class DistutilsError (Exception):
    """The root of all Distutils evil."""
    pass

class DistutilsModuleError (DistutilsError):
    """Unable to load an expected module, or to find an expected class
    within some module (in particular, command modules and classes)."""
    pass

class DistutilsFileError (DistutilsError):
    """Any problems in the filesystem: expected file not found, etc.
    Typically this is for problems that we detect before OSError
    could be raised."""
    pass

class DistutilsPlatformError (DistutilsError):
    """We don't know how to do something on the current platform (but
    we do know how to do it on some platform) -- eg. trying to compile
    C files on a platform not supported by a CCompiler subclass."""
    pass

class DistutilsExecError (DistutilsError):
    """Any problems executing an external program (such as the C
    compiler, when compiling C files)."""
    pass

# Exception classes used by the CCompiler implementation classes
class CCompilerError (Exception):
    """Some compile/link operation failed."""

class PreprocessError (CCompilerError):
    """Failure to preprocess one or more C/C++ files."""

class CompileError (CCompilerError):
    """Failure to compile one or more C/C++ source files."""

class UnknownFileError (CCompilerError):
    """Attempt to process an unknown file type."""


================================================
File: /Tools/c-analyzer/distutils/log.py
================================================
"""A simple log mechanism styled after PEP 282."""

# The class here is styled after PEP 282 so that it could later be
# replaced with a standard Python logging implementation.

DEBUG = 1
INFO = 2
WARN = 3
ERROR = 4
FATAL = 5

import sys

class Log:

    def __init__(self, threshold=WARN):
        self.threshold = threshold

    def _log(self, level, msg, args):
        if level not in (DEBUG, INFO, WARN, ERROR, FATAL):
            raise ValueError('%s wrong log level' % str(level))

        if level >= self.threshold:
            if args:
                msg = msg % args
            if level in (WARN, ERROR, FATAL):
                stream = sys.stderr
            else:
                stream = sys.stdout
            try:
                stream.write('%s\n' % msg)
            except UnicodeEncodeError:
                # emulate backslashreplace error handler
                encoding = stream.encoding
                msg = msg.encode(encoding, "backslashreplace").decode(encoding)
                stream.write('%s\n' % msg)
            stream.flush()

    def log(self, level, msg, *args):
        self._log(level, msg, args)

    def debug(self, msg, *args):
        self._log(DEBUG, msg, args)

    def info(self, msg, *args):
        self._log(INFO, msg, args)

    def warn(self, msg, *args):
        self._log(WARN, msg, args)

    def error(self, msg, *args):
        self._log(ERROR, msg, args)

    def fatal(self, msg, *args):
        self._log(FATAL, msg, args)

_global_log = Log()
log = _global_log.log
debug = _global_log.debug
info = _global_log.info
warn = _global_log.warn
error = _global_log.error
fatal = _global_log.fatal


================================================
File: /Tools/c-analyzer/distutils/msvc9compiler.py
================================================
"""distutils.msvc9compiler

Contains MSVCCompiler, an implementation of the abstract CCompiler class
for the Microsoft Visual Studio 2008.

The module is compatible with VS 2005 and VS 2008. You can find legacy support
for older versions of VS in distutils.msvccompiler.
"""

# Written by Perry Stoll
# hacked by Robin Becker and Thomas Heller to do a better job of
#   finding DevStudio (through the registry)
# ported to VS2005 and VS 2008 by Christian Heimes

import os
import subprocess
import sys
import re

from distutils.errors import DistutilsPlatformError
from distutils.ccompiler import CCompiler
from distutils import log

import winreg

RegOpenKeyEx = winreg.OpenKeyEx
RegEnumKey = winreg.EnumKey
RegEnumValue = winreg.EnumValue
RegError = winreg.error

HKEYS = (winreg.HKEY_USERS,
         winreg.HKEY_CURRENT_USER,
         winreg.HKEY_LOCAL_MACHINE,
         winreg.HKEY_CLASSES_ROOT)

NATIVE_WIN64 = (sys.platform == 'win32' and sys.maxsize > 2**32)
if NATIVE_WIN64:
    # Visual C++ is a 32-bit application, so we need to look in
    # the corresponding registry branch, if we're running a
    # 64-bit Python on Win64
    VS_BASE = r"Software\Wow6432Node\Microsoft\VisualStudio\%0.1f"
    WINSDK_BASE = r"Software\Wow6432Node\Microsoft\Microsoft SDKs\Windows"
    NET_BASE = r"Software\Wow6432Node\Microsoft\.NETFramework"
else:
    VS_BASE = r"Software\Microsoft\VisualStudio\%0.1f"
    WINSDK_BASE = r"Software\Microsoft\Microsoft SDKs\Windows"
    NET_BASE = r"Software\Microsoft\.NETFramework"

# A map keyed by get_platform() return values to values accepted by
# 'vcvarsall.bat'.  Note a cross-compile may combine these (eg, 'x86_amd64' is
# the param to cross-compile on x86 targeting amd64.)
PLAT_TO_VCVARS = {
    'win32' : 'x86',
    'win-amd64' : 'amd64',
}

class Reg:
    """Helper class to read values from the registry
    """

    def get_value(cls, path, key):
        for base in HKEYS:
            d = cls.read_values(base, path)
            if d and key in d:
                return d[key]
        raise KeyError(key)
    get_value = classmethod(get_value)

    def read_keys(cls, base, key):
        """Return list of registry keys."""
        try:
            handle = RegOpenKeyEx(base, key)
        except RegError:
            return None
        L = []
        i = 0
        while True:
            try:
                k = RegEnumKey(handle, i)
            except RegError:
                break
            L.append(k)
            i += 1
        return L
    read_keys = classmethod(read_keys)

    def read_values(cls, base, key):
        """Return dict of registry keys and values.

        All names are converted to lowercase.
        """
        try:
            handle = RegOpenKeyEx(base, key)
        except RegError:
            return None
        d = {}
        i = 0
        while True:
            try:
                name, value, type = RegEnumValue(handle, i)
            except RegError:
                break
            name = name.lower()
            d[cls.convert_mbcs(name)] = cls.convert_mbcs(value)
            i += 1
        return d
    read_values = classmethod(read_values)

    def convert_mbcs(s):
        dec = getattr(s, "decode", None)
        if dec is not None:
            try:
                s = dec("mbcs")
            except UnicodeError:
                pass
        return s
    convert_mbcs = staticmethod(convert_mbcs)

class MacroExpander:

    def __init__(self, version):
        self.macros = {}
        self.vsbase = VS_BASE % version
        self.load_macros(version)

    def set_macro(self, macro, path, key):
        self.macros["$(%s)" % macro] = Reg.get_value(path, key)

    def load_macros(self, version):
        self.set_macro("VCInstallDir", self.vsbase + r"\Setup\VC", "productdir")
        self.set_macro("VSInstallDir", self.vsbase + r"\Setup\VS", "productdir")
        self.set_macro("FrameworkDir", NET_BASE, "installroot")
        try:
            if version >= 8.0:
                self.set_macro("FrameworkSDKDir", NET_BASE,
                               "sdkinstallrootv2.0")
            else:
                raise KeyError("sdkinstallrootv2.0")
        except KeyError:
            raise DistutilsPlatformError(
            """Python was built with Visual Studio 2008;
extensions must be built with a compiler than can generate compatible binaries.
Visual Studio 2008 was not found on this system. If you have Cygwin installed,
you can try compiling with MingW32, by passing "-c mingw32" to setup.py.""")

        if version >= 9.0:
            self.set_macro("FrameworkVersion", self.vsbase, "clr version")
            self.set_macro("WindowsSdkDir", WINSDK_BASE, "currentinstallfolder")
        else:
            p = r"Software\Microsoft\NET Framework Setup\Product"
            for base in HKEYS:
                try:
                    h = RegOpenKeyEx(base, p)
                except RegError:
                    continue
                key = RegEnumKey(h, 0)
                d = Reg.get_value(base, r"%s\%s" % (p, key))
                self.macros["$(FrameworkVersion)"] = d["version"]

    def sub(self, s):
        for k, v in self.macros.items():
            s = s.replace(k, v)
        return s

def get_build_version():
    """Return the version of MSVC that was used to build Python.

    For Python 2.3 and up, the version number is included in
    sys.version.  For earlier versions, assume the compiler is MSVC 6.
    """
    prefix = "MSC v."
    i = sys.version.find(prefix)
    if i == -1:
        return 6
    i = i + len(prefix)
    s, rest = sys.version[i:].split(" ", 1)
    majorVersion = int(s[:-2]) - 6
    if majorVersion >= 13:
        # v13 was skipped and should be v14
        majorVersion += 1
    minorVersion = int(s[2:3]) / 10.0
    # I don't think paths are affected by minor version in version 6
    if majorVersion == 6:
        minorVersion = 0
    if majorVersion >= 6:
        return majorVersion + minorVersion
    # else we don't know what version of the compiler this is
    return None

def normalize_and_reduce_paths(paths):
    """Return a list of normalized paths with duplicates removed.

    The current order of paths is maintained.
    """
    # Paths are normalized so things like:  /a and /a/ aren't both preserved.
    reduced_paths = []
    for p in paths:
        np = os.path.normpath(p)
        # XXX(nnorwitz): O(n**2), if reduced_paths gets long perhaps use a set.
        if np not in reduced_paths:
            reduced_paths.append(np)
    return reduced_paths

def removeDuplicates(variable):
    """Remove duplicate values of an environment variable.
    """
    oldList = variable.split(os.pathsep)
    newList = []
    for i in oldList:
        if i not in newList:
            newList.append(i)
    newVariable = os.pathsep.join(newList)
    return newVariable

def find_vcvarsall(version):
    """Find the vcvarsall.bat file

    At first it tries to find the productdir of VS 2008 in the registry. If
    that fails it falls back to the VS90COMNTOOLS env var.
    """
    vsbase = VS_BASE % version
    try:
        productdir = Reg.get_value(r"%s\Setup\VC" % vsbase,
                                   "productdir")
    except KeyError:
        log.debug("Unable to find productdir in registry")
        productdir = None

    if not productdir or not os.path.isdir(productdir):
        toolskey = "VS%0.f0COMNTOOLS" % version
        toolsdir = os.environ.get(toolskey, None)

        if toolsdir and os.path.isdir(toolsdir):
            productdir = os.path.join(toolsdir, os.pardir, os.pardir, "VC")
            productdir = os.path.abspath(productdir)
            if not os.path.isdir(productdir):
                log.debug("%s is not a valid directory" % productdir)
                return None
        else:
            log.debug("Env var %s is not set or invalid" % toolskey)
    if not productdir:
        log.debug("No productdir found")
        return None
    vcvarsall = os.path.join(productdir, "vcvarsall.bat")
    if os.path.isfile(vcvarsall):
        return vcvarsall
    log.debug("Unable to find vcvarsall.bat")
    return None

def query_vcvarsall(version, arch="x86"):
    """Launch vcvarsall.bat and read the settings from its environment
    """
    vcvarsall = find_vcvarsall(version)
    interesting = {"include", "lib", "libpath", "path"}
    result = {}

    if vcvarsall is None:
        raise DistutilsPlatformError("Unable to find vcvarsall.bat")
    log.debug("Calling 'vcvarsall.bat %s' (version=%s)", arch, version)
    popen = subprocess.Popen('"%s" %s & set' % (vcvarsall, arch),
                             stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE)
    try:
        stdout, stderr = popen.communicate()
        if popen.wait() != 0:
            raise DistutilsPlatformError(stderr.decode("mbcs"))

        stdout = stdout.decode("mbcs")
        for line in stdout.split("\n"):
            line = Reg.convert_mbcs(line)
            if '=' not in line:
                continue
            line = line.strip()
            key, value = line.split('=', 1)
            key = key.lower()
            if key in interesting:
                if value.endswith(os.pathsep):
                    value = value[:-1]
                result[key] = removeDuplicates(value)

    finally:
        popen.stdout.close()
        popen.stderr.close()

    if len(result) != len(interesting):
        raise ValueError(str(list(result.keys())))

    return result

# More globals
VERSION = get_build_version()
if VERSION < 8.0:
    raise DistutilsPlatformError("VC %0.1f is not supported by this module" % VERSION)
# MACROS = MacroExpander(VERSION)

class MSVCCompiler(CCompiler) :
    """Concrete class that implements an interface to Microsoft Visual C++,
       as defined by the CCompiler abstract class."""

    compiler_type = 'msvc'

    # Just set this so CCompiler's constructor doesn't barf.  We currently
    # don't use the 'set_executables()' bureaucracy provided by CCompiler,
    # as it really isn't necessary for this sort of single-compiler class.
    # Would be nice to have a consistent interface with UnixCCompiler,
    # though, so it's worth thinking about.
    executables = {}

    # Private class data (need to distinguish C from C++ source for compiler)
    _c_extensions = ['.c']
    _cpp_extensions = ['.cc', '.cpp', '.cxx']
    _rc_extensions = ['.rc']
    _mc_extensions = ['.mc']

    # Needed for the filename generation methods provided by the
    # base class, CCompiler.
    src_extensions = (_c_extensions + _cpp_extensions +
                      _rc_extensions + _mc_extensions)
    res_extension = '.res'
    obj_extension = '.obj'
    static_lib_extension = '.lib'
    shared_lib_extension = '.dll'
    static_lib_format = shared_lib_format = '%s%s'
    exe_extension = '.exe'

    def __init__(self, verbose=0, dry_run=0, force=0):
        CCompiler.__init__ (self, verbose, dry_run, force)
        self.__version = VERSION
        self.__root = r"Software\Microsoft\VisualStudio"
        # self.__macros = MACROS
        self.__paths = []
        # target platform (.plat_name is consistent with 'bdist')
        self.plat_name = None
        self.__arch = None # deprecated name
        self.initialized = False

    # -- Worker methods ------------------------------------------------

    def manifest_setup_ldargs(self, output_filename, build_temp, ld_args):
        # If we need a manifest at all, an embedded manifest is recommended.
        # See MSDN article titled
        # "How to: Embed a Manifest Inside a C/C++ Application"
        # (currently at http://msdn2.microsoft.com/en-us/library/ms235591(VS.80).aspx)
        # Ask the linker to generate the manifest in the temp dir, so
        # we can check it, and possibly embed it, later.
        temp_manifest = os.path.join(
                build_temp,
                os.path.basename(output_filename) + ".manifest")
        ld_args.append('/MANIFESTFILE:' + temp_manifest)

    def manifest_get_embed_info(self, target_desc, ld_args):
        # If a manifest should be embedded, return a tuple of
        # (manifest_filename, resource_id).  Returns None if no manifest
        # should be embedded.  See http://bugs.python.org/issue7833 for why
        # we want to avoid any manifest for extension modules if we can.
        for arg in ld_args:
            if arg.startswith("/MANIFESTFILE:"):
                temp_manifest = arg.split(":", 1)[1]
                break
        else:
            # no /MANIFESTFILE so nothing to do.
            return None
        if target_desc == CCompiler.EXECUTABLE:
            # by default, executables always get the manifest with the
            # CRT referenced.
            mfid = 1
        else:
            # Extension modules try and avoid any manifest if possible.
            mfid = 2
            temp_manifest = self._remove_visual_c_ref(temp_manifest)
        if temp_manifest is None:
            return None
        return temp_manifest, mfid

    def _remove_visual_c_ref(self, manifest_file):
        try:
            # Remove references to the Visual C runtime, so they will
            # fall through to the Visual C dependency of Python.exe.
            # This way, when installed for a restricted user (e.g.
            # runtimes are not in WinSxS folder, but in Python's own
            # folder), the runtimes do not need to be in every folder
            # with .pyd's.
            # Returns either the filename of the modified manifest or
            # None if no manifest should be embedded.
            manifest_f = open(manifest_file)
            try:
                manifest_buf = manifest_f.read()
            finally:
                manifest_f.close()
            pattern = re.compile(
                r"""<assemblyIdentity.*?name=("|')Microsoft\."""\
                r"""VC\d{2}\.CRT("|').*?(/>|</assemblyIdentity>)""",
                re.DOTALL)
            manifest_buf = re.sub(pattern, "", manifest_buf)
            pattern = r"<dependentAssembly>\s*</dependentAssembly>"
            manifest_buf = re.sub(pattern, "", manifest_buf)
            # Now see if any other assemblies are referenced - if not, we
            # don't want a manifest embedded.
            pattern = re.compile(
                r"""<assemblyIdentity.*?name=(?:"|')(.+?)(?:"|')"""
                r""".*?(?:/>|</assemblyIdentity>)""", re.DOTALL)
            if re.search(pattern, manifest_buf) is None:
                return None

            manifest_f = open(manifest_file, 'w')
            try:
                manifest_f.write(manifest_buf)
                return manifest_file
            finally:
                manifest_f.close()
        except OSError:
            pass

    # -- Miscellaneous methods -----------------------------------------

    # Helper methods for using the MSVC registry settings

    def find_exe(self, exe):
        """Return path to an MSVC executable program.

        Tries to find the program in several places: first, one of the
        MSVC program search paths from the registry; next, the directories
        in the PATH environment variable.  If any of those work, return an
        absolute path that is known to exist.  If none of them work, just
        return the original program name, 'exe'.
        """
        for p in self.__paths:
            fn = os.path.join(os.path.abspath(p), exe)
            if os.path.isfile(fn):
                return fn

        # didn't find it; try existing path
        for p in os.environ['Path'].split(';'):
            fn = os.path.join(os.path.abspath(p),exe)
            if os.path.isfile(fn):
                return fn

        return exe


================================================
File: /Tools/c-analyzer/distutils/msvccompiler.py
================================================
"""distutils.msvccompiler

Contains MSVCCompiler, an implementation of the abstract CCompiler class
for the Microsoft Visual Studio.
"""

# Written by Perry Stoll
# hacked by Robin Becker and Thomas Heller to do a better job of
#   finding DevStudio (through the registry)

import sys, os
from distutils.errors import DistutilsPlatformError
from distutils.ccompiler import CCompiler
from distutils import log

_can_read_reg = False
try:
    import winreg

    _can_read_reg = True
    hkey_mod = winreg

    RegOpenKeyEx = winreg.OpenKeyEx
    RegEnumKey = winreg.EnumKey
    RegEnumValue = winreg.EnumValue
    RegError = winreg.error

except ImportError:
    try:
        import win32api
        import win32con
        _can_read_reg = True
        hkey_mod = win32con

        RegOpenKeyEx = win32api.RegOpenKeyEx
        RegEnumKey = win32api.RegEnumKey
        RegEnumValue = win32api.RegEnumValue
        RegError = win32api.error
    except ImportError:
        log.info("Warning: Can't read registry to find the "
                 "necessary compiler setting\n"
                 "Make sure that Python modules winreg, "
                 "win32api or win32con are installed.")

if _can_read_reg:
    HKEYS = (hkey_mod.HKEY_USERS,
             hkey_mod.HKEY_CURRENT_USER,
             hkey_mod.HKEY_LOCAL_MACHINE,
             hkey_mod.HKEY_CLASSES_ROOT)

def read_keys(base, key):
    """Return list of registry keys."""
    try:
        handle = RegOpenKeyEx(base, key)
    except RegError:
        return None
    L = []
    i = 0
    while True:
        try:
            k = RegEnumKey(handle, i)
        except RegError:
            break
        L.append(k)
        i += 1
    return L

def read_values(base, key):
    """Return dict of registry keys and values.

    All names are converted to lowercase.
    """
    try:
        handle = RegOpenKeyEx(base, key)
    except RegError:
        return None
    d = {}
    i = 0
    while True:
        try:
            name, value, type = RegEnumValue(handle, i)
        except RegError:
            break
        name = name.lower()
        d[convert_mbcs(name)] = convert_mbcs(value)
        i += 1
    return d

def convert_mbcs(s):
    dec = getattr(s, "decode", None)
    if dec is not None:
        try:
            s = dec("mbcs")
        except UnicodeError:
            pass
    return s

class MacroExpander:
    def __init__(self, version):
        self.macros = {}
        self.load_macros(version)

    def set_macro(self, macro, path, key):
        for base in HKEYS:
            d = read_values(base, path)
            if d:
                self.macros["$(%s)" % macro] = d[key]
                break

    def load_macros(self, version):
        vsbase = r"Software\Microsoft\VisualStudio\%0.1f" % version
        self.set_macro("VCInstallDir", vsbase + r"\Setup\VC", "productdir")
        self.set_macro("VSInstallDir", vsbase + r"\Setup\VS", "productdir")
        net = r"Software\Microsoft\.NETFramework"
        self.set_macro("FrameworkDir", net, "installroot")
        try:
            if version > 7.0:
                self.set_macro("FrameworkSDKDir", net, "sdkinstallrootv1.1")
            else:
                self.set_macro("FrameworkSDKDir", net, "sdkinstallroot")
        except KeyError as exc: #
            raise DistutilsPlatformError(
            """Python was built with Visual Studio 2003;
extensions must be built with a compiler than can generate compatible binaries.
Visual Studio 2003 was not found on this system. If you have Cygwin installed,
you can try compiling with MingW32, by passing "-c mingw32" to setup.py.""")

        p = r"Software\Microsoft\NET Framework Setup\Product"
        for base in HKEYS:
            try:
                h = RegOpenKeyEx(base, p)
            except RegError:
                continue
            key = RegEnumKey(h, 0)
            d = read_values(base, r"%s\%s" % (p, key))
            self.macros["$(FrameworkVersion)"] = d["version"]

    def sub(self, s):
        for k, v in self.macros.items():
            s = s.replace(k, v)
        return s

def get_build_version():
    """Return the version of MSVC that was used to build Python.

    For Python 2.3 and up, the version number is included in
    sys.version.  For earlier versions, assume the compiler is MSVC 6.
    """
    prefix = "MSC v."
    i = sys.version.find(prefix)
    if i == -1:
        return 6
    i = i + len(prefix)
    s, rest = sys.version[i:].split(" ", 1)
    majorVersion = int(s[:-2]) - 6
    if majorVersion >= 13:
        # v13 was skipped and should be v14
        majorVersion += 1
    minorVersion = int(s[2:3]) / 10.0
    # I don't think paths are affected by minor version in version 6
    if majorVersion == 6:
        minorVersion = 0
    if majorVersion >= 6:
        return majorVersion + minorVersion
    # else we don't know what version of the compiler this is
    return None

def get_build_architecture():
    """Return the processor architecture.

    Possible results are "Intel" or "AMD64".
    """

    prefix = " bit ("
    i = sys.version.find(prefix)
    if i == -1:
        return "Intel"
    j = sys.version.find(")", i)
    return sys.version[i+len(prefix):j]

def normalize_and_reduce_paths(paths):
    """Return a list of normalized paths with duplicates removed.

    The current order of paths is maintained.
    """
    # Paths are normalized so things like:  /a and /a/ aren't both preserved.
    reduced_paths = []
    for p in paths:
        np = os.path.normpath(p)
        # XXX(nnorwitz): O(n**2), if reduced_paths gets long perhaps use a set.
        if np not in reduced_paths:
            reduced_paths.append(np)
    return reduced_paths


class MSVCCompiler(CCompiler) :
    """Concrete class that implements an interface to Microsoft Visual C++,
       as defined by the CCompiler abstract class."""

    compiler_type = 'msvc'

    # Just set this so CCompiler's constructor doesn't barf.  We currently
    # don't use the 'set_executables()' bureaucracy provided by CCompiler,
    # as it really isn't necessary for this sort of single-compiler class.
    # Would be nice to have a consistent interface with UnixCCompiler,
    # though, so it's worth thinking about.
    executables = {}

    # Private class data (need to distinguish C from C++ source for compiler)
    _c_extensions = ['.c']
    _cpp_extensions = ['.cc', '.cpp', '.cxx']
    _rc_extensions = ['.rc']
    _mc_extensions = ['.mc']

    # Needed for the filename generation methods provided by the
    # base class, CCompiler.
    src_extensions = (_c_extensions + _cpp_extensions +
                      _rc_extensions + _mc_extensions)
    res_extension = '.res'
    obj_extension = '.obj'
    static_lib_extension = '.lib'
    shared_lib_extension = '.dll'
    static_lib_format = shared_lib_format = '%s%s'
    exe_extension = '.exe'

    def __init__(self, verbose=0, dry_run=0, force=0):
        CCompiler.__init__ (self, verbose, dry_run, force)
        self.__version = get_build_version()
        self.__arch = get_build_architecture()
        if self.__arch == "Intel":
            # x86
            if self.__version >= 7:
                self.__root = r"Software\Microsoft\VisualStudio"
                self.__macros = MacroExpander(self.__version)
            else:
                self.__root = r"Software\Microsoft\Devstudio"
            self.__product = "Visual Studio version %s" % self.__version
        else:
            # Win64. Assume this was built with the platform SDK
            self.__product = "Microsoft SDK compiler %s" % (self.__version + 6)

        self.initialized = False


    # -- Miscellaneous methods -----------------------------------------

    # Helper methods for using the MSVC registry settings

    def find_exe(self, exe):
        """Return path to an MSVC executable program.

        Tries to find the program in several places: first, one of the
        MSVC program search paths from the registry; next, the directories
        in the PATH environment variable.  If any of those work, return an
        absolute path that is known to exist.  If none of them work, just
        return the original program name, 'exe'.
        """
        for p in self.__paths:
            fn = os.path.join(os.path.abspath(p), exe)
            if os.path.isfile(fn):
                return fn

        # didn't find it; try existing path
        for p in os.environ['Path'].split(';'):
            fn = os.path.join(os.path.abspath(p),exe)
            if os.path.isfile(fn):
                return fn

        return exe

    def get_msvc_paths(self, path, platform='x86'):
        """Get a list of devstudio directories (include, lib or path).

        Return a list of strings.  The list will be empty if unable to
        access the registry or appropriate registry keys not found.
        """
        if not _can_read_reg:
            return []

        path = path + " dirs"
        if self.__version >= 7:
            key = (r"%s\%0.1f\VC\VC_OBJECTS_PLATFORM_INFO\Win32\Directories"
                   % (self.__root, self.__version))
        else:
            key = (r"%s\6.0\Build System\Components\Platforms"
                   r"\Win32 (%s)\Directories" % (self.__root, platform))

        for base in HKEYS:
            d = read_values(base, key)
            if d:
                if self.__version >= 7:
                    return self.__macros.sub(d[path]).split(";")
                else:
                    return d[path].split(";")
        # MSVC 6 seems to create the registry entries we need only when
        # the GUI is run.
        if self.__version == 6:
            for base in HKEYS:
                if read_values(base, r"%s\6.0" % self.__root) is not None:
                    self.warn("It seems you have Visual Studio 6 installed, "
                        "but the expected registry settings are not present.\n"
                        "You must at least run the Visual Studio GUI once "
                        "so that these entries are created.")
                    break
        return []

    def set_path_env_var(self, name):
        """Set environment variable 'name' to an MSVC path type value.

        This is equivalent to a SET command prior to execution of spawned
        commands.
        """

        if name == "lib":
            p = self.get_msvc_paths("library")
        else:
            p = self.get_msvc_paths(name)
        if p:
            os.environ[name] = ';'.join(p)


if get_build_version() >= 8.0:
    log.debug("Importing new compiler from distutils.msvc9compiler")
    OldMSVCCompiler = MSVCCompiler
    from distutils.msvc9compiler import MSVCCompiler
    # get_build_architecture not really relevant now we support cross-compile
    from distutils.msvc9compiler import MacroExpander


================================================
File: /Tools/c-analyzer/distutils/spawn.py
================================================
"""distutils.spawn

Provides the 'spawn()' function, a front-end to various platform-
specific functions for launching another program in a sub-process.
Also provides the 'find_executable()' to search the path for a given
executable name.
"""

import sys
import os
import os.path


def find_executable(executable, path=None):
    """Tries to find 'executable' in the directories listed in 'path'.

    A string listing directories separated by 'os.pathsep'; defaults to
    os.environ['PATH'].  Returns the complete filename or None if not found.
    """
    _, ext = os.path.splitext(executable)
    if (sys.platform == 'win32') and (ext != '.exe'):
        executable = executable + '.exe'

    if os.path.isfile(executable):
        return executable

    if path is None:
        path = os.environ.get('PATH', None)
        if path is None:
            try:
                path = os.confstr("CS_PATH")
            except (AttributeError, ValueError):
                # os.confstr() or CS_PATH is not available
                path = os.defpath
        # bpo-35755: Don't use os.defpath if the PATH environment variable is
        # set to an empty string

    # PATH='' doesn't match, whereas PATH=':' looks in the current directory
    if not path:
        return None

    paths = path.split(os.pathsep)
    for p in paths:
        f = os.path.join(p, executable)
        if os.path.isfile(f):
            # the file exists, we have a shot at spawn working
            return f
    return None


================================================
File: /Tools/c-analyzer/distutils/unixccompiler.py
================================================
"""distutils.unixccompiler

Contains the UnixCCompiler class, a subclass of CCompiler that handles
the "typical" Unix-style command-line C compiler:
  * macros defined with -Dname[=value]
  * macros undefined with -Uname
  * include search directories specified with -Idir
  * libraries specified with -lllib
  * library search directories specified with -Ldir
  * compile handled by 'cc' (or similar) executable with -c option:
    compiles .c to .o
  * link static library handled by 'ar' command (possibly with 'ranlib')
  * link shared library handled by 'cc -shared'
"""

import os, sys

from distutils.dep_util import newer
from distutils.ccompiler import CCompiler, gen_preprocess_options
from distutils.errors import DistutilsExecError, CompileError

# XXX Things not currently handled:
#   * optimization/debug/warning flags; we just use whatever's in Python's
#     Makefile and live with it.  Is this adequate?  If not, we might
#     have to have a bunch of subclasses GNUCCompiler, SGICCompiler,
#     SunCCompiler, and I suspect down that road lies madness.
#   * even if we don't know a warning flag from an optimization flag,
#     we need some way for outsiders to feed preprocessor/compiler/linker
#     flags in to us -- eg. a sysadmin might want to mandate certain flags
#     via a site config file, or a user might want to set something for
#     compiling this module distribution only via the setup.py command
#     line, whatever.  As long as these options come from something on the
#     current system, they can be as system-dependent as they like, and we
#     should just happily stuff them into the preprocessor/compiler/linker
#     options and carry on.


class UnixCCompiler(CCompiler):

    compiler_type = 'unix'

    # These are used by CCompiler in two places: the constructor sets
    # instance attributes 'preprocessor', 'compiler', etc. from them, and
    # 'set_executable()' allows any of these to be set.  The defaults here
    # are pretty generic; they will probably have to be set by an outsider
    # (eg. using information discovered by the sysconfig about building
    # Python extensions).
    executables = {'preprocessor' : None,
                   'compiler'     : ["cc"],
                   'compiler_so'  : ["cc"],
                   'compiler_cxx' : ["cc"],
                   'linker_so'    : ["cc", "-shared"],
                   'linker_exe'   : ["cc"],
                   'archiver'     : ["ar", "-cr"],
                   'ranlib'       : None,
                  }

    if sys.platform[:6] == "darwin":
        executables['ranlib'] = ["ranlib"]

    # Needed for the filename generation methods provided by the base
    # class, CCompiler.  NB. whoever instantiates/uses a particular
    # UnixCCompiler instance should set 'shared_lib_ext' -- we set a
    # reasonable common default here, but it's not necessarily used on all
    # Unices!

    src_extensions = [".c",".C",".cc",".cxx",".cpp",".m"]
    obj_extension = ".o"
    static_lib_extension = ".a"
    shared_lib_extension = ".so"
    dylib_lib_extension = ".dylib"
    xcode_stub_lib_extension = ".tbd"
    static_lib_format = shared_lib_format = dylib_lib_format = "lib%s%s"
    xcode_stub_lib_format = dylib_lib_format
    if sys.platform == "cygwin":
        exe_extension = ".exe"

    def preprocess(self, source, output_file=None, macros=None,
                   include_dirs=None, extra_preargs=None, extra_postargs=None):
        fixed_args = self._fix_compile_args(None, macros, include_dirs)
        ignore, macros, include_dirs = fixed_args
        pp_opts = gen_preprocess_options(macros, include_dirs)
        pp_args = self.preprocessor + pp_opts
        if output_file:
            pp_args.extend(['-o', output_file])
        if extra_preargs:
            pp_args[:0] = extra_preargs
        if extra_postargs:
            pp_args.extend(extra_postargs)
        pp_args.append(source)

        # We need to preprocess: either we're being forced to, or we're
        # generating output to stdout, or there's a target output file and
        # the source file is newer than the target (or the target doesn't
        # exist).
        if self.force or output_file is None or newer(source, output_file):
            if output_file:
                self.mkpath(os.path.dirname(output_file))
            try:
                self.spawn(pp_args)
            except DistutilsExecError as msg:
                raise CompileError(msg)


================================================
File: /Tools/c-analyzer/distutils/util.py
================================================
"""distutils.util

Miscellaneous utility functions -- anything that doesn't fit into
one of the other *util.py modules.
"""

import os
import re
import string
import sys
from distutils.errors import DistutilsPlatformError

def get_host_platform():
    """Return a string that identifies the current platform.  This is used mainly to
    distinguish platform-specific build directories and platform-specific built
    distributions.  Typically includes the OS name and version and the
    architecture (as supplied by 'os.uname()'), although the exact information
    included depends on the OS; eg. on Linux, the kernel version isn't
    particularly important.

    Examples of returned values:
       linux-i586
       linux-alpha (?)
       solaris-2.6-sun4u

    Windows will return one of:
       win-amd64 (64bit Windows on AMD64 (aka x86_64, Intel64, EM64T, etc)
       win32 (all others - specifically, sys.platform is returned)

    For other non-POSIX platforms, currently just returns 'sys.platform'.

    """
    if os.name == 'nt':
        if 'amd64' in sys.version.lower():
            return 'win-amd64'
        if '(arm)' in sys.version.lower():
            return 'win-arm32'
        if '(arm64)' in sys.version.lower():
            return 'win-arm64'
        return sys.platform

    # Set for cross builds explicitly
    if "_PYTHON_HOST_PLATFORM" in os.environ:
        return os.environ["_PYTHON_HOST_PLATFORM"]

    if os.name != "posix" or not hasattr(os, 'uname'):
        # XXX what about the architecture? NT is Intel or Alpha,
        # Mac OS is M68k or PPC, etc.
        return sys.platform

    # Try to distinguish various flavours of Unix

    (osname, host, release, version, machine) = os.uname()

    # Convert the OS name to lowercase, remove '/' characters, and translate
    # spaces (for "Power Macintosh")
    osname = osname.lower().replace('/', '')
    machine = machine.replace(' ', '_')
    machine = machine.replace('/', '-')

    if osname[:5] == "linux":
        # At least on Linux/Intel, 'machine' is the processor --
        # i386, etc.
        # XXX what about Alpha, SPARC, etc?
        return  "%s-%s" % (osname, machine)
    elif osname[:5] == "sunos":
        if release[0] >= "5":           # SunOS 5 == Solaris 2
            osname = "solaris"
            release = "%d.%s" % (int(release[0]) - 3, release[2:])
            # We can't use "platform.architecture()[0]" because a
            # bootstrap problem. We use a dict to get an error
            # if some suspicious happens.
            bitness = {2147483647:"32bit", 9223372036854775807:"64bit"}
            machine += ".%s" % bitness[sys.maxsize]
        # fall through to standard osname-release-machine representation
    elif osname[:3] == "aix":
        from _aix_support import aix_platform
        return aix_platform()
    elif osname[:6] == "cygwin":
        osname = "cygwin"
        rel_re = re.compile (r'[\d.]+', re.ASCII)
        m = rel_re.match(release)
        if m:
            release = m.group()
    elif osname[:6] == "darwin":
        import _osx_support, sysconfig
        osname, release, machine = _osx_support.get_platform_osx(
                                        sysconfig.get_config_vars(),
                                        osname, release, machine)

    return "%s-%s-%s" % (osname, release, machine)

def get_platform():
    if os.name == 'nt':
        TARGET_TO_PLAT = {
            'x86' : 'win32',
            'x64' : 'win-amd64',
            'arm' : 'win-arm32',
        }
        return TARGET_TO_PLAT.get(os.environ.get('VSCMD_ARG_TGT_ARCH')) or get_host_platform()
    else:
        return get_host_platform()


# Needed by 'split_quoted()'
_wordchars_re = _squote_re = _dquote_re = None
def _init_regex():
    global _wordchars_re, _squote_re, _dquote_re
    _wordchars_re = re.compile(r'[^\\\'\"%s ]*' % string.whitespace)
    _squote_re = re.compile(r"'(?:[^'\\]|\\.)*'")
    _dquote_re = re.compile(r'"(?:[^"\\]|\\.)*"')

def split_quoted (s):
    """Split a string up according to Unix shell-like rules for quotes and
    backslashes.  In short: words are delimited by spaces, as long as those
    spaces are not escaped by a backslash, or inside a quoted string.
    Single and double quotes are equivalent, and the quote characters can
    be backslash-escaped.  The backslash is stripped from any two-character
    escape sequence, leaving only the escaped character.  The quote
    characters are stripped from any quoted string.  Returns a list of
    words.
    """

    # This is a nice algorithm for splitting up a single string, since it
    # doesn't require character-by-character examination.  It was a little
    # bit of a brain-bender to get it working right, though...
    if _wordchars_re is None: _init_regex()

    s = s.strip()
    words = []
    pos = 0

    while s:
        m = _wordchars_re.match(s, pos)
        end = m.end()
        if end == len(s):
            words.append(s[:end])
            break

        if s[end] in string.whitespace: # unescaped, unquoted whitespace: now
            words.append(s[:end])       # we definitely have a word delimiter
            s = s[end:].lstrip()
            pos = 0

        elif s[end] == '\\':            # preserve whatever is being escaped;
                                        # will become part of the current word
            s = s[:end] + s[end+1:]
            pos = end+1

        else:
            if s[end] == "'":           # slurp singly-quoted string
                m = _squote_re.match(s, end)
            elif s[end] == '"':         # slurp doubly-quoted string
                m = _dquote_re.match(s, end)
            else:
                raise RuntimeError("this can't happen (bad char '%c')" % s[end])

            if m is None:
                raise ValueError("bad string (mismatched %s quotes?)" % s[end])

            (beg, end) = m.span()
            s = s[:beg] + s[beg+1:end-1] + s[end:]
            pos = m.end() - 2

        if pos >= len(s):
            words.append(s)
            break

    return words

# split_quoted ()


================================================
File: /Tools/cases_generator/README.md
================================================
# Tooling to generate interpreters

Documentation for the instruction definitions in `Python/bytecodes.c`
("the DSL") is [here](interpreter_definition.md).

What's currently here:

- `analyzer.py`: code for converting `AST` generated by `Parser`
  to more high-level structure for easier interaction
- `lexer.py`: lexer for C, originally written by Mark Shannon
- `plexer.py`: OO interface on top of lexer.py; main class: `PLexer`
- `parsing.py`: Parser for instruction definition DSL; main class: `Parser`
- `parser.py` helper for interactions with `parsing.py`
- `tierN_generator.py`: a couple of driver scripts to read `Python/bytecodes.c` and
  write `Python/generated_cases.c.h` (and several other files)
- `optimizer_generator.py`: reads `Python/bytecodes.c` and
  `Python/optimizer_bytecodes.c` and writes
  `Python/optimizer_cases.c.h`
- `stack.py`: code to handle generalized stack effects
- `cwriter.py`: code which understands tokens and how to format C code;
  main class: `CWriter`
- `generators_common.py`: helpers for generators
- `opcode_id_generator.py`: generate a list of opcodes and write them to
  `Include/opcode_ids.h`
- `opcode_metadata_generator.py`: reads the instruction definitions and
  write the metadata to `Include/internal/pycore_opcode_metadata.h`
- `py_metadata_generator.py`: reads the instruction definitions and
  write the metadata to `Lib/_opcode_metadata.py`
- `target_generator.py`: generate targets for computed goto dispatch and
  write them to `Python/opcode_targets.h`
- `uop_id_generator.py`: generate a list of uop IDs and write them to
  `Include/internal/pycore_uop_ids.h`
- `uop_metadata_generator.py`: reads the instruction definitions and
  write the metadata to `Include/internal/pycore_uop_metadata.h`

Note that there is some dummy C code at the top and bottom of
`Python/bytecodes.c`
to fool text editors like VS Code into believing this is valid C code.

## A bit about the parser

The parser class uses a pretty standard recursive descent scheme,
but with unlimited backtracking.
The `PLexer` class tokenizes the entire input before parsing starts.
We do not run the C preprocessor.
Each parsing method returns either an AST node (a `Node` instance)
or `None`, or raises `SyntaxError` (showing the error in the C source).

Most parsing methods are decorated with `@contextual`, which automatically
resets the tokenizer input position when `None` is returned.
Parsing methods may also raise `SyntaxError`, which is irrecoverable.
When a parsing method returns `None`, it is possible that after backtracking
a different parsing method returns a valid AST.

Neither the lexer nor the parsers are complete or fully correct.
Most known issues are tersely indicated by `# TODO:` comments.
We plan to fix issues as they become relevant.


================================================
File: /Tools/cases_generator/_typing_backports.py
================================================
"""Backports from newer versions of the typing module.

We backport these features here so that Python can still build
while using an older Python version for PYTHON_FOR_REGEN.
"""

from typing import NoReturn


def assert_never(obj: NoReturn) -> NoReturn:
    """Statically assert that a line of code is unreachable.

    Backport of typing.assert_never (introduced in Python 3.11).
    """
    raise AssertionError(f"Expected code to be unreachable, but got: {obj}")


================================================
File: /Tools/cases_generator/analyzer.py
================================================
from dataclasses import dataclass, field
import itertools
import lexer
import parser
import re
from typing import Optional

@dataclass
class Properties:
    escaping_calls: dict[lexer.Token, tuple[lexer.Token, lexer.Token]]
    error_with_pop: bool
    error_without_pop: bool
    deopts: bool
    oparg: bool
    jumps: bool
    eval_breaker: bool
    needs_this: bool
    always_exits: bool
    stores_sp: bool
    uses_co_consts: bool
    uses_co_names: bool
    uses_locals: bool
    has_free: bool
    side_exit: bool
    pure: bool
    tier: int | None = None
    oparg_and_1: bool = False
    const_oparg: int = -1
    needs_prev: bool = False
    no_save_ip: bool = False

    def dump(self, indent: str) -> None:
        simple_properties = self.__dict__.copy()
        del simple_properties["escaping_calls"]
        text = "escaping_calls:\n"
        for tkns in self.escaping_calls.values():
            text += f"{indent}    {tkns}\n"
        text += ", ".join([f"{key}: {value}" for (key, value) in simple_properties.items()])
        print(indent, text, sep="")

    @staticmethod
    def from_list(properties: list["Properties"]) -> "Properties":
        escaping_calls: dict[lexer.Token, tuple[lexer.Token, lexer.Token]] = {}
        for p in properties:
            escaping_calls.update(p.escaping_calls)
        return Properties(
            escaping_calls=escaping_calls,
            error_with_pop=any(p.error_with_pop for p in properties),
            error_without_pop=any(p.error_without_pop for p in properties),
            deopts=any(p.deopts for p in properties),
            oparg=any(p.oparg for p in properties),
            jumps=any(p.jumps for p in properties),
            eval_breaker=any(p.eval_breaker for p in properties),
            needs_this=any(p.needs_this for p in properties),
            always_exits=any(p.always_exits for p in properties),
            stores_sp=any(p.stores_sp for p in properties),
            uses_co_consts=any(p.uses_co_consts for p in properties),
            uses_co_names=any(p.uses_co_names for p in properties),
            uses_locals=any(p.uses_locals for p in properties),
            has_free=any(p.has_free for p in properties),
            side_exit=any(p.side_exit for p in properties),
            pure=all(p.pure for p in properties),
            needs_prev=any(p.needs_prev for p in properties),
            no_save_ip=all(p.no_save_ip for p in properties),
        )

    @property
    def infallible(self) -> bool:
        return not self.error_with_pop and not self.error_without_pop

    @property
    def escapes(self) -> bool:
        return bool(self.escaping_calls)

SKIP_PROPERTIES = Properties(
    escaping_calls={},
    error_with_pop=False,
    error_without_pop=False,
    deopts=False,
    oparg=False,
    jumps=False,
    eval_breaker=False,
    needs_this=False,
    always_exits=False,
    stores_sp=False,
    uses_co_consts=False,
    uses_co_names=False,
    uses_locals=False,
    has_free=False,
    side_exit=False,
    pure=True,
    no_save_ip=False,
)


@dataclass
class Skip:
    "Unused cache entry"
    size: int

    @property
    def name(self) -> str:
        return f"unused/{self.size}"

    @property
    def properties(self) -> Properties:
        return SKIP_PROPERTIES


class Flush:
    @property
    def properties(self) -> Properties:
        return SKIP_PROPERTIES

    @property
    def name(self) -> str:
        return "flush"

    @property
    def size(self) -> int:
        return 0


@dataclass
class StackItem:
    name: str
    type: str | None
    condition: str | None
    size: str
    peek: bool = False
    used: bool = False

    def __str__(self) -> str:
        cond = f" if ({self.condition})" if self.condition else ""
        size = f"[{self.size}]" if self.size else ""
        type = "" if self.type is None else f"{self.type} "
        return f"{type}{self.name}{size}{cond} {self.peek}"

    def is_array(self) -> bool:
        return self.size != ""

    def get_size(self) -> str:
        return self.size if self.size else "1"


@dataclass
class StackEffect:
    inputs: list[StackItem]
    outputs: list[StackItem]

    def __str__(self) -> str:
        return f"({', '.join([str(i) for i in self.inputs])} -- {', '.join([str(i) for i in self.outputs])})"


@dataclass
class CacheEntry:
    name: str
    size: int

    def __str__(self) -> str:
        return f"{self.name}/{self.size}"


@dataclass
class Uop:
    name: str
    context: parser.Context | None
    annotations: list[str]
    stack: StackEffect
    caches: list[CacheEntry]
    deferred_refs: dict[lexer.Token, str | None]
    output_stores: list[lexer.Token]
    body: list[lexer.Token]
    properties: Properties
    _size: int = -1
    implicitly_created: bool = False
    replicated = 0
    replicates: "Uop | None" = None
    # Size of the instruction(s), only set for uops containing the INSTRUCTION_SIZE macro
    instruction_size: int | None = None

    def dump(self, indent: str) -> None:
        print(
            indent, self.name, ", ".join(self.annotations) if self.annotations else ""
        )
        print(indent, self.stack, ", ".join([str(c) for c in self.caches]))
        self.properties.dump("    " + indent)

    @property
    def size(self) -> int:
        if self._size < 0:
            self._size = sum(c.size for c in self.caches)
        return self._size

    def why_not_viable(self) -> str | None:
        if self.name == "_SAVE_RETURN_OFFSET":
            return None  # Adjusts next_instr, but only in tier 1 code
        if "INSTRUMENTED" in self.name:
            return "is instrumented"
        if "replaced" in self.annotations:
            return "is replaced"
        if self.name in ("INTERPRETER_EXIT", "JUMP_BACKWARD"):
            return "has tier 1 control flow"
        if self.properties.needs_this:
            return "uses the 'this_instr' variable"
        if len([c for c in self.caches if c.name != "unused"]) > 2:
            return "has unused cache entries"
        if self.properties.error_with_pop and self.properties.error_without_pop:
            return "has both popping and not-popping errors"
        return None

    def is_viable(self) -> bool:
        return self.why_not_viable() is None

    def is_super(self) -> bool:
        for tkn in self.body:
            if tkn.kind == "IDENTIFIER" and tkn.text == "oparg1":
                return True
        return False


Part = Uop | Skip | Flush


@dataclass
class Instruction:
    where: lexer.Token
    name: str
    parts: list[Part]
    _properties: Properties | None
    is_target: bool = False
    family: Optional["Family"] = None
    opcode: int = -1

    @property
    def properties(self) -> Properties:
        if self._properties is None:
            self._properties = self._compute_properties()
        return self._properties

    def _compute_properties(self) -> Properties:
        return Properties.from_list([part.properties for part in self.parts])

    def dump(self, indent: str) -> None:
        print(indent, self.name, "=", ", ".join([part.name for part in self.parts]))
        self.properties.dump("    " + indent)

    @property
    def size(self) -> int:
        return 1 + sum(part.size for part in self.parts)

    def is_super(self) -> bool:
        if len(self.parts) != 1:
            return False
        uop = self.parts[0]
        if isinstance(uop, Uop):
            return uop.is_super()
        else:
            return False


@dataclass
class PseudoInstruction:
    name: str
    stack: StackEffect
    targets: list[Instruction]
    as_sequence: bool
    flags: list[str]
    opcode: int = -1

    def dump(self, indent: str) -> None:
        print(indent, self.name, "->", " or ".join([t.name for t in self.targets]))

    @property
    def properties(self) -> Properties:
        return Properties.from_list([i.properties for i in self.targets])


@dataclass
class Family:
    name: str
    size: str
    members: list[Instruction]

    def dump(self, indent: str) -> None:
        print(indent, self.name, "= ", ", ".join([m.name for m in self.members]))


@dataclass
class Analysis:
    instructions: dict[str, Instruction]
    uops: dict[str, Uop]
    families: dict[str, Family]
    pseudos: dict[str, PseudoInstruction]
    opmap: dict[str, int]
    have_arg: int
    min_instrumented: int


def analysis_error(message: str, tkn: lexer.Token) -> SyntaxError:
    # To do -- support file and line output
    # Construct a SyntaxError instance from message and token
    return lexer.make_syntax_error(message, tkn.filename, tkn.line, tkn.column, "")


def override_error(
    name: str,
    context: parser.Context | None,
    prev_context: parser.Context | None,
    token: lexer.Token,
) -> SyntaxError:
    return analysis_error(
        f"Duplicate definition of '{name}' @ {context} "
        f"previous definition @ {prev_context}",
        token,
    )


def convert_stack_item(
    item: parser.StackEffect, replace_op_arg_1: str | None
) -> StackItem:
    cond = item.cond
    if replace_op_arg_1 and OPARG_AND_1.match(item.cond):
        cond = replace_op_arg_1
    return StackItem(item.name, item.type, cond, item.size)


def analyze_stack(
    op: parser.InstDef | parser.Pseudo, replace_op_arg_1: str | None = None
) -> StackEffect:
    inputs: list[StackItem] = [
        convert_stack_item(i, replace_op_arg_1)
        for i in op.inputs
        if isinstance(i, parser.StackEffect)
    ]
    outputs: list[StackItem] = [
        convert_stack_item(i, replace_op_arg_1) for i in op.outputs
    ]
    # Mark variables with matching names at the base of the stack as "peek"
    modified = False
    input_names: dict[str, lexer.Token] = { i.name : i.first_token for i in op.inputs if i.name != "unused" }
    for input, output in itertools.zip_longest(inputs, outputs):
        if output is None:
            pass
        elif input is None:
            if output.name in input_names:
                raise analysis_error(
                    f"Reuse of variable '{output.name}' at different stack location",
                    input_names[output.name])
        elif input.name == output.name:
            if not modified:
                input.peek = output.peek = True
        else:
            modified = True
            if output.name in input_names:
                raise analysis_error(
                    f"Reuse of variable '{output.name}' at different stack location",
                    input_names[output.name])
    if isinstance(op, parser.InstDef):
        output_names = [out.name for out in outputs]
        for input in inputs:
            if (
                variable_used(op, input.name)
                or variable_used(op, "DECREF_INPUTS")
                or (not input.peek and input.name in output_names)
            ):
                input.used = True
        for output in outputs:
            if variable_used(op, output.name):
                output.used = True
    return StackEffect(inputs, outputs)


def analyze_caches(inputs: list[parser.InputEffect]) -> list[CacheEntry]:
    caches: list[parser.CacheEffect] = [
        i for i in inputs if isinstance(i, parser.CacheEffect)
    ]
    for cache in caches:
        if cache.name == "unused":
            raise analysis_error(
                "Unused cache entry in op. Move to enclosing macro.", cache.tokens[0]
            )
    return [CacheEntry(i.name, int(i.size)) for i in caches]


def find_assignment_target(node: parser.InstDef, idx: int) -> list[lexer.Token]:
    """Find the tokens that make up the left-hand side of an assignment"""
    offset = 0
    for tkn in reversed(node.block.tokens[: idx]):
        if tkn.kind in {"SEMI", "LBRACE", "RBRACE"}:
            return node.block.tokens[idx - offset : idx]
        offset += 1
    return []


def find_stores_outputs(node: parser.InstDef) -> list[lexer.Token]:
    res: list[lexer.Token] = []
    outnames = { out.name for out in node.outputs }
    innames = { out.name for out in node.inputs }
    for idx, tkn in enumerate(node.block.tokens):
        if tkn.kind == "AND":
            name = node.block.tokens[idx+1]
            if name.text in outnames:
                res.append(name)
        if tkn.kind != "EQUALS":
            continue
        lhs = find_assignment_target(node, idx)
        assert lhs
        while lhs and lhs[0].kind == "COMMENT":
            lhs = lhs[1:]
        if len(lhs) != 1 or lhs[0].kind != "IDENTIFIER":
            continue
        name = lhs[0]
        if name.text in innames:
            raise analysis_error(f"Cannot assign to input variable '{name.text}'", name)
        if name.text in outnames:
            res.append(name)
    return res

def analyze_deferred_refs(node: parser.InstDef) -> dict[lexer.Token, str | None]:
    """Look for PyStackRef_FromPyObjectNew() calls"""

    def in_frame_push(idx: int) -> bool:
        for tkn in reversed(node.block.tokens[: idx - 1]):
            if tkn.kind in {"SEMI", "LBRACE", "RBRACE"}:
                return False
            if tkn.kind == "IDENTIFIER" and tkn.text == "_PyFrame_PushUnchecked":
                return True
        return False

    refs: dict[lexer.Token, str | None] = {}
    for idx, tkn in enumerate(node.block.tokens):
        if tkn.kind != "IDENTIFIER" or tkn.text != "PyStackRef_FromPyObjectNew":
            continue

        if idx == 0 or node.block.tokens[idx - 1].kind != "EQUALS":
            if in_frame_push(idx):
                # PyStackRef_FromPyObjectNew() is called in _PyFrame_PushUnchecked()
                refs[tkn] = None
                continue
            raise analysis_error("Expected '=' before PyStackRef_FromPyObjectNew", tkn)

        lhs = find_assignment_target(node, idx - 1)
        if len(lhs) == 0:
            raise analysis_error(
                "PyStackRef_FromPyObjectNew() must be assigned to an output", tkn
            )

        if lhs[0].kind == "TIMES" or any(
            t.kind == "ARROW" or t.kind == "LBRACKET" for t in lhs[1:]
        ):
            # Don't handle: *ptr = ..., ptr->field = ..., or ptr[field] = ...
            # Assume that they are visible to the GC.
            refs[tkn] = None
            continue

        if len(lhs) != 1 or lhs[0].kind != "IDENTIFIER":
            raise analysis_error(
                "PyStackRef_FromPyObjectNew() must be assigned to an output", tkn
            )

        name = lhs[0].text
        match = (
            any(var.name == name for var in node.inputs)
            or any(var.name == name for var in node.outputs)
        )
        if not match:
            raise analysis_error(
                f"PyStackRef_FromPyObjectNew() must be assigned to an input or output, not '{name}'",
                tkn,
            )

        refs[tkn] = name

    return refs


def variable_used(node: parser.InstDef, name: str) -> bool:
    """Determine whether a variable with a given name is used in a node."""
    return any(
        token.kind == "IDENTIFIER" and token.text == name for token in node.block.tokens
    )


def oparg_used(node: parser.InstDef) -> bool:
    """Determine whether `oparg` is used in a node."""
    return any(
        token.kind == "IDENTIFIER" and token.text == "oparg" for token in node.tokens
    )


def tier_variable(node: parser.InstDef) -> int | None:
    """Determine whether a tier variable is used in a node."""
    for token in node.tokens:
        if token.kind == "ANNOTATION":
            if token.text == "specializing":
                return 1
            if re.fullmatch(r"tier\d", token.text):
                return int(token.text[-1])
    return None


def has_error_with_pop(op: parser.InstDef) -> bool:
    return (
        variable_used(op, "ERROR_IF")
        or variable_used(op, "pop_1_error")
        or variable_used(op, "exception_unwind")
        or variable_used(op, "resume_with_error")
    )


def has_error_without_pop(op: parser.InstDef) -> bool:
    return (
        variable_used(op, "ERROR_NO_POP")
        or variable_used(op, "pop_1_error")
        or variable_used(op, "exception_unwind")
        or variable_used(op, "resume_with_error")
    )


NON_ESCAPING_FUNCTIONS = (
    "PyCFunction_GET_FLAGS",
    "PyCFunction_GET_FUNCTION",
    "PyCFunction_GET_SELF",
    "PyCell_GetRef",
    "PyCell_New",
    "PyCell_SwapTakeRef",
    "PyExceptionInstance_Class",
    "PyException_GetCause",
    "PyException_GetContext",
    "PyException_GetTraceback",
    "PyFloat_AS_DOUBLE",
    "PyFloat_FromDouble",
    "PyFunction_GET_CODE",
    "PyFunction_GET_GLOBALS",
    "PyList_GET_ITEM",
    "PyList_GET_SIZE",
    "PyList_SET_ITEM",
    "PyLong_AsLong",
    "PyLong_FromLong",
    "PyLong_FromSsize_t",
    "PySlice_New",
    "PyStackRef_AsPyObjectBorrow",
    "PyStackRef_AsPyObjectNew",
    "PyStackRef_AsPyObjectSteal",
    "PyStackRef_CLEAR",
    "PyStackRef_CLOSE",
    "PyStackRef_CLOSE_SPECIALIZED",
    "PyStackRef_DUP",
    "PyStackRef_False",
    "PyStackRef_FromPyObjectImmortal",
    "PyStackRef_FromPyObjectNew",
    "PyStackRef_FromPyObjectSteal",
    "PyStackRef_IsExactly",
    "PyStackRef_IsNone",
    "PyStackRef_IsTrue",
    "PyStackRef_IsFalse",
    "PyStackRef_IsNull",
    "PyStackRef_None",
    "PyStackRef_TYPE",
    "PyStackRef_True",
    "PyTuple_GET_ITEM",
    "PyTuple_GET_SIZE",
    "PyType_HasFeature",
    "PyUnicode_Append",
    "PyUnicode_Concat",
    "PyUnicode_GET_LENGTH",
    "PyUnicode_READ_CHAR",
    "Py_ARRAY_LENGTH",
    "Py_CLEAR",
    "Py_DECREF",
    "Py_FatalError",
    "Py_INCREF",
    "Py_IS_TYPE",
    "Py_NewRef",
    "Py_REFCNT",
    "Py_SIZE",
    "Py_TYPE",
    "Py_UNREACHABLE",
    "Py_Unicode_GET_LENGTH",
    "Py_XDECREF",
    "_PyCode_CODE",
    "_PyDictValues_AddToInsertionOrder",
    "_PyErr_Occurred",
    "_PyEval_FrameClearAndPop",
    "_PyFloat_FromDouble_ConsumeInputs",
    "_PyFrame_GetCode",
    "_PyFrame_IsIncomplete",
    "_PyFrame_PushUnchecked",
    "_PyFrame_SetStackPointer",
    "_PyFrame_StackPush",
    "_PyFunction_SetVersion",
    "_PyGen_GetGeneratorFromFrame",
    "_PyInterpreterState_GET",
    "_PyList_AppendTakeRef",
    "_PyList_FromStackRefSteal",
    "_PyList_ITEMS",
    "_PyLong_Add",
    "_PyLong_CompactValue",
    "_PyLong_DigitCount",
    "_PyLong_IsCompact",
    "_PyLong_IsNegative",
    "_PyLong_IsNonNegativeCompact",
    "_PyLong_IsZero",
    "_PyLong_Multiply",
    "_PyLong_Subtract",
    "_PyManagedDictPointer_IsValues",
    "_PyObject_GC_IS_TRACKED",
    "_PyObject_GC_MAY_BE_TRACKED",
    "_PyObject_GC_TRACK",
    "_PyObject_GetManagedDict",
    "_PyObject_InlineValues",
    "_PyObject_ManagedDictPointer",
    "_PyThreadState_HasStackSpace",
    "_PyTuple_FromArraySteal",
    "_PyTuple_FromStackRefSteal",
    "_PyTuple_ITEMS",
    "_PyType_HasFeature",
    "_PyType_NewManagedObject",
    "_PyUnicode_Equal",
    "_PyUnicode_JoinArray",
    "_Py_CHECK_EMSCRIPTEN_SIGNALS_PERIODICALLY",
    "_Py_DECREF_NO_DEALLOC",
    "_Py_DECREF_SPECIALIZED",
    "_Py_EnterRecursiveCallTstateUnchecked",
    "_Py_ID",
    "_Py_IsImmortal",
    "_Py_LeaveRecursiveCallPy",
    "_Py_LeaveRecursiveCallTstate",
    "_Py_NewRef",
    "_Py_SINGLETON",
    "_Py_STR",
    "_Py_TryIncrefCompare",
    "_Py_TryIncrefCompareStackRef",
    "_Py_atomic_load_ptr_acquire",
    "_Py_atomic_load_uintptr_relaxed",
    "_Py_set_eval_breaker_bit",
    "advance_backoff_counter",
    "assert",
    "backoff_counter_triggers",
    "initial_temperature_backoff_counter",
    "maybe_lltrace_resume_frame",
    "restart_backoff_counter",
)

def find_stmt_start(node: parser.InstDef, idx: int) -> lexer.Token:
    assert idx < len(node.block.tokens)
    while True:
        tkn = node.block.tokens[idx-1]
        if tkn.kind in {"SEMI", "LBRACE", "RBRACE", "CMACRO"}:
            break
        idx -= 1
        assert idx > 0
    while node.block.tokens[idx].kind == "COMMENT":
        idx += 1
    return node.block.tokens[idx]


def find_stmt_end(node: parser.InstDef, idx: int) -> lexer.Token:
    assert idx < len(node.block.tokens)
    while True:
        idx += 1
        tkn = node.block.tokens[idx]
        if tkn.kind == "SEMI":
            return node.block.tokens[idx+1]

def check_escaping_calls(instr: parser.InstDef, escapes: dict[lexer.Token, tuple[lexer.Token, lexer.Token]]) -> None:
    calls = {escapes[t][0] for t in escapes}
    in_if = 0
    tkn_iter = iter(instr.block.tokens)
    for tkn in tkn_iter:
        if tkn.kind == "IF":
            next(tkn_iter)
            in_if = 1
        if tkn.kind == "IDENTIFIER" and tkn.text in ("DEOPT_IF", "ERROR_IF", "EXIT_IF"):
            next(tkn_iter)
            in_if = 1
        elif tkn.kind == "LPAREN" and in_if:
            in_if += 1
        elif tkn.kind == "RPAREN":
            if in_if:
                in_if -= 1
        elif tkn in calls and in_if:
            raise analysis_error(f"Escaping call '{tkn.text} in condition", tkn)

def find_escaping_api_calls(instr: parser.InstDef) -> dict[lexer.Token, tuple[lexer.Token, lexer.Token]]:
    result: dict[lexer.Token, tuple[lexer.Token, lexer.Token]] = {}
    tokens = instr.block.tokens
    for idx, tkn in enumerate(tokens):
        try:
            next_tkn = tokens[idx+1]
        except IndexError:
            break
        if tkn.kind == "SWITCH":
            raise analysis_error(f"switch statements are not supported due to their complex flow control. Sorry.", tkn)
        if next_tkn.kind != lexer.LPAREN:
            continue
        if tkn.kind == lexer.IDENTIFIER:
            if tkn.text.upper() == tkn.text:
                # simple macro
                continue
            #if not tkn.text.startswith(("Py", "_Py", "monitor")):
            #    continue
            if tkn.text.startswith(("sym_", "optimize_")):
                # Optimize functions
                continue
            if tkn.text.endswith("Check"):
                continue
            if tkn.text.startswith("Py_Is"):
                continue
            if tkn.text.endswith("CheckExact"):
                continue
            if tkn.text in NON_ESCAPING_FUNCTIONS:
                continue
        elif tkn.kind == "RPAREN":
            prev = tokens[idx-1]
            if prev.text.endswith("_t") or prev.text == "*" or prev.text == "int":
                #cast
                continue
        elif tkn.kind != "RBRACKET":
            continue
        start = find_stmt_start(instr, idx)
        end = find_stmt_end(instr, idx)
        result[start] = tkn, end
    check_escaping_calls(instr, result)
    return result


EXITS = {
    "DISPATCH",
    "GO_TO_INSTRUCTION",
    "Py_UNREACHABLE",
    "DISPATCH_INLINED",
    "DISPATCH_GOTO",
}


def always_exits(op: parser.InstDef) -> bool:
    depth = 0
    tkn_iter = iter(op.tokens)
    for tkn in tkn_iter:
        if tkn.kind == "LBRACE":
            depth += 1
        elif tkn.kind == "RBRACE":
            depth -= 1
        elif depth > 1:
            continue
        elif tkn.kind == "GOTO" or tkn.kind == "RETURN":
            return True
        elif tkn.kind == "KEYWORD":
            if tkn.text in EXITS:
                return True
        elif tkn.kind == "IDENTIFIER":
            if tkn.text in EXITS:
                return True
            if tkn.text == "DEOPT_IF" or tkn.text == "ERROR_IF":
                next(tkn_iter)  # '('
                t = next(tkn_iter)
                if t.text in ("true", "1"):
                    return True
    return False


def stack_effect_only_peeks(instr: parser.InstDef) -> bool:
    stack_inputs = [s for s in instr.inputs if not isinstance(s, parser.CacheEffect)]
    if len(stack_inputs) != len(instr.outputs):
        return False
    if len(stack_inputs) == 0:
        return False
    if any(s.cond for s in stack_inputs) or any(s.cond for s in instr.outputs):
        return False
    return all(
        (s.name == other.name and s.type == other.type and s.size == other.size)
        for s, other in zip(stack_inputs, instr.outputs)
    )


OPARG_AND_1 = re.compile("\\(*oparg *& *1")


def effect_depends_on_oparg_1(op: parser.InstDef) -> bool:
    for effect in op.inputs:
        if isinstance(effect, parser.CacheEffect):
            continue
        if not effect.cond:
            continue
        if OPARG_AND_1.match(effect.cond):
            return True
    for effect in op.outputs:
        if not effect.cond:
            continue
        if OPARG_AND_1.match(effect.cond):
            return True
    return False


def compute_properties(op: parser.InstDef) -> Properties:
    escaping_calls = find_escaping_api_calls(op)
    has_free = (
        variable_used(op, "PyCell_New")
        or variable_used(op, "PyCell_GetRef")
        or variable_used(op, "PyCell_SetTakeRef")
        or variable_used(op, "PyCell_SwapTakeRef")
    )
    deopts_if = variable_used(op, "DEOPT_IF")
    exits_if = variable_used(op, "EXIT_IF")
    if deopts_if and exits_if:
        tkn = op.tokens[0]
        raise lexer.make_syntax_error(
            "Op cannot contain both EXIT_IF and DEOPT_IF",
            tkn.filename,
            tkn.line,
            tkn.column,
            op.name,
        )
    error_with_pop = has_error_with_pop(op)
    error_without_pop = has_error_without_pop(op)
    return Properties(
        escaping_calls=escaping_calls,
        error_with_pop=error_with_pop,
        error_without_pop=error_without_pop,
        deopts=deopts_if,
        side_exit=exits_if,
        oparg=oparg_used(op),
        jumps=variable_used(op, "JUMPBY"),
        eval_breaker="CHECK_PERIODIC" in op.name,
        needs_this=variable_used(op, "this_instr"),
        always_exits=always_exits(op),
        stores_sp=variable_used(op, "SYNC_SP"),
        uses_co_consts=variable_used(op, "FRAME_CO_CONSTS"),
        uses_co_names=variable_used(op, "FRAME_CO_NAMES"),
        uses_locals=(variable_used(op, "GETLOCAL") or variable_used(op, "SETLOCAL"))
        and not has_free,
        has_free=has_free,
        pure="pure" in op.annotations,
        no_save_ip="no_save_ip" in op.annotations,
        tier=tier_variable(op),
        needs_prev=variable_used(op, "prev_instr"),
    )


def make_uop(
    name: str,
    op: parser.InstDef,
    inputs: list[parser.InputEffect],
    uops: dict[str, Uop],
) -> Uop:
    result = Uop(
        name=name,
        context=op.context,
        annotations=op.annotations,
        stack=analyze_stack(op),
        caches=analyze_caches(inputs),
        deferred_refs=analyze_deferred_refs(op),
        output_stores=find_stores_outputs(op),
        body=op.block.tokens,
        properties=compute_properties(op),
    )
    if effect_depends_on_oparg_1(op) and "split" in op.annotations:
        result.properties.oparg_and_1 = True
        for bit in ("0", "1"):
            name_x = name + "_" + bit
            properties = compute_properties(op)
            if properties.oparg:
                # May not need oparg anymore
                properties.oparg = any(
                    token.text == "oparg" for token in op.block.tokens
                )
            rep = Uop(
                name=name_x,
                context=op.context,
                annotations=op.annotations,
                stack=analyze_stack(op, bit),
                caches=analyze_caches(inputs),
                deferred_refs=analyze_deferred_refs(op),
                output_stores=find_stores_outputs(op),
                body=op.block.tokens,
                properties=properties,
            )
            rep.replicates = result
            uops[name_x] = rep
    for anno in op.annotations:
        if anno.startswith("replicate"):
            result.replicated = int(anno[10:-1])
            break
    else:
        return result
    for oparg in range(result.replicated):
        name_x = name + "_" + str(oparg)
        properties = compute_properties(op)
        properties.oparg = False
        properties.const_oparg = oparg
        rep = Uop(
            name=name_x,
            context=op.context,
            annotations=op.annotations,
            stack=analyze_stack(op),
            caches=analyze_caches(inputs),
            deferred_refs=analyze_deferred_refs(op),
            output_stores=find_stores_outputs(op),
            body=op.block.tokens,
            properties=properties,
        )
        rep.replicates = result
        uops[name_x] = rep

    return result


def add_op(op: parser.InstDef, uops: dict[str, Uop]) -> None:
    assert op.kind == "op"
    if op.name in uops:
        if "override" not in op.annotations:
            raise override_error(
                op.name, op.context, uops[op.name].context, op.tokens[0]
            )
    uops[op.name] = make_uop(op.name, op, op.inputs, uops)


def add_instruction(
    where: lexer.Token,
    name: str,
    parts: list[Part],
    instructions: dict[str, Instruction],
) -> None:
    instructions[name] = Instruction(where, name, parts, None)


def desugar_inst(
    inst: parser.InstDef, instructions: dict[str, Instruction], uops: dict[str, Uop]
) -> None:
    assert inst.kind == "inst"
    name = inst.name
    op_inputs: list[parser.InputEffect] = []
    parts: list[Part] = []
    uop_index = -1
    # Move unused cache entries to the Instruction, removing them from the Uop.
    for input in inst.inputs:
        if isinstance(input, parser.CacheEffect) and input.name == "unused":
            parts.append(Skip(input.size))
        else:
            op_inputs.append(input)
            if uop_index < 0:
                uop_index = len(parts)
                # Place holder for the uop.
                parts.append(Skip(0))
    uop = make_uop("_" + inst.name, inst, op_inputs, uops)
    uop.implicitly_created = True
    uops[inst.name] = uop
    if uop_index < 0:
        parts.append(uop)
    else:
        parts[uop_index] = uop
    add_instruction(inst.first_token, name, parts, instructions)


def add_macro(
    macro: parser.Macro, instructions: dict[str, Instruction], uops: dict[str, Uop]
) -> None:
    parts: list[Part] = []
    for part in macro.uops:
        match part:
            case parser.OpName():
                if part.name == "flush":
                    parts.append(Flush())
                else:
                    if part.name not in uops:
                        raise analysis_error(
                            f"No Uop named {part.name}", macro.tokens[0]
                        )
                    parts.append(uops[part.name])
            case parser.CacheEffect():
                parts.append(Skip(part.size))
            case _:
                assert False
    assert parts
    add_instruction(macro.first_token, macro.name, parts, instructions)


def add_family(
    pfamily: parser.Family,
    instructions: dict[str, Instruction],
    families: dict[str, Family],
) -> None:
    family = Family(
        pfamily.name,
        pfamily.size,
        [instructions[member_name] for member_name in pfamily.members],
    )
    for member in family.members:
        member.family = family
    # The head of the family is an implicit jump target for DEOPTs
    instructions[family.name].is_target = True
    families[family.name] = family


def add_pseudo(
    pseudo: parser.Pseudo,
    instructions: dict[str, Instruction],
    pseudos: dict[str, PseudoInstruction],
) -> None:
    pseudos[pseudo.name] = PseudoInstruction(
        pseudo.name,
        analyze_stack(pseudo),
        [instructions[target] for target in pseudo.targets],
        pseudo.as_sequence,
        pseudo.flags,
    )


def assign_opcodes(
    instructions: dict[str, Instruction],
    families: dict[str, Family],
    pseudos: dict[str, PseudoInstruction],
) -> tuple[dict[str, int], int, int]:
    """Assigns opcodes, then returns the opmap,
    have_arg and min_instrumented values"""
    instmap: dict[str, int] = {}

    # 0 is reserved for cache entries. This helps debugging.
    instmap["CACHE"] = 0

    # 17 is reserved as it is the initial value for the specializing counter.
    # This helps catch cases where we attempt to execute a cache.
    instmap["RESERVED"] = 17

    # 149 is RESUME - it is hard coded as such in Tools/build/deepfreeze.py
    instmap["RESUME"] = 149

    # This is an historical oddity.
    instmap["BINARY_OP_INPLACE_ADD_UNICODE"] = 3

    instmap["INSTRUMENTED_LINE"] = 254
    instmap["ENTER_EXECUTOR"] = 255

    instrumented = [name for name in instructions if name.startswith("INSTRUMENTED")]

    specialized: set[str] = set()
    no_arg: list[str] = []
    has_arg: list[str] = []

    for family in families.values():
        specialized.update(inst.name for inst in family.members)

    for inst in instructions.values():
        name = inst.name
        if name in specialized:
            continue
        if name in instrumented:
            continue
        if inst.properties.oparg:
            has_arg.append(name)
        else:
            no_arg.append(name)

    # Specialized ops appear in their own section
    # Instrumented opcodes are at the end of the valid range
    min_internal = 150
    min_instrumented = 254 - (len(instrumented) - 1)
    assert min_internal + len(specialized) < min_instrumented

    next_opcode = 1

    def add_instruction(name: str) -> None:
        nonlocal next_opcode
        if name in instmap:
            return  # Pre-defined name
        while next_opcode in instmap.values():
            next_opcode += 1
        instmap[name] = next_opcode
        next_opcode += 1

    for name in sorted(no_arg):
        add_instruction(name)
    for name in sorted(has_arg):
        add_instruction(name)
    # For compatibility
    next_opcode = min_internal
    for name in sorted(specialized):
        add_instruction(name)
    next_opcode = min_instrumented
    for name in instrumented:
        add_instruction(name)

    for name in instructions:
        instructions[name].opcode = instmap[name]

    for op, name in enumerate(sorted(pseudos), 256):
        instmap[name] = op
        pseudos[name].opcode = op

    return instmap, len(no_arg), min_instrumented


def get_instruction_size_for_uop(instructions: dict[str, Instruction], uop: Uop) -> int | None:
    """Return the size of the instruction that contains the given uop or
    `None` if the uop does not contains the `INSTRUCTION_SIZE` macro.

    If there is more than one instruction that contains the uop,
    ensure that they all have the same size.
    """
    for tkn in uop.body:
          if tkn.text == "INSTRUCTION_SIZE":
               break
    else:
        return None

    size = None
    for inst in instructions.values():
        if uop in inst.parts:
            if size is None:
                size = inst.size
            if size != inst.size:
                raise analysis_error(
                    "All instructions containing a uop with the `INSTRUCTION_SIZE` macro "
                    f"must have the same size: {size} != {inst.size}",
                    tkn
                )
    if size is None:
        raise analysis_error(f"No instruction containing the uop '{uop.name}' was found", tkn)
    return size


def analyze_forest(forest: list[parser.AstNode]) -> Analysis:
    instructions: dict[str, Instruction] = {}
    uops: dict[str, Uop] = {}
    families: dict[str, Family] = {}
    pseudos: dict[str, PseudoInstruction] = {}
    for node in forest:
        match node:
            case parser.InstDef(name):
                if node.kind == "inst":
                    desugar_inst(node, instructions, uops)
                else:
                    assert node.kind == "op"
                    add_op(node, uops)
            case parser.Macro():
                pass
            case parser.Family():
                pass
            case parser.Pseudo():
                pass
            case _:
                assert False
    for node in forest:
        if isinstance(node, parser.Macro):
            add_macro(node, instructions, uops)
    for node in forest:
        match node:
            case parser.Family():
                add_family(node, instructions, families)
            case parser.Pseudo():
                add_pseudo(node, instructions, pseudos)
            case _:
                pass
    for uop in uops.values():
        tkn_iter = iter(uop.body)
        for tkn in tkn_iter:
            if tkn.kind == "IDENTIFIER" and tkn.text == "GO_TO_INSTRUCTION":
                if next(tkn_iter).kind != "LPAREN":
                    continue
                target = next(tkn_iter)
                if target.kind != "IDENTIFIER":
                    continue
                if target.text in instructions:
                    instructions[target.text].is_target = True
    for uop in uops.values():
        uop.instruction_size = get_instruction_size_for_uop(instructions, uop)
    # Special case BINARY_OP_INPLACE_ADD_UNICODE
    # BINARY_OP_INPLACE_ADD_UNICODE is not a normal family member,
    # as it is the wrong size, but we need it to maintain an
    # historical optimization.
    if "BINARY_OP_INPLACE_ADD_UNICODE" in instructions:
        inst = instructions["BINARY_OP_INPLACE_ADD_UNICODE"]
        inst.family = families["BINARY_OP"]
        families["BINARY_OP"].members.append(inst)
    opmap, first_arg, min_instrumented = assign_opcodes(instructions, families, pseudos)
    return Analysis(
        instructions, uops, families, pseudos, opmap, first_arg, min_instrumented
    )


def analyze_files(filenames: list[str]) -> Analysis:
    return analyze_forest(parser.parse_files(filenames))


def dump_analysis(analysis: Analysis) -> None:
    print("Uops:")
    for u in analysis.uops.values():
        u.dump("    ")
    print("Instructions:")
    for i in analysis.instructions.values():
        i.dump("    ")
    print("Families:")
    for f in analysis.families.values():
        f.dump("    ")
    print("Pseudos:")
    for p in analysis.pseudos.values():
        p.dump("    ")


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("No input")
    else:
        filenames = sys.argv[1:]
        dump_analysis(analyze_files(filenames))


================================================
File: /Tools/cases_generator/cwriter.py
================================================
import contextlib
from lexer import Token
from typing import TextIO, Iterator


class CWriter:
    "A writer that understands tokens and how to format C code"

    last_token: Token | None

    def __init__(self, out: TextIO, indent: int, line_directives: bool):
        self.out = out
        self.base_column = indent * 4
        self.indents = [i * 4 for i in range(indent + 1)]
        self.line_directives = line_directives
        self.last_token = None
        self.newline = True

    def set_position(self, tkn: Token) -> None:
        if self.last_token is not None:
            if self.last_token.end_line < tkn.line:
                self.out.write("\n")
            if self.last_token.line < tkn.line:
                if self.line_directives:
                    self.out.write(f'#line {tkn.line} "{tkn.filename}"\n')
                self.out.write(" " * self.indents[-1])
            else:
                gap = tkn.column - self.last_token.end_column
                self.out.write(" " * gap)
        elif self.newline:
            self.out.write(" " * self.indents[-1])
        self.last_token = tkn
        self.newline = False

    def emit_at(self, txt: str, where: Token) -> None:
        self.set_position(where)
        self.out.write(txt)

    def maybe_dedent(self, txt: str) -> None:
        parens = txt.count("(") - txt.count(")")
        if parens < 0:
            self.indents.pop()
        braces = txt.count("{") - txt.count("}")
        if braces < 0 or is_label(txt):
            self.indents.pop()

    def maybe_indent(self, txt: str) -> None:
        parens = txt.count("(") - txt.count(")")
        if parens > 0:
            if self.last_token:
                offset = self.last_token.end_column - 1
                if offset <= self.indents[-1] or offset > 40:
                    offset = self.indents[-1] + 4
            else:
                offset = self.indents[-1] + 4
            self.indents.append(offset)
        if is_label(txt):
            self.indents.append(self.indents[-1] + 4)
        else:
            braces = txt.count("{") - txt.count("}")
            if braces > 0:
                assert braces == 1
                if 'extern "C"' in txt:
                    self.indents.append(self.indents[-1])
                else:
                    self.indents.append(self.indents[-1] + 4)

    def emit_text(self, txt: str) -> None:
        self.out.write(txt)

    def emit_multiline_comment(self, tkn: Token) -> None:
        self.set_position(tkn)
        lines = tkn.text.splitlines(True)
        first = True
        for line in lines:
            text = line.lstrip()
            if first:
                spaces = 0
            else:
                spaces = self.indents[-1]
                if text.startswith("*"):
                    spaces += 1
                else:
                    spaces += 3
            first = False
            self.out.write(" " * spaces)
            self.out.write(text)

    def emit_token(self, tkn: Token) -> None:
        if tkn.kind == "COMMENT" and "\n" in tkn.text:
            return self.emit_multiline_comment(tkn)
        self.maybe_dedent(tkn.text)
        self.set_position(tkn)
        self.emit_text(tkn.text)
        if tkn.kind == "CMACRO":
            self.newline = True
        self.maybe_indent(tkn.text)

    def emit_str(self, txt: str) -> None:
        self.maybe_dedent(txt)
        if self.newline and txt:
            if txt[0] != "\n":
                self.out.write(" " * self.indents[-1])
            self.newline = False
        self.emit_text(txt)
        if txt.endswith("\n"):
            self.newline = True
        self.maybe_indent(txt)
        self.last_token = None

    def emit(self, txt: str | Token) -> None:
        if isinstance(txt, Token):
            self.emit_token(txt)
        elif isinstance(txt, str):
            self.emit_str(txt)
        else:
            assert False

    def start_line(self) -> None:
        if not self.newline:
            self.out.write("\n")
        self.newline = True
        self.last_token = None

    @contextlib.contextmanager
    def header_guard(self, name: str) -> Iterator[None]:
        self.out.write(
            f"""
#ifndef {name}
#define {name}
#ifdef __cplusplus
extern "C" {{
#endif

"""
        )
        yield
        self.out.write(
            f"""
#ifdef __cplusplus
}}
#endif
#endif /* !{name} */
"""
        )


def is_label(txt: str) -> bool:
    return not txt.startswith("//") and txt.endswith(":")


================================================
File: /Tools/cases_generator/generators_common.py
================================================
from pathlib import Path
from typing import TextIO

from analyzer import (
    Instruction,
    Uop,
    Properties,
    StackItem,
    analysis_error,
)
from cwriter import CWriter
from typing import Callable, TextIO, Iterator, Iterable
from lexer import Token
from stack import Storage, StackError

# Set this to true for voluminous output showing state of stack and locals
PRINT_STACKS = False

class TokenIterator:

    look_ahead: Token | None
    iterator: Iterator[Token]

    def __init__(self, tkns: Iterable[Token]):
        self.iterator = iter(tkns)
        self.look_ahead = None

    def __iter__(self) -> "TokenIterator":
        return self

    def __next__(self) -> Token:
        if self.look_ahead is None:
            return next(self.iterator)
        else:
            res = self.look_ahead
            self.look_ahead = None
            return res

    def peek(self) -> Token | None:
        if self.look_ahead is None:
            for tkn in self.iterator:
                self.look_ahead = tkn
                break
        return self.look_ahead

ROOT = Path(__file__).parent.parent.parent.resolve()
DEFAULT_INPUT = (ROOT / "Python/bytecodes.c").as_posix()


def root_relative_path(filename: str) -> str:
    try:
        return Path(filename).resolve().relative_to(ROOT).as_posix()
    except ValueError:
        # Not relative to root, just return original path.
        return filename


def type_and_null(var: StackItem) -> tuple[str, str]:
    if var.type:
        return var.type, "NULL"
    elif var.is_array():
        return "_PyStackRef *", "NULL"
    else:
        return "_PyStackRef", "PyStackRef_NULL"


def write_header(
    generator: str, sources: list[str], outfile: TextIO, comment: str = "//"
) -> None:
    outfile.write(
        f"""{comment} This file is generated by {root_relative_path(generator)}
{comment} from:
{comment}   {", ".join(root_relative_path(src) for src in sources)}
{comment} Do not edit!
"""
    )


def emit_to(out: CWriter, tkn_iter: TokenIterator, end: str) -> Token:
    parens = 0
    for tkn in tkn_iter:
        if tkn.kind == end and parens == 0:
            return tkn
        if tkn.kind == "LPAREN":
            parens += 1
        if tkn.kind == "RPAREN":
            parens -= 1
        out.emit(tkn)
    raise analysis_error(f"Expecting {end}. Reached end of file", tkn)


ReplacementFunctionType = Callable[
    [Token, TokenIterator, Uop, Storage, Instruction | None], bool
]

def always_true(tkn: Token | None) -> bool:
    if tkn is None:
        return False
    return tkn.text in {"true", "1"}


class Emitter:
    out: CWriter
    _replacers: dict[str, ReplacementFunctionType]

    def __init__(self, out: CWriter):
        self._replacers = {
            "EXIT_IF": self.exit_if,
            "DEOPT_IF": self.deopt_if,
            "ERROR_IF": self.error_if,
            "ERROR_NO_POP": self.error_no_pop,
            "DECREF_INPUTS": self.decref_inputs,
            "DEAD": self.kill,
            "INPUTS_DEAD": self.kill_inputs,
            "SYNC_SP": self.sync_sp,
            "SAVE_STACK": self.save_stack,
            "RELOAD_STACK": self.reload_stack,
            "PyStackRef_CLOSE": self.stackref_close,
            "PyStackRef_CLOSE_SPECIALIZED": self.stackref_close,
            "PyStackRef_AsPyObjectSteal": self.stackref_steal,
            "DISPATCH": self.dispatch,
            "INSTRUCTION_SIZE": self.instruction_size,
            "POP_DEAD_INPUTS": self.pop_dead_inputs,
        }
        self.out = out

    def dispatch(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        self.emit(tkn)
        return False

    def deopt_if(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        self.out.emit_at("DEOPT_IF", tkn)
        lparen = next(tkn_iter)
        self.emit(lparen)
        assert lparen.kind == "LPAREN"
        first_tkn = tkn_iter.peek()
        emit_to(self.out, tkn_iter, "RPAREN")
        next(tkn_iter)  # Semi colon
        self.out.emit(", ")
        assert inst is not None
        assert inst.family is not None
        self.out.emit(inst.family.name)
        self.out.emit(");\n")
        return not always_true(first_tkn)

    exit_if = deopt_if

    def error_if(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        lparen = next(tkn_iter)
        assert lparen.kind == "LPAREN"
        first_tkn = tkn_iter.peek()
        unconditional = always_true(first_tkn)
        if unconditional:
            next(tkn_iter)
            comma = next(tkn_iter)
            if comma.kind != "COMMA":
                raise analysis_error(f"Expected comma, got '{comma.text}'", comma)
            self.out.start_line()
        else:
            self.out.emit_at("if ", tkn)
            self.emit(lparen)
            emit_to(self.out, tkn_iter, "COMMA")
            self.out.emit(") ")
        label = next(tkn_iter).text
        next(tkn_iter)  # RPAREN
        next(tkn_iter)  # Semi colon
        storage.clear_inputs("at ERROR_IF")
        c_offset = storage.stack.peek_offset()
        try:
            offset = -int(c_offset)
        except ValueError:
            offset = -1
        if offset > 0:
            self.out.emit(f"goto pop_{offset}_")
            self.out.emit(label)
            self.out.emit(";\n")
        elif offset == 0:
            self.out.emit("goto ")
            self.out.emit(label)
            self.out.emit(";\n")
        else:
            self.out.emit("{\n")
            storage.copy().flush(self.out)
            self.out.emit("goto ")
            self.out.emit(label)
            self.out.emit(";\n")
            self.out.emit("}\n")
        return not unconditional

    def error_no_pop(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        next(tkn_iter)  # LPAREN
        next(tkn_iter)  # RPAREN
        next(tkn_iter)  # Semi colon
        self.out.emit_at("goto error;", tkn)
        return False

    def decref_inputs(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        next(tkn_iter)
        next(tkn_iter)
        next(tkn_iter)
        self.out.emit_at("", tkn)
        for var in uop.stack.inputs:
            if var.name == "unused" or var.name == "null" or var.peek:
                continue
            if var.size:
                if var.size == "1":
                    self.out.emit(f"PyStackRef_CLOSE({var.name}[0]);\n")
                else:
                    self.out.emit(f"for (int _i = {var.size}; --_i >= 0;) {{\n")
                    self.out.emit(f"PyStackRef_CLOSE({var.name}[_i]);\n")
                    self.out.emit("}\n")
            elif var.condition:
                if var.condition == "1":
                    self.out.emit(f"PyStackRef_CLOSE({var.name});\n")
                elif var.condition != "0":
                    self.out.emit(f"PyStackRef_XCLOSE({var.name});\n")
            else:
                self.out.emit(f"PyStackRef_CLOSE({var.name});\n")
        for input in storage.inputs:
            input.defined = False
        return True

    def kill_inputs(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        next(tkn_iter)
        next(tkn_iter)
        next(tkn_iter)
        for var in storage.inputs:
            var.defined = False
        return True

    def kill(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        next(tkn_iter)
        name_tkn = next(tkn_iter)
        name = name_tkn.text
        next(tkn_iter)
        next(tkn_iter)
        for var in storage.inputs:
            if var.name == name:
                var.defined = False
                break
        else:
            raise analysis_error(f"'{name}' is not a live input-only variable", name_tkn)
        return True

    def stackref_close(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        self.out.emit(tkn)
        tkn = next(tkn_iter)
        assert tkn.kind == "LPAREN"
        self.out.emit(tkn)
        name = next(tkn_iter)
        self.out.emit(name)
        if name.kind == "IDENTIFIER":
            for var in storage.inputs:
                if var.name == name.text:
                    var.defined = False
        rparen = emit_to(self.out, tkn_iter, "RPAREN")
        self.emit(rparen)
        return True

    stackref_steal = stackref_close

    def sync_sp(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        next(tkn_iter)
        next(tkn_iter)
        next(tkn_iter)
        storage.clear_inputs("when syncing stack")
        storage.flush(self.out)
        self._print_storage(storage)
        return True

    def emit_save(self, storage: Storage) -> None:
        storage.save(self.out)
        self._print_storage(storage)

    def save_stack(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        next(tkn_iter)
        next(tkn_iter)
        next(tkn_iter)
        self.emit_save(storage)
        return True

    def pop_dead_inputs(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        next(tkn_iter)
        next(tkn_iter)
        next(tkn_iter)
        storage.pop_dead_inputs(self.out)
        return True

    def emit_reload(self, storage: Storage) -> None:
        storage.reload(self.out)
        self._print_storage(storage)

    def reload_stack(
        self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        next(tkn_iter)
        next(tkn_iter)
        next(tkn_iter)
        self.emit_reload(storage)
        return True

    def instruction_size(self,
        tkn: Token,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> bool:
        """Replace the INSTRUCTION_SIZE macro with the size of the current instruction."""
        if uop.instruction_size is None:
            raise analysis_error("The INSTRUCTION_SIZE macro requires uop.instruction_size to be set", tkn)
        self.out.emit(f" {uop.instruction_size} ")
        return True

    def _print_storage(self, storage: Storage) -> None:
        if PRINT_STACKS:
            self.out.start_line()
            self.emit(storage.as_comment())
            self.out.start_line()

    def _emit_if(
        self,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> tuple[bool, Token, Storage]:
        """Returns (reachable?, closing '}', stack)."""
        tkn = next(tkn_iter)
        assert tkn.kind == "LPAREN"
        self.out.emit(tkn)
        rparen = emit_to(self.out, tkn_iter, "RPAREN")
        self.emit(rparen)
        if_storage = storage.copy()
        reachable, rbrace, if_storage = self._emit_block(tkn_iter, uop, if_storage, inst, True)
        try:
            maybe_else = tkn_iter.peek()
            if maybe_else and maybe_else.kind == "ELSE":
                self._print_storage(storage)
                self.emit(rbrace)
                self.emit(next(tkn_iter))
                maybe_if = tkn_iter.peek()
                if maybe_if and maybe_if.kind == "IF":
                    #Emit extra braces around the if to get scoping right
                    self.emit(" {\n")
                    self.emit(next(tkn_iter))
                    else_reachable, rbrace, else_storage = self._emit_if(tkn_iter, uop, storage, inst)
                    self.out.start_line()
                    self.emit("}\n")
                else:
                    else_reachable, rbrace, else_storage = self._emit_block(tkn_iter, uop, storage, inst, True)
                if not reachable:
                    # Discard the if storage
                    reachable = else_reachable
                    storage = else_storage
                elif not else_reachable:
                    # Discard the else storage
                    storage = if_storage
                    reachable = True
                else:
                    if PRINT_STACKS:
                        self.emit("/* Merge */\n")
                    else_storage.merge(if_storage, self.out)
                    storage = else_storage
                    self._print_storage(storage)
            else:
                if reachable:
                    if PRINT_STACKS:
                        self.emit("/* Merge */\n")
                    if_storage.merge(storage, self.out)
                    storage = if_storage
                    self._print_storage(storage)
                else:
                    # Discard the if storage
                    reachable = True
        except StackError as ex:
            self._print_storage(if_storage)
            raise analysis_error(ex.args[0], rbrace) # from None
        return reachable, rbrace, storage

    def _emit_block(
        self,
        tkn_iter: TokenIterator,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
        emit_first_brace: bool
    ) -> tuple[bool, Token, Storage]:
        """ Returns (reachable?, closing '}', stack)."""
        braces = 1
        out_stores = set(uop.output_stores)
        tkn = next(tkn_iter)
        reload: Token | None = None
        try:
            reachable = True
            line : int = -1
            if tkn.kind != "LBRACE":
                raise analysis_error(f"PEP 7: expected '{{', found: {tkn.text}", tkn)
            escaping_calls = uop.properties.escaping_calls
            if emit_first_brace:
                self.emit(tkn)
            self._print_storage(storage)
            for tkn in tkn_iter:
                if PRINT_STACKS and tkn.line != line:
                    self.out.start_line()
                    self.emit(storage.as_comment())
                    self.out.start_line()
                    line = tkn.line
                if tkn in escaping_calls:
                    if tkn != reload:
                        self.emit_save(storage)
                    _, reload = escaping_calls[tkn]
                elif tkn == reload:
                    self.emit_reload(storage)
                if tkn.kind == "LBRACE":
                    self.out.emit(tkn)
                    braces += 1
                elif tkn.kind == "RBRACE":
                    self._print_storage(storage)
                    braces -= 1
                    if braces == 0:
                        return reachable, tkn, storage
                    self.out.emit(tkn)
                elif tkn.kind == "GOTO":
                    reachable = False;
                    self.out.emit(tkn)
                elif tkn.kind == "IDENTIFIER":
                    if tkn.text in self._replacers:
                        if not self._replacers[tkn.text](tkn, tkn_iter, uop, storage, inst):
                            reachable = False
                    else:
                        if tkn in out_stores:
                            for out in storage.outputs:
                                if out.name == tkn.text:
                                    out.defined = True
                                    out.in_memory = False
                                    break
                        if tkn.text.startswith("DISPATCH"):
                            self._print_storage(storage)
                            reachable = False
                        self.out.emit(tkn)
                elif tkn.kind == "IF":
                    self.out.emit(tkn)
                    if_reachable, rbrace, storage = self._emit_if(tkn_iter, uop, storage, inst)
                    if reachable:
                        reachable = if_reachable
                    self.out.emit(rbrace)
                else:
                    self.out.emit(tkn)
        except StackError as ex:
            raise analysis_error(ex.args[0], tkn) from None
        raise analysis_error("Expecting closing brace. Reached end of file", tkn)


    def emit_tokens(
        self,
        uop: Uop,
        storage: Storage,
        inst: Instruction | None,
    ) -> Storage:
        tkn_iter = TokenIterator(uop.body)
        self.out.start_line()
        _, rbrace, storage = self._emit_block(tkn_iter, uop, storage, inst, False)
        try:
            self._print_storage(storage)
            storage.push_outputs()
            self._print_storage(storage)
        except StackError as ex:
            raise analysis_error(ex.args[0], rbrace)
        return storage

    def emit(self, txt: str | Token) -> None:
        self.out.emit(txt)


def cflags(p: Properties) -> str:
    flags: list[str] = []
    if p.oparg:
        flags.append("HAS_ARG_FLAG")
    if p.uses_co_consts:
        flags.append("HAS_CONST_FLAG")
    if p.uses_co_names:
        flags.append("HAS_NAME_FLAG")
    if p.jumps:
        flags.append("HAS_JUMP_FLAG")
    if p.has_free:
        flags.append("HAS_FREE_FLAG")
    if p.uses_locals:
        flags.append("HAS_LOCAL_FLAG")
    if p.eval_breaker:
        flags.append("HAS_EVAL_BREAK_FLAG")
    if p.deopts:
        flags.append("HAS_DEOPT_FLAG")
    if p.side_exit:
        flags.append("HAS_EXIT_FLAG")
    if not p.infallible:
        flags.append("HAS_ERROR_FLAG")
    if p.error_without_pop:
        flags.append("HAS_ERROR_NO_POP_FLAG")
    if p.escapes:
        flags.append("HAS_ESCAPES_FLAG")
    if p.pure:
        flags.append("HAS_PURE_FLAG")
    if p.no_save_ip:
        flags.append("HAS_NO_SAVE_IP_FLAG")
    if p.oparg_and_1:
        flags.append("HAS_OPARG_AND_1_FLAG")
    if flags:
        return " | ".join(flags)
    else:
        return "0"


================================================
File: /Tools/cases_generator/interpreter_definition.md
================================================
# A higher level definition of the bytecode interpreter

## Abstract

The CPython interpreter is defined in C, meaning that the semantics of the
bytecode instructions, the dispatching mechanism, error handling, and
tracing and instrumentation are all intermixed.

This document proposes defining a custom C-like DSL for defining the
instruction semantics and tools for generating the code deriving from
the instruction definitions.

These tools would be used to:
* Generate the main interpreter (done)
* Generate the tier 2 interpreter
* Generate documentation for instructions
* Generate metadata about instructions, such as stack use (done).
* Generate the tier 2 optimizer's abstract interpreter.

Having a single definition file ensures that there is a single source
of truth for bytecode semantics.

Other tools that operate on bytecodes, like `frame.setlineno`
and the `dis` module, will be derived from the common semantic
definition, reducing errors.

## Motivation

The bytecode interpreter of CPython has traditionally been defined as standard
C code, but with a lot of macros.
The presence of these macros and the nature of bytecode interpreters means
that the interpreter is effectively defined in a domain specific language (DSL).

Rather than using an ad-hoc DSL embedded in the C code for the interpreter,
a custom DSL should be defined and the semantics of the bytecode instructions,
and the instructions defined in that DSL.

Generating the interpreter decouples low-level details of dispatching
and error handling from the semantics of the instructions, resulting
in more maintainable code and a potentially faster interpreter.

It also provides the ability to create and check optimizers and optimization
passes from the semantic definition, reducing errors.

## Rationale

As we improve the performance of CPython, we need to optimize larger regions
of code, use more complex optimizations and, ultimately, translate to machine
code.

All of these steps introduce the possibility of more bugs, and require more code
to be written. One way to mitigate this is through the use of code generators.
Code generators decouple the debugging of the code (the generator) from checking
the correctness (the DSL input).

For example, we are likely to want a new interpreter for the tier 2 optimizer
to be added in 3.12. That interpreter will have a different API, a different
set of instructions and potentially different dispatching mechanism.
But the instructions it will interpret will be built from the same building
blocks as the instructions for the tier 1 (PEP 659) interpreter.

Rewriting all the instructions is tedious and error-prone, and changing the
instructions is a maintenance headache as both versions need to be kept in sync.

By using a code generator and using a common source for the instructions, or
parts of instructions, we can reduce the potential for errors considerably.


## Specification

This specification is a work in progress.
We update it as the need arises.

### Syntax

Each op definition has a kind, a name, a stack and instruction stream effect,
and a piece of C code describing its semantics:

```
  file:
    (definition | family | pseudo)+

  definition:
    "inst" "(" NAME ["," stack_effect] ")" "{" C-code "}"
    |
    "op" "(" NAME "," stack_effect ")" "{" C-code "}"
    |
    "macro" "(" NAME ")" "=" uop ("+" uop)* ";"

  stack_effect:
    "(" [inputs] "--" [outputs] ")"

  inputs:
    input ("," input)*

  outputs:
    output ("," output)*

  input:
    object | stream | array

  output:
    object | array

  uop:
    NAME | stream

  object:
    NAME [":" type] [ "if" "(" C-expression ")" ]

  type:
    NAME ["*"]

  stream:
    NAME "/" size

  size:
    INTEGER

  array:
    object "[" C-expression "]"

  family:
    "family" "(" NAME ")" = "{" NAME ("," NAME)+ [","] "}" ";"

  pseudo:
    "pseudo" "(" NAME "," stack_effect ["," "(" flags ")"]")" = "{" NAME ("," NAME)+ [","] "}" ";"

  flags:
    flag ("|" flag)*

  flag:
    HAS_ARG | HAS_DEOPT | etc..
```

The following definitions may occur:

* `inst`: A normal instruction, as previously defined by `TARGET(NAME)` in `ceval.c`.
* `op`: A part instruction from which macros can be constructed.
* `macro`: A bytecode instruction constructed from ops and cache effects.

`NAME` can be any ASCII identifier that is a C identifier and not a C or Python keyword.
`foo_1` is legal. `$` is not legal, nor is `struct` or `class`.

The optional `type` in an `object` is the C type. It defaults to `PyObject *`.
The objects before the "--" are the objects on top of the stack at the start of
the instruction. Those after the "--" are the objects on top of the stack at the
end of the instruction.


An `inst` without `stack_effect` is a transitional form to allow the original C code
definitions to be copied. It lacks information to generate anything other than the
interpreter, but is useful for initial porting of code.

Stack effect names may be `unused`, indicating the space is to be reserved
but no use of it will be made in the instruction definition.
This is useful to ensure that all instructions in a family have the same
stack effect.

The number in a `stream` define how many codeunits are consumed from the
instruction stream. It returns a 16, 32 or 64 bit value.
If the name is `unused` the size can be any value and that many codeunits
will be skipped in the instruction stream.

By convention cache effects (`stream`) must precede the input effects.

The name `oparg` is pre-defined as a 32 bit value fetched from the instruction stream.

### Special instruction annotations

Instruction headers may be prefixed by one or more annotations. The non-exhaustive
list of annotations and their meanings are as follows:

* `override`. For external use by other interpreter definitions to override the current
   instruction definition.
* `pure`. This instruction has no side effects.
* 'tierN'. This instruction is only used by the tier N interpreter.

### Special functions/macros

The C code may include special functions and macros that are understood by the tools as
part of the DSL.

Those include:

* `DEOPT_IF(cond, instruction)`. Deoptimize if `cond` is met.
* `ERROR_IF(cond, label)`. Jump to error handler at `label` if `cond` is true.
* `DECREF_INPUTS()`. Generate `Py_DECREF()` calls for the input stack effects.
* `SYNC_SP()`. Synchronizes the physical stack pointer with the stack effects.
* `INSTRUCTION_SIZE`. Replaced with the size of the instruction which is equal
to `1 + INLINE_CACHE_ENTRIES`.

Note that the use of `DECREF_INPUTS()` is optional -- manual calls
to `Py_DECREF()` or other approaches are also acceptable
(e.g. calling an API that "steals" a reference).

Variables can either be defined in the input, output, or in the C code.
Variables defined in the input may not be assigned in the C code.
If an `ERROR_IF` occurs, all values will be removed from the stack;
they must already be `DECREF`'ed by the code block.
If a `DEOPT_IF` occurs, no values will be removed from the stack or
the instruction stream; no values must have been `DECREF`'ed or created.

These requirements result in the following constraints on the use of
`DEOPT_IF` and `ERROR_IF` in any instruction's code block:

1. Until the last `DEOPT_IF`, no objects may be allocated, `INCREF`ed,
   or `DECREF`ed.
2. Before the first `ERROR_IF`, all input values must be `DECREF`ed,
   and no objects may be allocated or `INCREF`ed, with the exception
   of attempting to create an object and checking for success using
   `ERROR_IF(result == NULL, label)`. (TODO: Unclear what to do with
   intermediate results.)
3. No `DEOPT_IF` may follow an `ERROR_IF` in the same block.

(There is some wiggle room: these rules apply to dynamic code paths,
not to static occurrences in the source code.)

If code detects an error condition before the first `DECREF` of an input,
two idioms are valid:

- Use `goto error`.
- Use a block containing the appropriate `DECREF` calls ending in
  `ERROR_IF(true, error)`.

An example of the latter would be:
```cc
    res = PyObject_Add(left, right);
    if (res == NULL) {
        DECREF_INPUTS();
        ERROR_IF(true, error);
    }
```

### Semantics

The underlying execution model is a stack machine.
Operations pop values from the stack, and push values to the stack.
They also can look at, and consume, values from the instruction stream.

All members of a family
(which represents a specializable instruction and its specializations)
must have the same stack and instruction stream effect.

The same is true for all members of a pseudo instruction
(which is mapped by the bytecode compiler to one of its members).

## Examples

(Another source of examples can be found in the
[tests](https://github.com/python/cpython/blob/main/Lib/test/test_generated_cases.py).)

Some examples:

### Output stack effect
```C
    inst ( LOAD_FAST, (-- value) ) {
        value = frame->f_localsplus[oparg];
        Py_INCREF(value);
    }
```
This would generate:
```C
    TARGET(LOAD_FAST) {
        PyObject *value;
        value = frame->f_localsplus[oparg];
        Py_INCREF(value);
        PUSH(value);
        DISPATCH();
    }
```

### Input stack effect
```C
    inst ( STORE_FAST, (value --) ) {
        SETLOCAL(oparg, value);
    }
```
This would generate:
```C
    TARGET(STORE_FAST) {
        PyObject *value = PEEK(1);
        SETLOCAL(oparg, value);
        STACK_SHRINK(1);
        DISPATCH();
    }
```

### Input stack effect and cache effect
```C
    op ( CHECK_OBJECT_TYPE, (owner, type_version/2 -- owner) ) {
        PyTypeObject *tp = Py_TYPE(owner);
        assert(type_version != 0);
        DEOPT_IF(tp->tp_version_tag != type_version);
    }
```
This might become (if it was an instruction):
```C
    TARGET(CHECK_OBJECT_TYPE) {
        PyObject *owner = PEEK(1);
        uint32 type_version = read32(next_instr);
        PyTypeObject *tp = Py_TYPE(owner);
        assert(type_version != 0);
        DEOPT_IF(tp->tp_version_tag != type_version);
        next_instr += 2;
        DISPATCH();
    }
```

### More examples

For explanations see "Generating the interpreter" below.
```C
    op ( CHECK_HAS_INSTANCE_VALUES, (owner -- owner) ) {
        PyDictOrValues dorv = *_PyObject_DictOrValuesPointer(owner);
        DEOPT_IF(!_PyDictOrValues_IsValues(dorv));
    }
```
```C
    op ( LOAD_INSTANCE_VALUE, (owner, index/1 -- null if (oparg & 1), res) ) {
        res = _PyDictOrValues_GetValues(dorv)->values[index];
        DEOPT_IF(res == NULL);
        Py_INCREF(res);
        null = NULL;
        Py_DECREF(owner);
    }
```
```C
    macro ( LOAD_ATTR_INSTANCE_VALUE ) =
        counter/1 + CHECK_OBJECT_TYPE + CHECK_HAS_INSTANCE_VALUES +
        LOAD_INSTANCE_VALUE + unused/4 ;
```
```C
    op ( LOAD_SLOT, (owner, index/1 -- null if (oparg & 1), res) ) {
        char *addr = (char *)owner + index;
        res = *(PyObject **)addr;
        DEOPT_IF(res == NULL);
        Py_INCREF(res);
        null = NULL;
        Py_DECREF(owner);
    }
```
```C
    macro ( LOAD_ATTR_SLOT ) = counter/1 + CHECK_OBJECT_TYPE + LOAD_SLOT + unused/4;
```
```C
    inst ( BUILD_TUPLE, (items[oparg] -- tuple) ) {
        tuple = _PyTuple_FromArraySteal(items, oparg);
        ERROR_IF(tuple == NULL, error);
    }
```
```C
    inst ( PRINT_EXPR ) {
        PyObject *value = POP();
        PyObject *hook = _PySys_GetAttr(tstate, &_Py_ID(displayhook));
        PyObject *res;
        if (hook == NULL) {
            _PyErr_SetString(tstate, PyExc_RuntimeError,
                                "lost sys.displayhook");
            Py_DECREF(value);
            goto error;
        }
        res = PyObject_CallOneArg(hook, value);
        Py_DECREF(value);
        ERROR_IF(res == NULL);
        Py_DECREF(res);
    }
```

### Defining an instruction family

A _family_ maps a specializable instruction to its specializations.

Example: These opcodes all share the same instruction format:
```C
    family(load_attr) = { LOAD_ATTR, LOAD_ATTR_INSTANCE_VALUE, LOAD_SLOT };
```

### Defining a pseudo instruction

A _pseudo instruction_ is used by the bytecode compiler to represent a set of possible concrete instructions.

Example: `JUMP` may expand to `JUMP_FORWARD` or `JUMP_BACKWARD`:
```C
    pseudo(JUMP) = { JUMP_FORWARD, JUMP_BACKWARD };
```


## Generating the interpreter

The generated C code for a single instruction includes a preamble and dispatch at the end
which can be easily inserted. What is more complex is ensuring the correct stack effects
and not generating excess pops and pushes.

For example, in `CHECK_HAS_INSTANCE_VALUES`, `owner` occurs in the input, so it cannot be
redefined. Thus, it doesn't need to be written and can be read without adjusting the stack pointer.
The C code generated for `CHECK_HAS_INSTANCE_VALUES` would look something like:

```C
    {
        PyObject *owner = stack_pointer[-1];
        PyDictOrValues dorv = *_PyObject_DictOrValuesPointer(owner);
        DEOPT_IF(!_PyDictOrValues_IsValues(dorv));
    }
```

