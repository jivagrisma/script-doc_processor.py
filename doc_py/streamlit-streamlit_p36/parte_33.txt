        fragment_id2 = _fragment(my_function, additional_hash_info="some_hash_info")()
        assert fragment_id1 != fragment_id2

        # countercheck
        fragment_id2 = _fragment(my_function, additional_hash_info="")()
        assert fragment_id1 == fragment_id2

    @patch("streamlit.runtime.fragment.get_script_run_ctx", MagicMock())
    @patch("streamlit.runtime.fragment.show_deprecation_warning")
    def test_calling_experimental_fragment_shows_warning(
        self, patched_show_deprecation_warning
    ):
        @experimental_fragment
        def my_fragment():
            pass

        my_fragment()

        patched_show_deprecation_warning.assert_called_once()

    @patch("streamlit.runtime.fragment.get_script_run_ctx", MagicMock())
    @patch("streamlit.runtime.fragment.show_deprecation_warning")
    def test_calling_fragment_does_not_show_warning(
        self, patched_show_deprecation_warning
    ):
        @fragment
        def my_fragment():
            pass

        my_fragment()

        patched_show_deprecation_warning.assert_not_called()


# TESTS FOR WRITING TO CONTAINERS OUTSIDE AND INSIDE OF FRAGMENT

APP_FUNCTION = Callable[[ELEMENT_PRODUCER], None]


def _run_fragment_writes_to_outside_container_app(
    element_producer: ELEMENT_PRODUCER,
) -> None:
    """App with container outside of fragment."""

    outside_container = st.container()

    @fragment
    def _some_method():
        st.write("Hello")
        # this is forbidden
        with outside_container:
            element_producer()

    _some_method()


def _run_fragment_writes_to_nested_outside_container_app(
    element_producer: ELEMENT_PRODUCER,
) -> None:
    """App with nested container outside of fragment."""
    with st.container():
        outside_container = st.container()

    @fragment
    def _some_method():
        st.write("Hello")
        # this is forbidden
        with outside_container:
            element_producer()

    _some_method()


def _run_fragment_writes_to_nested_outside_container_app2(
    element_producer: ELEMENT_PRODUCER,
) -> None:
    """App with nested container outside of fragment writing from nested container."""
    with st.container():
        outside_container = st.container()

    @fragment
    def _some_method():
        st.write("Hello")
        # this is forbidden
        with outside_container:
            with st.container():
                element_producer()

    _some_method()


def _run_fragment_writes_to_nested_outside_container_app3(
    element_producer: ELEMENT_PRODUCER,
) -> None:
    """App with nested container outside of fragment writing from nested container."""
    with st.container():
        outside_container = st.container()

    @fragment
    def _some_method():
        st.write("Hello")
        with st.container():
            # this is forbidden
            with outside_container:
                element_producer()

    _some_method()


def _run_fragment_writes_to_inside_container_app(
    element_producer: ELEMENT_PRODUCER,
) -> None:
    """App with container inside of fragment."""

    @fragment
    def _some_method():
        inside_container = st.container()

        st.write("Hello")
        with inside_container:
            element_producer()

    _some_method()


def _run_fragment_writes_to_nested_inside_container_app(
    element_producer: ELEMENT_PRODUCER,
) -> None:
    """App with container inside of fragment."""

    @fragment
    def _some_method():
        inside_container = st.container()

        st.write("Hello")
        with st.container():
            with inside_container:
                element_producer()

    _some_method()


outside_container_writing_apps: list[APP_FUNCTION] = [
    _run_fragment_writes_to_outside_container_app,
    _run_fragment_writes_to_nested_outside_container_app,
    _run_fragment_writes_to_nested_outside_container_app2,
    _run_fragment_writes_to_nested_outside_container_app3,
]

inside_container_writing_apps: list[APP_FUNCTION] = [
    _run_fragment_writes_to_inside_container_app,
    _run_fragment_writes_to_nested_inside_container_app,
]

TEST_TUPLE = Tuple[str, APP_FUNCTION, ELEMENT_PRODUCER]


def get_test_tuples(
    app_functions: list[APP_FUNCTION],
    elements: list[tuple[str, Callable[[], DeltaGenerator]]],
) -> list[TEST_TUPLE]:
    """Create a tuple of (name, app-to-run, element-producer), so that each passed app runs with every passed element.

    Parameters
    ----------
    app_functions : list[APP_FUNCTION]
        Functions that run Streamlit elements like they are an app.
    elements : list[tuple[str, Callable[[], DeltaGenerator]]]
        Tuples of (name, element-producer) where name describes the produced element and element_producer is a function that executes a Streamlit element.
    """
    return [
        (_element_producer[0], _app, _element_producer[1])
        for _app in app_functions
        for _element_producer in elements
    ]


class FragmentCannotWriteToOutsidePathTest(DeltaGeneratorTestCase):
    @parameterized.expand(
        get_test_tuples(outside_container_writing_apps, WIDGET_ELEMENTS)
    )
    def test_write_element_outside_container_raises_exception_for_widgets(
        self,
        _: str,  # the test name argument used by pytest
        _app: Callable[[Callable[[], DeltaGenerator]], None],
        _element_producer: ELEMENT_PRODUCER,
    ):
        with pytest.raises(FragmentHandledException) as ex:
            _app(_element_producer)

        inner_exception = ex.value.__cause__ or ex.value.__context__

        assert isinstance(
            inner_exception, StreamlitFragmentWidgetsNotAllowedOutsideError
        )

    @parameterized.expand(
        get_test_tuples(outside_container_writing_apps, NON_WIDGET_ELEMENTS)
    )
    def test_write_element_outside_container_succeeds_for_nonwidgets(
        self,
        _: str,  # the test name argument used by pytest
        _app: Callable[[Callable[[], DeltaGenerator]], None],
        element_producer: ELEMENT_PRODUCER,
    ):
        _app(element_producer)

    @parameterized.expand(
        get_test_tuples(
            inside_container_writing_apps, WIDGET_ELEMENTS + NON_WIDGET_ELEMENTS
        )
    )
    def test_write_elements_inside_container_succeeds_for_all(
        self,
        _: str,  # the test name argument used by pytest
        _app: Callable[[Callable[[], DeltaGenerator]], None],
        element_producer: ELEMENT_PRODUCER,
    ):
        _app(element_producer)


================================================
File: /lib/tests/streamlit/runtime/media_file_manager_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Unit tests for MediaFileManager"""

from __future__ import annotations

import random
import unittest
from unittest import TestCase, mock
from unittest.mock import MagicMock, call, mock_open

from streamlit.runtime.media_file_manager import MediaFileManager
from streamlit.runtime.media_file_storage import MediaFileKind
from streamlit.runtime.memory_media_file_storage import (
    MemoryFile,
    MemoryMediaFileStorage,
    _calculate_file_id,
)
from tests.exception_capturing_thread import call_on_threads


def random_coordinates():
    return f"{random.randint(1, 4)}.{(random.randint(1, 12), random.randint(1, 12))}.{random.randint(1, 99)}"


# Smallest possible "real" media files for a handful of different formats.
# Sourced from https://github.com/mathiasbynens/small
AUDIO_FIXTURES = {
    "wav": {
        "content": b"RIFF$\x00\x00\x00WAVEfmt \x10\x00\x00\x00\x01\x00\x01\x00D\xac\x00\x00\x88X\x01\x00\x02\x00\x10\x00data\x00\x00\x00\x00",
        "mimetype": "audio/wav",
    },
    "mp3": {
        "content": b"\xff\xe3\x18\xc4\x00\x00\x00\x03H\x00\x00\x00\x00LAME3.98.2\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00",
        "mimetype": "audio/mp3",
    },
}


VIDEO_FIXTURES = {
    "mp4": {
        "content": b"\x00\x00\x00\x1cftypisom\x00\x00\x02\x00isomiso2mp41\x00\x00\x00\x08free\x00\x00\x02\xefmdat!\x10\x05",
        "mimetype": "video/mp4",
    },
    "webm": {
        "content": b'\x1aE\xdf\xa3@ B\x86\x81\x01B\xf7\x81\x01B\xf2\x81\x04B\xf3\x81\x08B\x82@\x04webmB\x87\x81\x02B\x85\x81\x02\x18S\x80g@\x8d\x15I\xa9f@(*\xd7\xb1@\x03\x0fB@M\x80@\x06whammyWA@\x06whammyD\x89@\x08@\x8f@\x00\x00\x00\x00\x00\x16T\xaek@1\xae@.\xd7\x81\x01c\xc5\x81\x01\x9c\x81\x00"\xb5\x9c@\x03und\x86@\x05V_VP8%\x86\x88@\x03VP8\x83\x81\x01\xe0@\x06\xb0\x81\x08\xba\x81\x08\x1fC\xb6u@"\xe7\x81\x00\xa3@\x1c\x81\x00\x00\x800\x01\x00\x9d\x01*\x08\x00\x08\x00\x01@&%\xa4\x00\x03p\x00\xfe\xfc\xf4\x00\x00',
        "mimetype": "video/webm",
    },
}


IMAGE_FIXTURES = {
    "png": {
        "content": b"\x89PNG\r\n\x1a\n\x00\x00\x00\rIHDR\x00\x00\x00\x01\x00\x00\x00\x01\x08\x06\x00\x00\x00\x1f\x15\xc4\x89\x00\x00\x00\nIDATx\x9cc\x00\x01\x00\x00\x05\x00\x01\r\n-\xb4\x00\x00\x00\x00IEND\xaeB`\x82",
        "mimetype": "image/png",
    },
    "jpg": {
        "content": b"\xff\xd8\xff\xdb\x00C\x00\x03\x02\x02\x02\x02\x02\x03\x02\x02\x02\x03\x03\x03\x03\x04\x06\x04\x04\x04\x04\x04\x08\x06\x06\x05\x06\t\x08\n\n\t\x08\t\t\n\x0c\x0f\x0c\n\x0b\x0e\x0b\t\t\r\x11\r\x0e\x0f\x10\x10\x11\x10\n\x0c\x12\x13\x12\x10\x13\x0f\x10\x10\x10\xff\xc9\x00\x0b\x08\x00\x01\x00\x01\x01\x01\x11\x00\xff\xcc\x00\x06\x00\x10\x10\x05\xff\xda\x00\x08\x01\x01\x00\x00?\x00\xd2\xcf \xff\xd9",
        "mimetype": "image/jpg",
    },
}

TEXT_FIXTURES = {
    "txt": {"content": b"Hello world", "mimetype": "text/plain"},
    "csv": {
        "content": b"""
                    Foo, Bar
                    123, 456
                    789, 111""",
        "mimetype": "text/csv",
    },
}

ALL_FIXTURES = {}
ALL_FIXTURES.update(AUDIO_FIXTURES)
ALL_FIXTURES.update(VIDEO_FIXTURES)
ALL_FIXTURES.update(IMAGE_FIXTURES)
ALL_FIXTURES.update(TEXT_FIXTURES)


class MediaFileManagerTest(TestCase):
    def setUp(self):
        super().setUp()
        self.storage = MemoryMediaFileStorage("/mock/endpoint")
        self.media_file_manager = MediaFileManager(self.storage)
        random.seed(1337)

    def _add_file_and_get_object(
        self,
        content: bytes,
        mimetype: str,
        coordinates: str,
        filename: str | None = None,
    ) -> MemoryFile:
        """Add a new file to our test manager and return its MediaFile object."""
        file_id = _calculate_file_id(content, mimetype, filename)
        self.media_file_manager.add(content, mimetype, coordinates, filename)
        return self.storage.get_file(file_id)

    def test_calculate_file_id(self):
        """Test that file_id generation from data works as expected."""

        fake_bytes = "\x00\x00\xff\x00\x00\xff\x00\x00\xff\x00\x00\xff\x00".encode()
        test_hash = "2ba850426b188d25adc5a37ad313080c346f5e88e069e0807d0cdb2b"
        self.assertEqual(test_hash, _calculate_file_id(fake_bytes, "media/any"))

        # Make sure we get different file ids for files with same bytes but diff't mimetypes.
        self.assertNotEqual(
            _calculate_file_id(fake_bytes, "audio/wav"),
            _calculate_file_id(fake_bytes, "video/mp4"),
        )

        # Make sure we get different file ids for files with same bytes and mimetypes but diff't filenames.
        self.assertNotEqual(
            _calculate_file_id(fake_bytes, "audio/wav", filename="name1.wav"),
            _calculate_file_id(fake_bytes, "audio/wav", filename="name2.wav"),
        )

    @mock.patch(
        "streamlit.runtime.media_file_manager._get_session_id",
        MagicMock(return_value="mock_session_id"),
    )
    def test_reject_null_files(self):
        """MediaFileManager.add raises a TypeError if it's passed None."""
        with self.assertRaises(TypeError):
            self.media_file_manager.add(None, "media/any", random_coordinates())

    @mock.patch(
        "streamlit.runtime.media_file_manager._get_session_id",
        MagicMock(return_value="mock_session"),
    )
    def test_add_binary_files(self):
        """Test that we can add binary files to the manager."""
        storage_load_spy = MagicMock(side_effect=self.storage.load_and_get_id)
        self.storage.load_and_get_id = storage_load_spy

        sample_coords = set()
        while len(sample_coords) < len(ALL_FIXTURES):
            sample_coords.add(random_coordinates())

        for sample in ALL_FIXTURES.values():
            content = sample["content"]
            self.assertIsInstance(content, bytes)
            mimetype = sample["mimetype"]
            media_file = self._add_file_and_get_object(
                content, mimetype, sample_coords.pop()
            )
            self.assertIsNotNone(media_file)

            # Ensure MediaFileStorage.load_and_get_id was called as expected.
            storage_load_spy.assert_called_once_with(
                content, mimetype, MediaFileKind.MEDIA, None
            )
            storage_load_spy.reset_mock()

        # There should be as many files in MFM as we added.
        self.assertEqual(len(self.media_file_manager._file_metadata), len(ALL_FIXTURES))

        # There should only be 1 session with registered files.
        self.assertEqual(len(self.media_file_manager._files_by_session_and_coord), 1)

    @mock.patch(
        "streamlit.runtime.media_file_manager._get_session_id",
        MagicMock(return_value="mock_session"),
    )
    @mock.patch(
        "streamlit.runtime.memory_media_file_storage.open",
        mock_open(read_data=b"mock_test_file"),
        create=True,
    )
    def test_add_file_by_name(self):
        """Test that we can add files by filename."""
        storage_load_spy = MagicMock(side_effect=self.storage.load_and_get_id)
        self.storage.load_and_get_id = storage_load_spy

        self.media_file_manager.add(
            "mock/file/path.png", "image/png", random_coordinates()
        )

        # We should have a single file in the MFM.
        self.assertEqual(len(self.media_file_manager._file_metadata), 1)

        # And it should be registered to our session
        self.assertEqual(
            len(self.media_file_manager._files_by_session_and_coord["mock_session"]), 1
        )

        # Ensure MediaFileStorage.load_and_get_id was called as expected.
        storage_load_spy.assert_called_once_with(
            "mock/file/path.png", "image/png", MediaFileKind.MEDIA, None
        )

    @mock.patch(
        "streamlit.runtime.media_file_manager._get_session_id",
        MagicMock(return_value="mock_session_id"),
    )
    def test_add_files_same_coord(self):
        """We can add multiple files that share the same coordinate."""
        coord = random_coordinates()

        for sample in ALL_FIXTURES.values():
            self.media_file_manager.add(sample["content"], sample["mimetype"], coord)

        # There should be 6 files in MFM.
        self.assertEqual(len(self.media_file_manager._file_metadata), len(ALL_FIXTURES))

        # There should only be 1 session with registered files.
        self.assertEqual(len(self.media_file_manager._files_by_session_and_coord), 1)

        # There should only be 1 coord in that session.
        self.assertEqual(
            len(self.media_file_manager._files_by_session_and_coord["mock_session_id"]),
            1,
        )

        self.media_file_manager.clear_session_refs()
        self.media_file_manager.remove_orphaned_files()

        # There should be only 0 file in MFM.
        self.assertEqual(len(self.media_file_manager._file_metadata), 0)

        # There should only be 0 session with registered files.
        self.assertEqual(len(self.media_file_manager._files_by_session_and_coord), 0)

    @mock.patch(
        "streamlit.runtime.media_file_manager._get_session_id",
        MagicMock(return_value="mock_session_id"),
    )
    def test_add_file_already_exists_same_coord(self):
        """Adding a file that already exists results in just a single file in
        the manager.
        """
        sample = IMAGE_FIXTURES["png"]
        coord = random_coordinates()

        self.media_file_manager.add(sample["content"], sample["mimetype"], coord)
        file_id = _calculate_file_id(sample["content"], sample["mimetype"])
        self.assertTrue(file_id in self.media_file_manager._file_metadata)

        self.media_file_manager.add(sample["content"], sample["mimetype"], coord)
        self.assertTrue(file_id in self.media_file_manager._file_metadata)

        # There should only be 1 file in MFM.
        self.assertEqual(len(self.media_file_manager._file_metadata), 1)

        # There should only be 1 session with registered files.
        self.assertEqual(len(self.media_file_manager._files_by_session_and_coord), 1)

    @mock.patch(
        "streamlit.runtime.media_file_manager._get_session_id",
        MagicMock(return_value="mock_session_id"),
    )
    def test_add_file_already_exists_different_coord(self):
        """Adding a file that already exists, but with different coordinates,
        results in just a single file in the manager.
        """
        sample = IMAGE_FIXTURES["png"]

        coord = random_coordinates()
        self.media_file_manager.add(sample["content"], sample["mimetype"], coord)
        file_id = _calculate_file_id(sample["content"], sample["mimetype"])
        self.assertTrue(file_id in self.media_file_manager._file_metadata)

        coord = random_coordinates()
        self.media_file_manager.add(sample["content"], sample["mimetype"], coord)
        self.assertTrue(file_id in self.media_file_manager._file_metadata)

        # There should only be 1 file in MFM.
        self.assertEqual(len(self.media_file_manager._file_metadata), 1)

        # There should only be 1 session with registered files.
        self.assertEqual(len(self.media_file_manager._files_by_session_and_coord), 1)

    @mock.patch(
        "streamlit.runtime.media_file_manager._get_session_id",
        MagicMock(return_value="mock_session_id"),
    )
    def test_remove_orphaned_files_in_empty_manager(self):
        """Calling clear_session_refs/remove_orphaned_files in an empty manager
        is a no-op.
        """
        storage_delete_spy = MagicMock(side_effect=self.storage.delete_file)
        self.storage.delete_file = storage_delete_spy

        self.assertEqual(len(self.media_file_manager._file_metadata), 0)
        self.assertEqual(len(self.media_file_manager._files_by_session_and_coord), 0)

        self.media_file_manager.clear_session_refs()
        self.media_file_manager.remove_orphaned_files()

        self.assertEqual(len(self.media_file_manager._file_metadata), 0)
        self.assertEqual(len(self.media_file_manager._files_by_session_and_coord), 0)

        # MediaFileStorage.delete_file should not have been called, because
        # no files were actually deleted.
        storage_delete_spy.assert_not_called()

    @mock.patch("streamlit.runtime.media_file_manager._get_session_id")
    def test_remove_orphaned_files_multiple_sessions(self, mock_get_session_id):
        """clear_session_refs/remove_orphaned_files behaves correctly when multiple
        sessions are referencing some of the same files.
        """
        storage_delete_spy = MagicMock(side_effect=self.storage.delete_file)
        self.storage.delete_file = storage_delete_spy

        # Have two sessions add the same set of files
        for session_id in ("mock_session_1", "mock_session_2"):
            mock_get_session_id.return_value = session_id
            for sample in VIDEO_FIXTURES.values():
                coord = random_coordinates()
                self.media_file_manager.add(
                    sample["content"], sample["mimetype"], coord
                )

        self.assertEqual(
            len(self.media_file_manager._file_metadata), len(VIDEO_FIXTURES)
        )

        file_ids = list(self.media_file_manager._file_metadata.keys())

        # Remove session1's references
        mock_get_session_id.return_value = "mock_session_1"
        self.media_file_manager.clear_session_refs()
        self.media_file_manager.remove_orphaned_files()

        # The files are all still referenced by session_2
        self.assertEqual(
            len(self.media_file_manager._file_metadata), len(VIDEO_FIXTURES)
        )

        # MediaFileStorage.delete_file should not have been called yet...
        storage_delete_spy.assert_not_called()

        # Remove session2's references, but don't call "remove_orphaned_files" yet...
        mock_get_session_id.return_value = "mock_session_2"
        self.media_file_manager.clear_session_refs()

        # The files still exist, because they've only been de-referenced and not
        # removed.
        self.assertEqual(
            len(self.media_file_manager._file_metadata), len(VIDEO_FIXTURES)
        )

        # MediaFileStorage.delete_file should not have been called yet...
        storage_delete_spy.assert_not_called()

        # After a final call to remove_orphaned_files, the files should be gone.
        self.media_file_manager.remove_orphaned_files()
        self.assertEqual(len(self.media_file_manager._file_metadata), 0)

        # MediaFileStorage.delete_file should have been called once for each
        # file.
        storage_delete_spy.assert_has_calls(
            [call(file_id) for file_id in file_ids], any_order=True
        )


class MediaFileManagerThreadingTest(unittest.TestCase):
    # The number of threads to run our tests on
    NUM_THREADS = 50

    def setUp(self):
        super().setUp()
        self.storage = MemoryMediaFileStorage("/mock/endpoint")
        self.media_file_manager = MediaFileManager(self.storage)
        random.seed(1337)

    @mock.patch(
        "streamlit.runtime.media_file_manager._get_session_id",
        MagicMock(return_value="mock_session_id"),
    )
    def test_add_file_multiple_threads(self):
        """We can safely call `add` from multiple threads simultaneously."""

        def add_file(ii: int) -> None:
            coord = random_coordinates()
            data = bytes(f"{ii}", "utf-8")
            self.media_file_manager.add(data, "image/png", coord)

        call_on_threads(add_file, num_threads=self.NUM_THREADS)
        self.assertEqual(self.NUM_THREADS, len(self.media_file_manager._file_metadata))

    @mock.patch(
        "streamlit.runtime.media_file_manager._get_session_id",
        MagicMock(return_value="mock_session_id"),
    )
    def test_clear_files_multiple_threads(self):
        """We can safely clear session refs and remove orphaned files
        from multiple threads simultaneously.
        """
        # Add a bunch of files
        for sample in ALL_FIXTURES.values():
            self.media_file_manager.add(
                sample["content"], sample["mimetype"], random_coordinates()
            )
        self.assertEqual(len(ALL_FIXTURES), len(self.media_file_manager._file_metadata))

        # Remove those files from multiple threads
        def remove_files(_: int) -> None:
            self.media_file_manager.clear_session_refs("mock_session_id")
            self.media_file_manager.remove_orphaned_files()

        call_on_threads(remove_files, num_threads=self.NUM_THREADS)

        # Our files should be gone!
        self.assertEqual(0, len(self.media_file_manager._file_metadata))


================================================
File: /lib/tests/streamlit/runtime/memory_media_file_storage_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Unit tests for MemoryMediaFileStorage"""

from __future__ import annotations

import unittest
from unittest import mock
from unittest.mock import MagicMock, mock_open

from parameterized import parameterized

from streamlit.runtime.media_file_storage import MediaFileKind, MediaFileStorageError
from streamlit.runtime.memory_media_file_storage import (
    MemoryFile,
    MemoryMediaFileStorage,
    get_extension_for_mimetype,
)


class MemoryMediaFileStorageTest(unittest.TestCase):
    def setUp(self):
        super().setUp()
        self.storage = MemoryMediaFileStorage(media_endpoint="/mock/media")

    @mock.patch(
        "streamlit.runtime.memory_media_file_storage.open",
        mock_open(read_data=b"mock_bytes"),
    )
    def test_load_with_path(self):
        """Adding a file by path creates a MemoryFile instance."""
        file_id = self.storage.load_and_get_id(
            "mock/file/path",
            mimetype="video/mp4",
            kind=MediaFileKind.MEDIA,
            filename="file.mp4",
        )
        self.assertEqual(
            MemoryFile(
                content=b"mock_bytes",
                mimetype="video/mp4",
                kind=MediaFileKind.MEDIA,
                filename="file.mp4",
            ),
            self.storage.get_file(file_id),
        )

    def test_load_with_bytes(self):
        """Adding a file with bytes creates a MemoryFile instance."""
        file_id = self.storage.load_and_get_id(
            b"mock_bytes",
            mimetype="video/mp4",
            kind=MediaFileKind.MEDIA,
            filename="file.mp4",
        )
        self.assertEqual(
            MemoryFile(
                content=b"mock_bytes",
                mimetype="video/mp4",
                kind=MediaFileKind.MEDIA,
                filename="file.mp4",
            ),
            self.storage.get_file(file_id),
        )

    def test_identical_files_have_same_id(self):
        """Two files with the same content, mimetype, and filename should share an ID."""
        # Create 2 identical files. We'll just get one ID.
        file_id1 = self.storage.load_and_get_id(
            b"mock_bytes",
            mimetype="video/mp4",
            kind=MediaFileKind.MEDIA,
            filename="file.mp4",
        )
        file_id2 = self.storage.load_and_get_id(
            b"mock_bytes",
            mimetype="video/mp4",
            kind=MediaFileKind.MEDIA,
            filename="file.mp4",
        )
        self.assertEqual(file_id1, file_id2)

        # Change file content -> different ID
        changed_content = self.storage.load_and_get_id(
            b"mock_bytes_2",
            mimetype="video/mp4",
            kind=MediaFileKind.MEDIA,
            filename="file.mp4",
        )
        self.assertNotEqual(file_id1, changed_content)

        # Change mimetype -> different ID
        changed_mimetype = self.storage.load_and_get_id(
            b"mock_bytes",
            mimetype="image/png",
            kind=MediaFileKind.MEDIA,
            filename="file.mp4",
        )
        self.assertNotEqual(file_id1, changed_mimetype)

        # Change (or omit) filename -> different ID
        changed_filename = self.storage.load_and_get_id(
            b"mock_bytes", mimetype="video/mp4", kind=MediaFileKind.MEDIA
        )
        self.assertNotEqual(file_id1, changed_filename)

    @mock.patch(
        "streamlit.runtime.memory_media_file_storage.open",
        MagicMock(side_effect=Exception),
    )
    def test_load_with_bad_path(self):
        """Adding a file by path raises a MediaFileStorageError if the file can't be read."""
        with self.assertRaises(MediaFileStorageError):
            self.storage.load_and_get_id(
                "mock/file/path",
                mimetype="video/mp4",
                kind=MediaFileKind.MEDIA,
                filename="file.mp4",
            )

    @parameterized.expand(
        [
            ("video/mp4", ".mp4"),
            ("audio/wav", ".wav"),
            ("image/png", ".png"),
            ("image/jpeg", ".jpg"),
        ]
    )
    def test_get_url(self, mimetype, extension):
        """URLs should be formatted correctly, and have the expected extension."""
        file_id = self.storage.load_and_get_id(
            b"mock_bytes", mimetype=mimetype, kind=MediaFileKind.MEDIA
        )
        url = self.storage.get_url(file_id)
        self.assertEqual(f"/mock/media/{file_id}{extension}", url)

    def test_get_url_invalid_fileid(self):
        """get_url raises if it gets a bad file_id."""
        with self.assertRaises(MediaFileStorageError):
            self.storage.get_url("not_a_file_id")

    def test_delete_file(self):
        """delete_file removes the file with the given ID."""
        file_id1 = self.storage.load_and_get_id(
            b"mock_bytes_1",
            mimetype="video/mp4",
            kind=MediaFileKind.MEDIA,
            filename="file.mp4",
        )
        file_id2 = self.storage.load_and_get_id(
            b"mock_bytes_2",
            mimetype="video/mp4",
            kind=MediaFileKind.MEDIA,
            filename="file.mp4",
        )

        # delete file 1. It should not exist, but file2 should.
        self.storage.delete_file(file_id1)
        with self.assertRaises(MediaFileStorageError):
            self.storage.get_file(file_id1)

        self.assertIsNotNone(self.storage.get_file(file_id2))

        # delete file 2
        self.storage.delete_file(file_id2)
        with self.assertRaises(MediaFileStorageError):
            self.storage.get_file(file_id2)

    def test_delete_invalid_file_is_a_noop(self):
        """deleting a file that doesn't exist doesn't raise an error."""
        self.storage.delete_file("mock_file_id")

    def test_cache_stats(self):
        """Test our CacheStatsProvider implementation."""
        self.assertEqual(0, len(self.storage.get_stats()))

        # Add several files to storage. We'll unique-ify them by filename.
        mock_data = b"some random mock binary data"
        num_files = 5
        for ii in range(num_files):
            self.storage.load_and_get_id(
                mock_data,
                mimetype="video/mp4",
                kind=MediaFileKind.MEDIA,
                filename=f"{ii}.mp4",
            )

        stats = self.storage.get_stats()
        self.assertEqual(len(stats), 1)
        self.assertEqual("st_memory_media_file_storage", stats[0].category_name)
        self.assertEqual(
            len(mock_data) * num_files, sum(stat.byte_length for stat in stats)
        )

        # Remove files, and ensure our cache doesn't report they still exist
        for file_id in list(self.storage._files_by_id.keys()):
            self.storage.delete_file(file_id)

        self.assertEqual(0, len(self.storage.get_stats()))


class MemoryMediaFileStorageUtilTest(unittest.TestCase):
    """Unit tests for utility functions in memory_media_file_storage.py"""

    @parameterized.expand(
        [
            ("video/mp4", ".mp4"),
            ("audio/wav", ".wav"),
            ("image/png", ".png"),
            ("image/jpeg", ".jpg"),
        ]
    )
    def test_get_extension_for_mimetype(self, mimetype: str, expected_extension: str):
        result = get_extension_for_mimetype(mimetype)
        self.assertEqual(expected_extension, result)


================================================
File: /lib/tests/streamlit/runtime/memory_session_storage_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import unittest
from unittest.mock import MagicMock

from cachetools import TTLCache

from streamlit.runtime.memory_session_storage import MemorySessionStorage


class MemorySessionStorageTest(unittest.TestCase):
    """Test MemorySessionStorage.

    These tests are intentionally extremely simple to ensure that we don't just end up
    testing cachetools.TTLCache. We try to just verify that we've wrapped TTLCache
    correctly, and in particular we avoid testing cache expiry functionality.
    """

    def test_uses_ttl_cache(self):
        """Verify that the backing cache of a MemorySessionStorage is a TTLCache.

        We do this because we're intentionally avoiding writing tests around cache
        expiry because the cachetools library should do this for us. In the case
        that the backing cache for a MemorySessionStorage ever changes, we'll likely be
        responsible for adding our own tests.
        """
        store = MemorySessionStorage()
        self.assertIsInstance(store._cache, TTLCache)

    def test_get(self):
        store = MemorySessionStorage()
        store._cache["foo"] = "bar"

        self.assertEqual(store.get("foo"), "bar")
        self.assertEqual(store.get("baz"), None)

    def test_save(self):
        store = MemorySessionStorage()
        session_info = MagicMock()
        session_info.session.id = "foo"

        store.save(session_info)
        self.assertEqual(store.get("foo"), session_info)

    def test_delete(self):
        store = MemorySessionStorage()
        store._cache["foo"] = "bar"

        store.delete("foo")
        self.assertEqual(store.get("foo"), None)

    def test_list(self):
        store = MemorySessionStorage()
        store._cache["foo"] = "bar"
        store._cache["baz"] = "qux"

        self.assertEqual(store.list(), ["bar", "qux"])


================================================
File: /lib/tests/streamlit/runtime/metrics_util_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import contextlib
import datetime
import unittest
from collections import Counter
from typing import Any, Callable
from unittest.mock import MagicMock, mock_open, patch

import pandas as pd
from parameterized import parameterized

import streamlit as st
import streamlit.components.v1 as components
from streamlit.connections import SnowparkConnection, SQLConnection
from streamlit.runtime import metrics_util
from streamlit.runtime.caching import cache_data_api, cache_resource_api
from streamlit.runtime.scriptrunner import get_script_run_ctx, magic_funcs
from streamlit.web.server import websocket_headers
from tests.delta_generator_test_case import DeltaGeneratorTestCase

MAC = "mac"
UUID = "uuid"
FILENAME = "/some/id/file"
mock_get_path = MagicMock(return_value=FILENAME)


class MetricsUtilTest(unittest.TestCase):
    def setUp(self):
        self.patch1 = patch("streamlit.file_util.os.stat")
        self.os_stat = self.patch1.start()

    def tearDown(self):
        self.patch1.stop()

    def test_machine_id_v3_from_etc(self):
        """Test getting the machine id from /etc"""
        file_data = "etc"

        with patch(
            "streamlit.runtime.metrics_util.uuid.getnode", return_value=MAC
        ), patch(
            "streamlit.runtime.metrics_util.open",
            mock_open(read_data=file_data),
            create=True,
        ), patch(
            "streamlit.runtime.metrics_util.os.path.isfile",
            side_effect=lambda path: path == "/etc/machine-id",
        ):
            machine_id = metrics_util._get_machine_id_v3()
        assert machine_id == file_data

    def test_machine_id_v3_from_dbus(self):
        """Test getting the machine id from /var/lib/dbus"""
        file_data = "dbus"

        with patch(
            "streamlit.runtime.metrics_util.uuid.getnode", return_value=MAC
        ), patch(
            "streamlit.runtime.metrics_util.open",
            mock_open(read_data=file_data),
            create=True,
        ), patch(
            "streamlit.runtime.metrics_util.os.path.isfile",
            side_effect=lambda path: path == "/var/lib/dbus/machine-id",
        ):
            machine_id = metrics_util._get_machine_id_v3()
        assert machine_id == file_data

    def test_machine_id_v3_from_node(self):
        """Test getting the machine id as the mac address"""

        with patch(
            "streamlit.runtime.metrics_util.uuid.getnode", return_value=MAC
        ), patch("streamlit.runtime.metrics_util.os.path.isfile", return_value=False):
            machine_id = metrics_util._get_machine_id_v3()
        assert machine_id == MAC


class PageTelemetryTest(DeltaGeneratorTestCase):
    def setUp(self):
        super().setUp()
        ctx = get_script_run_ctx()
        assert ctx is not None

        ctx.reset()
        ctx.gather_usage_stats = True

    @parameterized.expand(
        [
            (10, "int"),
            (0.01, "float"),
            (True, "bool"),
            (None, "NoneType"),
            (["1"], "list"),
            ({"foo": "bar"}, "dict"),
            ("foo", "str"),
            (datetime.date.today(), "datetime.date"),
            (datetime.datetime.today().time(), "datetime.time"),
            (pd.DataFrame(), "DataFrame"),
            (pd.Series(), "PandasSeries"),
            # Also support classes as input
            (datetime.date, "datetime.date"),
            (pd.DataFrame, "DataFrame"),
            (SnowparkConnection, "SnowparkConnection"),
            (SQLConnection, "SQLConnection"),
        ]
    )
    def test_get_type_name(self, obj: object, expected_type: str):
        """Test getting the type name via _get_type_name"""
        assert metrics_util._get_type_name(obj) == expected_type

    def test_get_command_telemetry(self):
        """Test getting command telemetry via _get_command_telemetry."""
        # Test with dataframe command:
        command_metadata = metrics_util._get_command_telemetry(
            st.dataframe, "dataframe", pd.DataFrame(), width=250
        )

        assert command_metadata.name == "dataframe"
        assert len(command_metadata.args) == 2
        assert (
            str(command_metadata.args[0]).strip()
            == 'k: "data"\nt: "DataFrame"\nm: "len:0"'
        )
        assert str(command_metadata.args[1]).strip() == 'k: "width"\nt: "int"'

        # Test with text_input command:
        command_metadata = metrics_util._get_command_telemetry(
            st.text_input, "text_input", label="text input", value="foo", disabled=True
        )

        assert command_metadata.name == "text_input"
        assert len(command_metadata.args) == 3
        assert (
            str(command_metadata.args[0]).strip() == 'k: "label"\nt: "str"\nm: "len:10"'
        )
        assert (
            str(command_metadata.args[1]).strip() == 'k: "value"\nt: "str"\nm: "len:3"'
        )
        assert (
            str(command_metadata.args[2]).strip()
            == 'k: "disabled"\nt: "bool"\nm: "val:True"'
        )

    def test_create_page_profile_message(self):
        """Test creating the page profile message via create_page_profile_message."""
        forward_msg = metrics_util.create_page_profile_message(
            commands=[
                metrics_util._get_command_telemetry(
                    st.dataframe, "dataframe", pd.DataFrame(), width=250
                )
            ],
            exec_time=1000,
            prep_time=2000,
        )

        assert len(forward_msg.page_profile.commands) == 1
        assert forward_msg.page_profile.exec_time == 1000
        assert forward_msg.page_profile.prep_time == 2000
        assert forward_msg.page_profile.commands[0].name == "dataframe"
        assert not forward_msg.page_profile.is_fragment_run

    def test_create_page_profile_message_is_fragment_run(self):
        ctx = get_script_run_ctx()
        ctx.fragment_ids_this_run = ["some_fragment_id"]

        forward_msg = metrics_util.create_page_profile_message(
            commands=[
                metrics_util._get_command_telemetry(
                    st.dataframe, "dataframe", pd.DataFrame(), width=250
                )
            ],
            exec_time=1000,
            prep_time=2000,
        )

        assert forward_msg.page_profile.is_fragment_run

    def test_gather_metrics_decorator(self):
        """The gather_metrics decorator works as expected."""
        ctx = get_script_run_ctx()
        assert ctx is not None

        @metrics_util.gather_metrics("test_function")
        def test_function(param1: int, param2: str, param3: float = 0.1) -> str:
            st.markdown("This command should not be tracked")
            st.text_input("This command should also not be tracked")
            st.text("This command should also not be tracked")
            return "foo"

        test_function(param1=10, param2="foobar")

        assert len(ctx.tracked_commands) == 1
        assert ctx.tracked_commands[0].name.endswith("test_function")
        assert ctx.tracked_commands[0].name.startswith("external:")

        st.markdown("This function should be tracked")

        assert len(ctx.tracked_commands) == 2
        assert ctx.tracked_commands[0].name.endswith("test_function")
        assert ctx.tracked_commands[0].name.startswith("external:")
        assert ctx.tracked_commands[1].name == "markdown"

        ctx.reset()
        # Deactivate usage stats gathering
        ctx.gather_usage_stats = False

        assert len(ctx.tracked_commands) == 0
        test_function(param1=10, param2="foobar")
        assert len(ctx.tracked_commands) == 0

    @parameterized.expand(
        [
            (magic_funcs.transparent_write, "magic"),
            (st.cache_data.clear, "clear_data_caches"),
            (st.cache_resource.clear, "clear_resource_caches"),
            (st.session_state.__setattr__, "session_state.set_attr"),
            (st.session_state.__setitem__, "session_state.set_item"),
            (cache_data_api.DataCache.write_result, "_cache_data_object"),
            (
                cache_resource_api.ResourceCache.write_result,
                "_cache_resource_object",
            ),
            (websocket_headers._get_websocket_headers, "_get_websocket_headers"),
            (components.html, "_html"),
            (components.iframe, "_iframe"),
            (st.query_params.__setattr__, "query_params.set_attr"),
            (st.query_params.__getattr__, "query_params.get_attr"),
            (st.query_params.__setitem__, "query_params.set_item"),
            (st.query_params.__getitem__, "query_params.get_item"),
        ]
    )
    def test_internal_api_commands(
        self, command: Callable[..., Any], expected_name: str
    ):
        """Some internal functions are also tracked and should use the correct name."""
        ctx = get_script_run_ctx()
        assert ctx is not None

        # This will always throw an exception because of missing arguments
        # This is fine since the command still get tracked
        with contextlib.suppress(Exception):
            command()

        assert len(ctx.tracked_commands) > 0, f"No command tracked for {expected_name}"

        # Sometimes multiple commands are executed
        # so we check the full list of tracked commands
        assert expected_name in [
            tracked_commands.name for tracked_commands in ctx.tracked_commands
        ], f"Command {expected_name} was not tracked."

    def test_public_api_commands(self):
        """All commands of the public API should be tracked with the correct name."""
        # Some commands are currently not tracked for various reasons:
        ignored_commands = {
            # We need to ignore `connection` because the `@gather_metrics` decorator is
            # attached to a helper function rather than the publicly-exported function,
            # which causes it not to be executed before an Exception is raised due to a
            # lack of required arguments.
            "connection",
            "experimental_connection",
            "spinner",
            "progress",
            "context",
            "login",
            "logout",
        }

        # Create a list of all public API names in the `st` module (minus
        # the ignored commands from above).
        public_api_names = sorted(
            [
                k
                for k, v in st.__dict__.items()
                if not k.startswith("_")
                and not isinstance(v, type(st))
                and k not in ignored_commands
            ]
        )

        for api_name in public_api_names:
            st_func = getattr(st, api_name)
            if not callable(st_func):
                continue

            # Reset tracked stats from previous calls.
            ctx = get_script_run_ctx()
            assert ctx is not None
            ctx.reset()
            ctx.gather_usage_stats = True

            # Call the API. This will often throw an exception due to missing
            # arguments. But that's fine: the command will still be tracked.
            with contextlib.suppress(Exception):
                st_func()

            # Assert that the API name is in the list of tracked commands.
            # (It's possible for multiple tracked commands to be issued as
            # the result of a single API call.)
            assert api_name in [cmd.name for cmd in ctx.tracked_commands], (
                f"When executing `st.{api_name}()`, we expect the string "
                f'"{api_name}" to be in the list of tracked commands.',
            )

    def test_column_config_commands(self):
        """All commands of the public column config API should be tracked with the correct name."""
        # Create a list of all public API names in the `st` module (minus
        # the ignored commands from above).
        public_api_names = sorted(
            [
                k
                for k, v in st.column_config.__dict__.items()
                if not k.startswith("_") and not isinstance(v, type(st.column_config))
            ]
        )

        for api_name in public_api_names:
            st_func = getattr(st.column_config, api_name)
            if not callable(st_func):
                continue

            # Reset tracked stats from previous calls.
            ctx = get_script_run_ctx()
            assert ctx is not None
            ctx.reset()
            ctx.gather_usage_stats = True

            # Call the API. This will often throw an exception due to missing
            # arguments. But that's fine: the command will still be tracked.
            with contextlib.suppress(Exception):
                st_func()

            # Assert that the API name is in the list of tracked commands.
            # (It's possible for multiple tracked commands to be issued as
            # the result of a single API call.)
            assert f"column_config.{api_name}" in [
                cmd.name for cmd in ctx.tracked_commands
            ], (
                f"When executing `st.{api_name}()`, we expect the string "
                f'"{api_name}" to be in the list of tracked commands.',
            )

    def test_command_tracking_limits(self):
        """Command tracking limits should be respected.

        Current limits are 25 per unique command and 200 in total.
        """
        ctx = get_script_run_ctx()
        assert ctx is not None
        ctx.reset()
        ctx.gather_usage_stats = True

        funcs = []
        for i in range(10):

            def test_function() -> str:
                return "foo"

            funcs.append(
                metrics_util.gather_metrics(f"test_function_{i}", test_function)
            )

        for _ in range(metrics_util._MAX_TRACKED_PER_COMMAND + 1):
            for func in funcs:
                func()

        assert len(ctx.tracked_commands) <= metrics_util._MAX_TRACKED_COMMANDS

        # Test that no individual command is tracked more than _MAX_TRACKED_PER_COMMAND
        command_counts = Counter(
            [command.name for command in ctx.tracked_commands]
        ).most_common()
        assert command_counts[0][1] <= metrics_util._MAX_TRACKED_PER_COMMAND


================================================
File: /lib/tests/streamlit/runtime/pages_manager_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Unit tests for PagesManager"""

from __future__ import annotations

import os
import unittest
from unittest.mock import MagicMock, patch

import streamlit.source_util as source_util
from streamlit.runtime.pages_manager import PagesManager, PagesStrategyV1
from streamlit.util import calc_md5


class PagesManagerTest(unittest.TestCase):
    def test_register_pages_changed_callback(self):
        """Test that the pages changed callback is correctly registered and unregistered"""
        pages_manager = PagesManager("main_script_path")
        with patch.object(source_util, "_on_pages_changed", MagicMock()):

            def callback():
                return None

            disconnect = pages_manager.register_pages_changed_callback(callback)

            source_util._on_pages_changed.connect.assert_called_once_with(
                callback, weak=False
            )

            disconnect()
            source_util._on_pages_changed.disconnect.assert_called_once_with(callback)

    @patch("streamlit.runtime.pages_manager.watch_dir")
    @patch.object(source_util, "invalidate_pages_cache", MagicMock())
    def test_install_pages_watcher(self, patched_watch_dir):
        """Test that the pages watcher is correctly installed and uninstalled"""
        # Ensure PagesStrategyV1.is_watching_pages_dir is False to start
        PagesStrategyV1.is_watching_pages_dir = False
        PagesManager(os.path.normpath("/foo/bar/streamlit_app.py"))

        patched_watch_dir.assert_called_once()
        args, _ = patched_watch_dir.call_args_list[0]
        on_pages_changed = args[1]

        patched_watch_dir.assert_called_once_with(
            os.path.normpath("/foo/bar/pages"),
            on_pages_changed,
            glob_pattern="*.py",
            allow_nonexistent=True,
        )

        patched_watch_dir.reset_mock()

        _ = PagesManager(os.path.normpath("/foo/bar/streamlit_app.py"))
        patched_watch_dir.assert_not_called()

        on_pages_changed("/foo/bar/pages")
        source_util.invalidate_pages_cache.assert_called_once()


class PagesManagerV2Test(unittest.TestCase):
    def setUp(self):
        self.pages_manager = PagesManager("main_script_path")

        # This signifies the change to V2
        self.pages_manager.set_pages({})

    def test_get_page_script_valid_hash(self):
        """Ensure the page script is provided with valid page hash specified"""

        self.pages_manager.set_script_intent("page_hash", "")
        self.pages_manager.set_pages({"page_hash": {"page_script_hash": "page_hash"}})

        page_script = self.pages_manager.get_page_script(
            self.pages_manager.main_script_hash
        )
        assert page_script["page_script_hash"] == "page_hash"

    def test_get_page_script_invalid_hash(self):
        """Ensure the page script is provided with invalid page hash specified"""

        self.pages_manager.set_script_intent("bad_hash", "")
        self.pages_manager.set_pages({"page_hash": {"page_script_hash": "page_hash"}})

        page_script = self.pages_manager.get_page_script(
            self.pages_manager.main_script_hash
        )
        assert page_script is None

    def test_get_page_script_valid_name(self):
        """Ensure the page script is provided with valid page name specified"""

        self.pages_manager.set_script_intent("", "page_name")
        self.pages_manager.set_pages(
            {
                "page_hash": {
                    "page_script_hash": "page_hash",
                    "url_pathname": "page_name",
                }
            }
        )

        page_script = self.pages_manager.get_page_script(
            self.pages_manager.main_script_hash
        )
        assert page_script["page_script_hash"] == "page_hash"

    def test_get_page_script_invalid_name(self):
        """Ensure the page script is not provided with invalid page name specified"""

        self.pages_manager.set_script_intent("", "foo")
        self.pages_manager.set_pages(
            {
                "page_hash": {
                    "page_script_hash": "page_hash",
                    "url_pathname": "page_name",
                }
            }
        )

        page_script = self.pages_manager.get_page_script(
            self.pages_manager.main_script_hash
        )
        assert page_script is None

    def test_get_initial_active_script(self):
        """Test that the initial active script is correctly retrieved with the
        main script path provided."""
        page_info = self.pages_manager.get_initial_active_script("page_hash", "")

        self.assertDictEqual(
            page_info,
            {"script_path": "main_script_path", "page_script_hash": "page_hash"},
        )


# NOTE: We write this test function using pytest conventions (as opposed to
# using unittest.TestCase like in the rest of the codebase) because the tmpdir
# pytest fixture is so useful for writing this test it's worth having the
# slight inconsistency.
@patch("streamlit.source_util._cached_pages", new=None)
def test_get_initial_active_script_v1(tmpdir):
    # Write an empty string to create a file.
    tmpdir.join("streamlit_app.py").write("")

    pages_dir = tmpdir.mkdir("pages")
    pages = [
        "03_other_page.py",
        "04 last numbered page.py",
        "01-page.py",
    ]
    for p in pages:
        pages_dir.join(p).write("")

    main_script_path = str(tmpdir / "streamlit_app.py")
    pages_manager = PagesManager(main_script_path)

    example_page_script_hash = calc_md5(str(pages_dir / "01-page.py"))

    # positive case - get by hash
    page = pages_manager.get_initial_active_script(example_page_script_hash, None)
    assert page["page_script_hash"] == example_page_script_hash

    # bad hash should not return a page
    page = pages_manager.get_initial_active_script("random_hash", None)
    assert page is None

    # Even if the page name is specified, we detect via the hash only
    page = pages_manager.get_initial_active_script("random_hash", "page")
    assert page is None

    # Find by page name works
    page = pages_manager.get_initial_active_script("", "page")
    assert page["page_script_hash"] == example_page_script_hash

    # Try different page name
    alternate_page_script_hash = calc_md5(str(pages_dir / "03_other_page.py"))
    page = pages_manager.get_initial_active_script("", "other_page")
    assert page["page_script_hash"] == alternate_page_script_hash

    # Even if the valid page name is specified, we detect via the hash only
    page = pages_manager.get_initial_active_script(alternate_page_script_hash, "page")
    assert page["page_script_hash"] == alternate_page_script_hash


================================================
File: /lib/tests/streamlit/runtime/runtime_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import asyncio
import os
import shutil
import tempfile
import unittest
from unittest.mock import ANY, MagicMock, call, patch

import pytest

from streamlit.components.lib.local_component_registry import LocalComponentRegistry
from streamlit.proto.ForwardMsg_pb2 import ForwardMsg
from streamlit.runtime import (
    Runtime,
    RuntimeConfig,
    RuntimeState,
    SessionClient,
    SessionClientDisconnectedError,
)
from streamlit.runtime.caching.storage.local_disk_cache_storage import (
    LocalDiskCacheStorageManager,
)
from streamlit.runtime.forward_msg_cache import populate_hash_if_needed
from streamlit.runtime.memory_media_file_storage import MemoryMediaFileStorage
from streamlit.runtime.memory_session_storage import MemorySessionStorage
from streamlit.runtime.memory_uploaded_file_manager import MemoryUploadedFileManager
from streamlit.runtime.runtime import AsyncObjects, RuntimeStoppedError
from streamlit.runtime.websocket_session_manager import WebsocketSessionManager
from streamlit.watcher import event_based_path_watcher
from tests.streamlit.message_mocks import (
    create_dataframe_msg,
    create_script_finished_message,
)
from tests.streamlit.runtime.runtime_test_case import RuntimeTestCase
from tests.testutil import patch_config_options


class MockSessionClient(SessionClient):
    """A SessionClient that captures all its ForwardMsgs into a list."""

    def __init__(self):
        self.forward_msgs: list[ForwardMsg] = []

    def write_forward_msg(self, msg: ForwardMsg) -> None:
        self.forward_msgs.append(msg)


class RuntimeConfigTests(unittest.TestCase):
    def test_runtime_config_defaults(self):
        config = RuntimeConfig(
            "/my/script.py",
            None,
            MemoryMediaFileStorage("/mock/media"),
            MemoryUploadedFileManager("/mock/upload"),
        )

        self.assertIsInstance(
            config.cache_storage_manager, LocalDiskCacheStorageManager
        )
        self.assertIs(config.session_manager_class, WebsocketSessionManager)
        self.assertIsInstance(config.session_storage, MemorySessionStorage)


class RuntimeSingletonTest(unittest.TestCase):
    def tearDown(self) -> None:
        Runtime._instance = None

    def test_runtime_constructor_sets_instance(self):
        """Creating a Runtime instance sets Runtime.instance"""
        self.assertIsNone(Runtime._instance)
        _ = Runtime(MagicMock())
        self.assertIsNotNone(Runtime._instance)

    def test_multiple_runtime_error(self):
        """Creating multiple Runtimes raises an error."""
        Runtime(MagicMock())
        with self.assertRaises(RuntimeError):
            Runtime(MagicMock())

    def test_instance_class_method(self):
        """Runtime.instance() returns our singleton instance."""
        with self.assertRaises(RuntimeError):
            # No Runtime: error
            Runtime.instance()

        # Runtime instantiated: no error
        _ = Runtime(MagicMock())
        Runtime.instance()

    def test_exists(self):
        """Runtime.exists() returns True iff the Runtime singleton exists."""
        self.assertFalse(Runtime.exists())
        _ = Runtime(MagicMock())
        self.assertTrue(Runtime.exists())


class RuntimeTest(RuntimeTestCase):
    async def test_start_stop(self):
        """starting and stopping the Runtime should work as expected."""
        self.assertEqual(RuntimeState.INITIAL, self.runtime.state)

        await self.runtime.start()
        self.assertEqual(RuntimeState.NO_SESSIONS_CONNECTED, self.runtime.state)

        self.runtime.stop()
        await asyncio.sleep(0)  # Wait 1 tick for the stop to be acknowledged
        self.assertEqual(RuntimeState.STOPPING, self.runtime.state)

        await self.runtime.stopped
        self.assertEqual(RuntimeState.STOPPED, self.runtime.state)

    async def test_connect_session(self):
        """We can create and remove a single session."""
        await self.runtime.start()

        session_id = self.runtime.connect_session(
            client=MockSessionClient(), user_info=MagicMock()
        )
        self.assertEqual(
            RuntimeState.ONE_OR_MORE_SESSIONS_CONNECTED, self.runtime.state
        )

        self.runtime.disconnect_session(session_id)
        self.assertEqual(RuntimeState.NO_SESSIONS_CONNECTED, self.runtime.state)

    async def test_connect_session_error_if_both_session_id_args(self):
        """Test that setting both existing_session_id and session_id_override is an error."""
        await self.runtime.start()

        with pytest.raises(AssertionError):
            self.runtime.connect_session(
                client=MockSessionClient(),
                user_info=MagicMock(),
                existing_session_id="existing_session_id",
                session_id_override="session_id_override",
            )

    async def test_connect_session_existing_session_id_plumbing(self):
        """The existing_session_id parameter is plumbed to _session_mgr.connect_session."""
        await self.runtime.start()

        with patch.object(
            self.runtime._session_mgr, "connect_session", new=MagicMock()
        ) as patched_connect_session:
            client = MockSessionClient()
            user_info = MagicMock()
            existing_session_id = "some_session_id"

            self.runtime.connect_session(
                client=client,
                user_info=user_info,
                existing_session_id=existing_session_id,
            )

            patched_connect_session.assert_called_with(
                client=client,
                script_data=ANY,
                user_info=user_info,
                existing_session_id=existing_session_id,
                session_id_override=None,
            )

    async def test_connect_session_session_id_override_plumbing(self):
        """The session_id_override parameter is plumbed to _session_mgr.connect_session."""
        await self.runtime.start()

        with patch.object(
            self.runtime._session_mgr, "connect_session", new=MagicMock()
        ) as patched_connect_session:
            client = MockSessionClient()
            user_info = MagicMock()
            session_id_override = "some_session_id"

            self.runtime.connect_session(
                client=client,
                user_info=user_info,
                session_id_override=session_id_override,
            )

            patched_connect_session.assert_called_with(
                client=client,
                script_data=ANY,
                user_info=user_info,
                existing_session_id=None,
                session_id_override=session_id_override,
            )

    @patch("streamlit.runtime.runtime._LOGGER")
    async def test_create_session_alias(self, patched_logger):
        """Test that create_session defers to connect_session and logs a warning."""
        await self.runtime.start()

        client = MockSessionClient()
        user_info = MagicMock()

        with patch.object(
            self.runtime, "connect_session", new=MagicMock()
        ) as patched_connect_session:
            self.runtime.create_session(client=client, user_info=user_info)

            patched_connect_session.assert_called_with(
                client=client,
                user_info=user_info,
                existing_session_id=None,
                session_id_override=None,
            )
            patched_logger.warning.assert_called_with(
                "create_session is deprecated! Use connect_session instead."
            )

    async def test_disconnect_session_disconnects_appsession(self):
        """Closing a session should disconnect its associated AppSession."""
        await self.runtime.start()

        session_id = self.runtime.connect_session(
            client=MockSessionClient(), user_info=MagicMock()
        )
        session = self.runtime._session_mgr.get_session_info(session_id).session

        with patch.object(
            self.runtime._session_mgr, "disconnect_session", new=MagicMock()
        ) as patched_disconnect_session, patch.object(
            self.runtime, "_on_session_disconnected", new=MagicMock()
        ) as patched_on_session_disconnected, patch.object(
            self.runtime._message_cache, "remove_refs_for_session", new=MagicMock()
        ) as patched_remove_refs_for_session:
            self.runtime.disconnect_session(session_id)
            patched_disconnect_session.assert_called_once_with(session_id)
            patched_on_session_disconnected.assert_called_once()
            patched_remove_refs_for_session.assert_called_once_with(session)

    async def test_close_session_closes_appsession(self):
        await self.runtime.start()

        session_id = self.runtime.connect_session(
            client=MockSessionClient(), user_info=MagicMock()
        )
        session = self.runtime._session_mgr.get_session_info(session_id).session

        with patch.object(
            self.runtime._session_mgr, "close_session", new=MagicMock()
        ) as patched_close_session, patch.object(
            self.runtime, "_on_session_disconnected", new=MagicMock()
        ) as patched_on_session_disconnected, patch.object(
            self.runtime._message_cache, "remove_refs_for_session", new=MagicMock()
        ) as patched_remove_refs_for_session:
            self.runtime.close_session(session_id)
            patched_close_session.assert_called_once_with(session_id)
            patched_on_session_disconnected.assert_called_once()
            patched_remove_refs_for_session.assert_called_once_with(session)

    async def test_multiple_sessions(self):
        """Multiple sessions can be connected."""
        await self.runtime.start()

        session_ids = []
        for _ in range(3):
            session_id = self.runtime.connect_session(
                client=MockSessionClient(),
                user_info=MagicMock(),
            )

            self.assertEqual(
                RuntimeState.ONE_OR_MORE_SESSIONS_CONNECTED, self.runtime.state
            )
            session_ids.append(session_id)

        for i in range(len(session_ids)):
            self.runtime.disconnect_session(session_ids[i])
            expected_state = (
                RuntimeState.NO_SESSIONS_CONNECTED
                if i == len(session_ids) - 1
                else RuntimeState.ONE_OR_MORE_SESSIONS_CONNECTED
            )
            self.assertEqual(expected_state, self.runtime.state)

        self.assertEqual(RuntimeState.NO_SESSIONS_CONNECTED, self.runtime.state)

    async def test_disconnect_invalid_session(self):
        """Disconnecting a session that doesn't exist is a no-op: no error raised."""
        await self.runtime.start()

        # Close a session that never existed
        self.runtime.disconnect_session("no_such_session")

        # Close a valid session twice
        session_id = self.runtime.connect_session(
            client=MockSessionClient(), user_info=MagicMock()
        )
        self.runtime.disconnect_session(session_id)
        self.runtime.disconnect_session(session_id)

    async def test_close_invalid_session(self):
        """Closing a session that doesn't exist is a no-op: no error raised."""
        await self.runtime.start()

        # Close a session that never existed
        self.runtime.close_session("no_such_session")

        # Close a valid session twice
        session_id = self.runtime.connect_session(
            client=MockSessionClient(), user_info=MagicMock()
        )
        self.runtime.close_session(session_id)
        self.runtime.close_session(session_id)

    async def test_is_active_session(self):
        """`is_active_session` should work as expected."""
        await self.runtime.start()
        session_id = self.runtime.connect_session(
            client=MockSessionClient(), user_info=MagicMock()
        )
        self.assertTrue(self.runtime.is_active_session(session_id))
        self.assertFalse(self.runtime.is_active_session("not_a_session_id"))

        self.runtime.disconnect_session(session_id)
        self.assertFalse(self.runtime.is_active_session(session_id))

    async def test_closes_app_sessions_on_stop(self):
        """When the Runtime stops, it should close all AppSessions."""
        await self.runtime.start()

        # Create a few sessions
        app_sessions = []
        for _ in range(3):
            session_id = self.runtime.connect_session(MockSessionClient(), MagicMock())
            app_session = self.runtime._session_mgr.get_active_session_info(
                session_id
            ).session
            app_sessions.append(app_session)

        with patch.object(
            self.runtime._session_mgr, "close_session"
        ) as patched_close_session:
            # Stop the Runtime
            self.runtime.stop()
            await self.runtime.stopped

            self.assertEqual(RuntimeState.STOPPED, self.runtime.state)

            # All sessions should be shut down via self._session_mgr.close_session
            patched_close_session.assert_has_calls(call(s.id) for s in app_sessions)

    @patch("streamlit.runtime.app_session.AppSession.handle_backmsg", new=MagicMock())
    async def test_handle_backmsg(self):
        """BackMsgs should be delivered to the appropriate AppSession."""
        await self.runtime.start()
        session_id = self.runtime.connect_session(
            client=MockSessionClient(), user_info=MagicMock()
        )

        back_msg = MagicMock()
        self.runtime.handle_backmsg(session_id, back_msg)

        app_session = self.runtime._session_mgr.get_active_session_info(
            session_id
        ).session
        app_session.handle_backmsg.assert_called_once_with(back_msg)

    async def test_handle_backmsg_invalid_session(self):
        """A BackMsg for an invalid session should get dropped without an error."""
        await self.runtime.start()
        self.runtime.handle_backmsg("not_a_session_id", MagicMock())

    @patch(
        "streamlit.runtime.app_session.AppSession.handle_backmsg_exception",
        new=MagicMock(),
    )
    async def test_handle_backmsg_deserialization_exception(self):
        """BackMsg deserialization Exceptions should be delivered to the
        appropriate AppSession.
        """
        await self.runtime.start()
        session_id = self.runtime.connect_session(
            client=MockSessionClient(), user_info=MagicMock()
        )

        exception = MagicMock()
        self.runtime.handle_backmsg_deserialization_exception(session_id, exception)

        app_session = self.runtime._session_mgr.get_active_session_info(
            session_id
        ).session
        app_session.handle_backmsg_exception.assert_called_once_with(exception)

    async def test_handle_backmsg_exception_invalid_session(self):
        """A BackMsg exception for an invalid session should get dropped without an
        error."""
        await self.runtime.start()
        self.runtime.handle_backmsg_deserialization_exception(
            "not_a_session_id", MagicMock()
        )

    async def test_connect_session_after_stop(self):
        """After Runtime.stop is called, `connect_session` is an error."""
        await self.runtime.start()
        self.runtime.stop()
        await self.tick_runtime_loop()

        with self.assertRaises(RuntimeStoppedError):
            self.runtime.connect_session(MagicMock(), MagicMock())

    async def test_handle_backmsg_after_stop(self):
        """After Runtime.stop is called, `handle_backmsg` is an error."""
        await self.runtime.start()
        self.runtime.stop()
        await self.tick_runtime_loop()

        with self.assertRaises(RuntimeStoppedError):
            self.runtime.handle_backmsg("not_a_session_id", MagicMock())

    async def test_handle_session_client_disconnected(self):
        """Runtime should gracefully handle `SessionClient.write_forward_msg`
        raising a `SessionClientDisconnectedError`.
        """
        await self.runtime.start()

        client = MagicMock(spec=SessionClient)
        session_id = self.runtime.connect_session(client, MagicMock())

        # Send the client a message. All should be well.
        self.enqueue_forward_msg(session_id, create_dataframe_msg([1, 2, 3]))
        await self.tick_runtime_loop()

        client.write_forward_msg.assert_called_once()
        self.assertTrue(self.runtime.is_active_session(session_id))

        # Send another message - but this time the client will raise an error.
        raise_disconnected_error = MagicMock(side_effect=SessionClientDisconnectedError)
        client.write_forward_msg = raise_disconnected_error
        self.enqueue_forward_msg(session_id, create_dataframe_msg([1, 2, 3]))
        await self.tick_runtime_loop()

        # Assert that our error was raised, and that our session was disconnected.
        raise_disconnected_error.assert_called_once()
        self.assertFalse(self.runtime.is_active_session(session_id))

    async def test_stable_number_of_async_tasks(self):
        """Test that the number of async tasks remains stable.

        This is a regression test for a memory leak issue where the number of
        tasks would grow with every loop.
        """
        await self.runtime.start()

        client = MockSessionClient()
        session_id = self.runtime.connect_session(client=client, user_info=MagicMock())

        for _ in range(100):
            self.enqueue_forward_msg(session_id, create_dataframe_msg([1, 2, 3]))
            await self.tick_runtime_loop()

        # It is expected that there are a couple of tasks, but not one per loop:
        self.assertLess(len(asyncio.all_tasks()), 10)

    async def test_forwardmsg_hashing(self):
        """Test that outgoing ForwardMsgs contain hashes."""
        await self.runtime.start()

        client = MockSessionClient()
        session_id = self.runtime.connect_session(client=client, user_info=MagicMock())

        # Create a message and ensure its hash is unset; we're testing
        # that _send_message adds the hash before it goes out.
        msg = create_dataframe_msg([1, 2, 3])
        msg.ClearField("hash")
        self.enqueue_forward_msg(session_id, msg)
        await self.tick_runtime_loop()

        received = client.forward_msgs.pop()
        self.assertEqual(populate_hash_if_needed(msg), received.hash)

    async def test_forwardmsg_cacheable_flag(self):
        """Test that the metadata.cacheable flag is set properly on outgoing
        ForwardMsgs."""
        await self.runtime.start()

        client = MockSessionClient()
        session_id = self.runtime.connect_session(client=client, user_info=MagicMock())

        with patch_config_options({"global.minCachedMessageSize": 0}):
            cacheable_msg = create_dataframe_msg([1, 2, 3])
            self.enqueue_forward_msg(session_id, cacheable_msg)
            await self.tick_runtime_loop()

            received = client.forward_msgs.pop()
            self.assertTrue(cacheable_msg.metadata.cacheable)
            self.assertTrue(received.metadata.cacheable)

        with patch_config_options({"global.minCachedMessageSize": 1000}):
            cacheable_msg = create_dataframe_msg([4, 5, 6])
            self.enqueue_forward_msg(session_id, cacheable_msg)
            await self.tick_runtime_loop()

            received = client.forward_msgs.pop()
            self.assertFalse(cacheable_msg.metadata.cacheable)
            self.assertFalse(received.metadata.cacheable)

    async def test_duplicate_forwardmsg_caching(self):
        """Test that duplicate ForwardMsgs are sent only once."""
        with patch_config_options({"global.minCachedMessageSize": 0}):
            await self.runtime.start()

            client = MockSessionClient()
            session_id = self.runtime.connect_session(
                client=client, user_info=MagicMock()
            )

            msg1 = create_dataframe_msg([1, 2, 3], 1)

            # Send the message, and read it back. It will not have been cached.
            self.enqueue_forward_msg(session_id, msg1)
            await self.tick_runtime_loop()

            uncached = client.forward_msgs.pop()
            self.assertEqual("delta", uncached.WhichOneof("type"))

            # Send an equivalent message. This time, it should be cached,
            # and a "hash_reference" message should be received instead.
            msg2 = create_dataframe_msg([1, 2, 3], 123)
            self.enqueue_forward_msg(session_id, msg2)
            await self.tick_runtime_loop()

            cached = client.forward_msgs.pop()
            self.assertEqual("ref_hash", cached.WhichOneof("type"))
            # We should have the *hash* of msg1 and msg2:
            self.assertEqual(msg1.hash, cached.ref_hash)
            self.assertEqual(msg2.hash, cached.ref_hash)
            # And the same *metadata* as msg2:
            self.assertEqual(msg2.metadata, cached.metadata)

    async def test_forwardmsg_cache_clearing(self):
        """Test that the ForwardMsgCache gets properly cleared when scripts
        finish running.
        """
        with patch_config_options(
            {"global.minCachedMessageSize": 0, "global.maxCachedMessageAge": 1}
        ):
            await self.runtime.start()

            client = MockSessionClient()
            session_id = self.runtime.connect_session(
                client=client, user_info=MagicMock()
            )

            data_msg = create_dataframe_msg([1, 2, 3])

            async def finish_script(success: bool) -> None:
                status = (
                    ForwardMsg.FINISHED_SUCCESSFULLY
                    if success
                    else ForwardMsg.FINISHED_WITH_COMPILE_ERROR
                )
                finish_msg = create_script_finished_message(status)
                self.enqueue_forward_msg(session_id, finish_msg)
                await self.tick_runtime_loop()

            def is_data_msg_cached() -> bool:
                return (
                    self.runtime._message_cache.get_message(data_msg.hash) is not None
                )

            async def send_data_msg() -> None:
                self.enqueue_forward_msg(session_id, data_msg)
                await self.tick_runtime_loop()

            # Send a cacheable message. It should be cached.
            await send_data_msg()
            self.assertTrue(is_data_msg_cached())

            # End the script with a compile error. Nothing should change;
            # compile errors don't increase the age of items in the cache.
            await finish_script(False)
            self.assertTrue(is_data_msg_cached())

            # End the script successfully. Nothing should change, because
            # the age of the cached message is now 1.
            await finish_script(True)
            self.assertTrue(is_data_msg_cached())

            # Send the message again. This should reset its age to 0 in the
            # cache, so it won't be evicted when the script next finishes.
            await send_data_msg()
            self.assertTrue(is_data_msg_cached())

            # Finish the script. The cached message age is now 1.
            await finish_script(True)
            self.assertTrue(is_data_msg_cached())

            # Finish again. The cached message age will be 2, and so it
            # should be evicted from the cache.
            await finish_script(True)
            self.assertFalse(is_data_msg_cached())

    async def test_get_async_objs(self):
        """Runtime._get_async_objs() will raise an error if called before the
        Runtime is started, and will return the Runtime's AsyncObjects instance otherwise.
        """
        with self.assertRaises(RuntimeError):
            # Runtime hasn't started yet: error!
            _ = self.runtime._get_async_objs()

        # Runtime has started: no error
        await self.runtime.start()
        self.assertIsInstance(self.runtime._get_async_objs(), AsyncObjects)


@patch("streamlit.source_util._cached_pages", new=None)
class ScriptCheckTest(RuntimeTestCase):
    """Tests for Runtime.does_script_run_without_error"""

    def setUp(self) -> None:
        self._home = tempfile.mkdtemp()
        self._old_home = os.environ["HOME"]
        os.environ["HOME"] = self._home

        self._fd, self._path = tempfile.mkstemp()

        super().setUp()

    async def asyncSetUp(self):
        # We don't call super().asyncSetUp() here. (Our superclass creates
        # its own Runtime instance with a mock script_path, but we want
        # to specify a non-mocked path.)
        config = RuntimeConfig(
            script_path=self._path,
            command_line=None,
            component_registry=LocalComponentRegistry(),
            media_file_storage=MemoryMediaFileStorage("/mock/media"),
            uploaded_file_manager=MemoryUploadedFileManager("/mock/upload"),
            session_manager_class=MagicMock,
            session_storage=MagicMock(),
            cache_storage_manager=MagicMock(),
            is_hello=False,
        )
        self.runtime = Runtime(config)
        await self.runtime.start()

    def tearDown(self) -> None:
        if event_based_path_watcher._MultiPathWatcher._singleton is not None:
            event_based_path_watcher._MultiPathWatcher.get_singleton().close()
            event_based_path_watcher._MultiPathWatcher._singleton = None

        os.environ["HOME"] = self._old_home
        os.remove(self._path)
        shutil.rmtree(self._home)

        super().tearDown()

    @pytest.mark.slow
    async def test_invalid_script(self):
        script = """
import streamlit as st
st.not_a_function('test')
"""

        await self._check_script_loading(script, False, "error")

    @pytest.mark.slow
    async def test_valid_script(self):
        script = """
import streamlit as st
st.write('test')
"""

        await self._check_script_loading(script, True, "ok")

    @pytest.mark.slow
    async def test_timeout_script(self):
        script = """
import time
time.sleep(5)
"""

        with patch("streamlit.runtime.runtime.SCRIPT_RUN_CHECK_TIMEOUT", new=0.1):
            await self._check_script_loading(script, False, "timeout")

    async def _check_script_loading(
        self, script: str, expected_loads: bool, expected_msg: str
    ) -> None:
        with os.fdopen(self._fd, "w") as tmp:
            tmp.write(script)

        ok, msg = await self.runtime.does_script_run_without_error()
        event_based_path_watcher._MultiPathWatcher.get_singleton().close()
        event_based_path_watcher._MultiPathWatcher._singleton = None
        self.assertEqual(expected_loads, ok)
        self.assertEqual(expected_msg, msg)


================================================
File: /lib/tests/streamlit/runtime/runtime_test_case.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import asyncio
from typing import TYPE_CHECKING, Callable
from unittest import IsolatedAsyncioTestCase, mock

from streamlit.components.lib.local_component_registry import LocalComponentRegistry
from streamlit.runtime import Runtime, RuntimeConfig, RuntimeState
from streamlit.runtime.app_session import AppSession
from streamlit.runtime.caching.storage.dummy_cache_storage import (
    MemoryCacheStorageManager,
)
from streamlit.runtime.memory_media_file_storage import MemoryMediaFileStorage
from streamlit.runtime.memory_uploaded_file_manager import MemoryUploadedFileManager
from streamlit.runtime.pages_manager import PagesManager
from streamlit.runtime.session_manager import (
    SessionClient,
    SessionInfo,
    SessionManager,
    SessionStorage,
)

if TYPE_CHECKING:
    from streamlit.proto.ForwardMsg_pb2 import ForwardMsg
    from streamlit.runtime.script_data import ScriptData
    from streamlit.runtime.scriptrunner.script_cache import ScriptCache
    from streamlit.runtime.uploaded_file_manager import UploadedFileManager


class MockSessionManager(SessionManager):
    """A MockSessionManager used for runtime tests.

    This is done so that our runtime tests don't rely on a specific SessionManager
    implementation.
    """

    def __init__(
        self,
        session_storage: SessionStorage,
        uploaded_file_manager: UploadedFileManager,
        script_cache: ScriptCache,
        message_enqueued_callback: Callable[[], None] | None,
    ) -> None:
        self._uploaded_file_mgr = uploaded_file_manager
        self._script_cache = script_cache
        self._message_enqueued_callback = message_enqueued_callback

        # Mapping of AppSession.id -> SessionInfo.
        self._session_info_by_id: dict[str, SessionInfo] = {}

    def connect_session(
        self,
        client: SessionClient,
        script_data: ScriptData,
        user_info: dict[str, str | None],
        existing_session_id: str | None = None,
        session_id_override: str | None = None,
    ) -> str:
        with mock.patch(
            "streamlit.runtime.scriptrunner.ScriptRunner", new=mock.MagicMock()
        ), mock.patch.object(
            PagesManager, "get_pages", mock.MagicMock(return_value={})
        ):
            session = AppSession(
                script_data=script_data,
                uploaded_file_manager=self._uploaded_file_mgr,
                script_cache=self._script_cache,
                message_enqueued_callback=self._message_enqueued_callback,
                user_info=user_info,
                session_id_override=session_id_override,
            )

        assert (
            session.id not in self._session_info_by_id
        ), f"session.id '{session.id}' registered multiple times!"

        self._session_info_by_id[session.id] = SessionInfo(client, session)
        return session.id

    def close_session(self, session_id: str) -> None:
        if session_id in self._session_info_by_id:
            session_info = self._session_info_by_id[session_id]
            del self._session_info_by_id[session_id]
            session_info.session.shutdown()

    def get_session_info(self, session_id: str) -> SessionInfo | None:
        return self._session_info_by_id.get(session_id, None)

    def list_sessions(self) -> list[SessionInfo]:
        return list(self._session_info_by_id.values())


class RuntimeTestCase(IsolatedAsyncioTestCase):
    """Base class for tests that use streamlit.Runtime directly."""

    _next_session_id = 0

    async def asyncSetUp(self):
        config = RuntimeConfig(
            script_path="mock/script/path.py",
            command_line=None,
            component_registry=LocalComponentRegistry(),
            media_file_storage=MemoryMediaFileStorage("/mock/media"),
            uploaded_file_manager=MemoryUploadedFileManager("/mock/upload"),
            session_manager_class=MockSessionManager,
            session_storage=mock.MagicMock(),
            cache_storage_manager=MemoryCacheStorageManager(),
            is_hello=False,
        )
        self.runtime = Runtime(config)

    async def asyncTearDown(self):
        # Stop the runtime, and return when it's stopped
        if self.runtime.state != RuntimeState.INITIAL:
            self.runtime.stop()
            await self.runtime.stopped
        Runtime._instance = None

    @staticmethod
    async def tick_runtime_loop() -> None:
        """Sleep just long enough to guarantee that the Runtime's loop
        has a chance to run.
        """
        # Our sleep time needs to be longer than the longest sleep time inside the
        # Runtime loop, which is 0.01 + (1 tick * number of connected sessions).
        # 0.03 is near-instant, and conservative enough that the tick will happen
        # under our test circumstances.
        await asyncio.sleep(0.03)

    def enqueue_forward_msg(self, session_id: str, msg: ForwardMsg) -> None:
        """Enqueue a ForwardMsg to a given session_id. It will be sent
        to the client on the next iteration through the run loop. (You can
        use `await self.tick_runtime_loop()` to tick the run loop.)
        """
        session_info = self.runtime._session_mgr.get_active_session_info(session_id)
        if session_info is None:
            return
        session_info.session._enqueue_forward_msg(msg)


================================================
File: /lib/tests/streamlit/runtime/runtime_threading_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import asyncio
import threading
from queue import Queue
from unittest import IsolatedAsyncioTestCase
from unittest.mock import MagicMock

from streamlit.runtime import Runtime, RuntimeConfig


class RuntimeThreadingTest(IsolatedAsyncioTestCase):
    """Threading-related Runtime tests."""

    async def test_create_runtime_on_another_thread(self):
        """Test that Runtime can be constructed on a thread that it doesn't actually
        run on.

        (This test will fail if Runtime's various asyncio initialization bits are
        performed in its constructor instead of in "start".)
        """

        queue = Queue()

        def create_runtime_on_another_thread():
            try:
                # This function should be called in another thread, which
                # should not already have an asyncio loop.
                with self.assertRaises(RuntimeError):
                    asyncio.get_running_loop()

                # Create a Runtime instance and put it in the (thread-safe) queue,
                # so that the main thread can retrieve it safely. If Runtime
                # creation fails, we'll stick an Exception in the queue instead.
                config = RuntimeConfig(
                    "mock/script/path.py",
                    "",
                    component_registry=MagicMock(),
                    media_file_storage=MagicMock(),
                    uploaded_file_manager=MagicMock(),
                    session_manager_class=MagicMock,
                    session_storage=MagicMock(),
                    cache_storage_manager=MagicMock(),
                )
                queue.put(Runtime(config))
            except BaseException as e:
                queue.put(e)

        thread = threading.Thread(target=create_runtime_on_another_thread)
        thread.start()
        thread.join(timeout=1)
        if thread.is_alive():
            raise RuntimeError("Thread.join timed out!")

        runtime = queue.get(block=True, timeout=1)
        if isinstance(runtime, BaseException):
            raise runtime

        # Ensure we can start and stop the Runtime
        await runtime.start()
        runtime.stop()
        await runtime.stopped


================================================
File: /lib/tests/streamlit/runtime/runtime_util_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Unit tests for runtime_util.py."""

from __future__ import annotations

import unittest

from streamlit.proto.ForwardMsg_pb2 import ForwardMsg
from streamlit.runtime import runtime_util
from streamlit.runtime.runtime_util import is_cacheable_msg, serialize_forward_msg
from tests.streamlit.message_mocks import create_dataframe_msg
from tests.testutil import patch_config_options


class RuntimeUtilTest(unittest.TestCase):
    def test_should_cache_msg(self):
        """Test runtime_util.should_cache_msg"""
        with patch_config_options({"global.minCachedMessageSize": 0}):
            self.assertTrue(is_cacheable_msg(create_dataframe_msg([1, 2, 3])))

        with patch_config_options({"global.minCachedMessageSize": 1000}):
            self.assertFalse(is_cacheable_msg(create_dataframe_msg([1, 2, 3])))

    def test_should_limit_msg_size(self):
        max_message_size_mb = 50

        runtime_util._max_message_size_bytes = None  # Reset cached value
        with patch_config_options({"server.maxMessageSize": max_message_size_mb}):
            # Set up a larger than limit ForwardMsg string
            large_msg = create_dataframe_msg([1, 2, 3])
            large_msg.delta.new_element.markdown.body = (
                "X" * (max_message_size_mb + 10) * 1000 * 1000
            )
            # Create a copy, since serialize_forward_msg modifies the original proto
            large_msg_copy = ForwardMsg()
            large_msg_copy.CopyFrom(large_msg)
            deserialized_msg = ForwardMsg()
            deserialized_msg.ParseFromString(serialize_forward_msg(large_msg_copy))

            # The metadata should be the same, but contents should be replaced
            self.assertEqual(deserialized_msg.metadata, large_msg.metadata)
            self.assertNotEqual(deserialized_msg, large_msg)
            self.assertTrue(
                "exceeds the message size limit"
                in deserialized_msg.delta.new_element.exception.message
            )


================================================
File: /lib/tests/streamlit/runtime/script_data_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import os.path
import unittest
from dataclasses import FrozenInstanceError

import pytest

from streamlit.runtime.script_data import ScriptData


class ScriptDataTest(unittest.TestCase):
    def test_script_folder_and_name_set(self):
        script_data = ScriptData(
            "/path/to/some/script.py",
            False,
        )

        assert os.path.realpath(script_data.main_script_path) == os.path.realpath(
            "/path/to/some/script.py"
        )
        assert script_data.is_hello is False
        assert os.path.realpath(script_data.script_folder) == os.path.realpath(
            "/path/to/some"
        )
        assert script_data.name == "script"

    def test_is_frozen(self):
        script_data = ScriptData(
            "/path/to/some/script.py",
            False,
        )

        with pytest.raises(FrozenInstanceError):
            script_data.name = "bob"


================================================
File: /lib/tests/streamlit/runtime/secrets_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""st.secrets unit tests."""

from __future__ import annotations

import os
import tempfile
import unittest
from collections.abc import Mapping as MappingABC
from collections.abc import MutableMapping as MutableMappingABC
from typing import Mapping, MutableMapping
from unittest.mock import MagicMock, mock_open, patch

from parameterized import parameterized
from toml import TomlDecodeError

from streamlit.runtime.secrets import (
    AttrDict,
    SecretErrorMessages,
    Secrets,
)
from tests import testutil
from tests.exception_capturing_thread import call_on_threads

MOCK_TOML = """
# Everything in this section will be available as an environment variable
db_username="Jane"
db_password="12345qwerty"

# Sub-sections are not loaded into os.environ
[subsection]
email="eng@streamlit.io"
"""

MOCK_SECRETS_FILE_LOC = "/mock/secrets.toml"


class TestSecretErrorMessages(unittest.TestCase):
    def test_changing_message(self):
        messages = SecretErrorMessages()
        self.assertEqual(
            messages.get_missing_attr_message("attr"),
            'st.secrets has no attribute "attr". Did you forget to add it to secrets.toml, mount it to secret directory, or the app settings on Streamlit Cloud? More info: https://docs.streamlit.io/deploy/streamlit-community-cloud/deploy-your-app/secrets-management',
        )

        messages.set_missing_attr_message(
            lambda attr: "Missing attribute message",
        )

        self.assertEqual(
            messages.get_missing_attr_message([""]),
            "Missing attribute message",
        )


class SecretsTest(unittest.TestCase):
    """Tests for st.secrets with a single secrets.toml file"""

    def setUp(self) -> None:
        # st.secrets modifies os.environ, so we save it here and
        # restore in tearDown.
        self._prev_environ = dict(os.environ)
        # Run tests on our own Secrets instance to reduce global state
        # mutations.
        self.secrets = Secrets()

    def tearDown(self) -> None:
        os.environ.clear()
        os.environ.update(self._prev_environ)

    @patch("streamlit.watcher.path_watcher.watch_file")
    @patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML)
    @patch("streamlit.config.get_option", return_value=[MOCK_SECRETS_FILE_LOC])
    def test_access_secrets(self, *mocks):
        self.assertEqual(self.secrets["db_username"], "Jane")
        self.assertEqual(self.secrets["subsection"]["email"], "eng@streamlit.io")
        self.assertEqual(self.secrets["subsection"].email, "eng@streamlit.io")

    @parameterized.expand(
        [
            [
                False,
                "Secrets",
            ],
            [
                True,
                (
                    "{'db_username': 'Jane', 'db_password': '12345qwerty', "
                    "'subsection': {'email': 'eng@streamlit.io'}}"
                ),
            ],
        ]
    )
    @patch("streamlit.watcher.path_watcher.watch_file")
    @patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML)
    @patch("streamlit.config.get_option", return_value=[MOCK_SECRETS_FILE_LOC])
    def test_repr_secrets(self, runtime_exists, secrets_repr, *mocks):
        with patch("streamlit.runtime.exists", return_value=runtime_exists):
            self.assertEqual(repr(self.secrets), secrets_repr)

    @patch("streamlit.watcher.path_watcher.watch_file")
    @patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML)
    @patch("streamlit.config.get_option", return_value=[MOCK_SECRETS_FILE_LOC])
    def test_access_secrets_via_attribute(self, *mocks):
        self.assertEqual(self.secrets.db_username, "Jane")
        self.assertEqual(self.secrets.subsection["email"], "eng@streamlit.io")
        self.assertEqual(self.secrets.subsection.email, "eng@streamlit.io")

    @patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML)
    def test_os_environ(self, _):
        """os.environ gets patched when we load our secrets.toml"""
        # We haven't loaded secrets yet
        self.assertEqual(os.environ.get("db_username"), None)

        self.secrets.load_if_toml_exists()
        self.assertEqual(os.environ["db_username"], "Jane")
        self.assertEqual(os.environ["db_password"], "12345qwerty")

        # Subsections do not get loaded into os.environ
        self.assertEqual(os.environ.get("subsection"), None)

    @patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML)
    def test_load_if_toml_exists_returns_true_if_parse_succeeds(self, _):
        self.assertTrue(self.secrets.load_if_toml_exists())

    def test_load_if_toml_exists_returns_false_if_parse_fails(self):
        self.assertFalse(self.secrets.load_if_toml_exists())

    @patch("streamlit.error")
    @patch("streamlit.config.get_option", return_value=[MOCK_SECRETS_FILE_LOC])
    def test_missing_toml_error(self, _, mock_st_error):
        """Secrets access raises an error, and calls st.error, if
        secrets.toml is missing.
        """
        with patch("builtins.open", mock_open()) as mock_file:
            mock_file.side_effect = FileNotFoundError()

            with self.assertRaises(FileNotFoundError):
                self.secrets.get("no_such_secret", None)

        mock_st_error.assert_called_once_with(
            f"No secrets found. Valid paths for a secrets.toml file or secret directories are: {MOCK_SECRETS_FILE_LOC}"
        )

    @patch("streamlit.error")
    @patch("streamlit.config.get_option", return_value=[MOCK_SECRETS_FILE_LOC])
    def test_missing_toml_error_with_suppressed_error(self, _, mock_st_error):
        """Secrets access raises an error, and does not calls st.error, if
        secrets.toml is missing because printing errors have been suppressed.
        """

        self.secrets.set_suppress_print_error_on_exception(True)

        with patch("builtins.open", mock_open()) as mock_file:
            mock_file.side_effect = FileNotFoundError()

            with self.assertRaises(FileNotFoundError):
                self.secrets.get("no_such_secret", None)

        mock_st_error.assert_not_called()

    @patch("builtins.open", new_callable=mock_open, read_data="invalid_toml")
    @patch("streamlit.error")
    @patch("streamlit.config.get_option", return_value=[MOCK_SECRETS_FILE_LOC])
    def test_malformed_toml_error(self, mock_get_option, mock_st_error, _):
        """Secrets access raises an error, and calls st.error, if
        secrets.toml is malformed.
        """
        with self.assertRaises(TomlDecodeError):
            self.secrets.get("no_such_secret", None)

        mock_st_error.assert_called_once_with(
            "Error parsing secrets file at /mock/secrets.toml: Key name found without value. Reached end of file. (line 1 column 13 char 12)"
        )

    @patch("streamlit.watcher.path_watcher.watch_file")
    @patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML)
    def test_getattr_nonexistent(self, *mocks):
        """Verify that access to missing attribute raises  AttributeError."""
        with self.assertRaises(AttributeError):
            self.secrets.nonexistent_secret  # noqa: B018

        with self.assertRaises(AttributeError):
            self.secrets.subsection.nonexistent_secret  # noqa: B018

    @patch("streamlit.watcher.path_watcher.watch_file")
    @patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML)
    def test_getattr_raises_exception_on_attr_dict(self, *mocks):
        """Verify that assignment to nested secrets raises TypeError."""
        with self.assertRaises(TypeError):
            self.secrets.subsection["new_secret"] = "123"

        with self.assertRaises(TypeError):
            self.secrets.subsection.new_secret = "123"

    @patch("streamlit.watcher.path_watcher.watch_file")
    @patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML)
    def test_getitem_nonexistent(self, *mocks):
        """Verify that access to missing key via dict notation raises KeyError."""
        with self.assertRaises(KeyError):
            self.secrets["nonexistent_secret"]

        with self.assertRaises(KeyError):
            self.secrets["subsection"]["nonexistent_secret"]

    @patch("streamlit.watcher.path_watcher.watch_file")
    @patch("streamlit.config.get_option", return_value=[MOCK_SECRETS_FILE_LOC])
    def test_reload_secrets_when_file_changes(self, mock_get_option, mock_watch_file):
        """When secrets.toml is loaded, the secrets file gets watched."""
        with patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML):
            self.assertEqual("Jane", self.secrets["db_username"])
            self.assertEqual("12345qwerty", self.secrets["db_password"])
            self.assertEqual("Jane", os.environ["db_username"])
            self.assertEqual("12345qwerty", os.environ["db_password"])

        # watch_file should have been called on the "secrets.toml" file with
        # the "poll" watcher_type. ("poll" is used here - rather than whatever
        # is set in config - because Streamlit Cloud loads secrets.toml from
        # a virtual filesystem that watchdog is unable to fire events for.)
        mock_watch_file.assert_called_once_with(
            MOCK_SECRETS_FILE_LOC,
            self.secrets._on_secrets_changed,
            watcher_type="poll",
        )

        # Mock the `send` method to later verify that it has been called.
        self.secrets.file_change_listener.send = MagicMock()

        # Change the text that will be loaded on the next call to `open`
        new_mock_toml = "db_username='Joan'"
        with patch("builtins.open", new_callable=mock_open, read_data=new_mock_toml):
            # Trigger a secrets file reload, ensure the secrets dict
            # gets repopulated as expected, and ensure that os.environ is
            # also updated properly.
            self.secrets._on_secrets_changed(MOCK_SECRETS_FILE_LOC)

            # A change in `secrets.toml` should emit a signal.
            self.secrets.file_change_listener.send.assert_called_once()

            self.assertEqual("Joan", self.secrets["db_username"])
            self.assertIsNone(self.secrets.get("db_password"))
            self.assertEqual("Joan", os.environ["db_username"])
            self.assertIsNone(os.environ.get("db_password"))


class MultipleSecretsFilesTest(unittest.TestCase):
    """Tests for st.secrets with multiple secrets.toml files."""

    def setUp(self) -> None:
        self._fd1, self._path1 = tempfile.mkstemp(".toml")
        self._fd2, self._path2 = tempfile.mkstemp(".toml")

        # st.secrets modifies os.environ, so we save it here and
        # restore in tearDown.
        self._prev_environ = dict(os.environ)

    def tearDown(self) -> None:
        os.environ.clear()
        os.environ.update(self._prev_environ)

        # close the file descriptors (which is required on windows before removing the file)
        for fd in (self._fd1, self._fd2):
            try:
                os.close(fd)
            except OSError:
                pass

        os.remove(self._path1)
        os.remove(self._path2)

    @patch("streamlit.error")
    def test_no_secrets_files_explodes(self, mock_st_error):
        """Validate that an error is thrown if none of the given secrets.toml files exist."""

        secrets_file_locations = [
            "/mock1/secrets.toml",
            "/mock2/secrets.toml",
        ]
        mock_get_option = testutil.build_mock_config_get_option(
            {"secrets.files": secrets_file_locations}
        )

        with patch("streamlit.config.get_option", new=mock_get_option):
            secrets = Secrets()

            with self.assertRaises(FileNotFoundError):
                secrets.get("no_such_secret", None)

            mock_st_error.assert_called_once_with(
                "No secrets found. Valid paths for a secrets.toml file or secret directories are: /mock1/secrets.toml, /mock2/secrets.toml"
            )

    @patch("streamlit.runtime.secrets._LOGGER")
    def test_only_one_secrets_file_fine(self, patched_logger):
        with os.fdopen(self._fd1, "w") as tmp:
            tmp.write(MOCK_TOML)

        secrets_file_locations = [
            self._path1,
            "/mock2/secrets.toml",
        ]
        mock_get_option = testutil.build_mock_config_get_option(
            {"secrets.files": secrets_file_locations}
        )

        with patch("streamlit.config.get_option", new=mock_get_option):
            secrets = Secrets()

            self.assertEqual(secrets.db_username, "Jane")
            patched_logger.info.assert_not_called()

    @patch("streamlit.runtime.secrets._LOGGER")
    def test_secret_overwriting(self, patched_logger):
        """Test that if both global and project-level secrets.toml files exist, secrets
        from both are present in st.secrets, and secrets from the project-level file
        "win" when secrets have conflicting names.
        """
        with os.fdopen(self._fd1, "w") as tmp:
            tmp.write(MOCK_TOML)

        with os.fdopen(self._fd2, "w") as tmp:
            tmp.write(
                """
db_password="54321dvorak"
hi="I'm new"

[subsection]
email2="eng2@streamlit.io"
"""
            )

        secrets_file_locations = [
            self._path1,
            self._path2,
        ]
        mock_get_option = testutil.build_mock_config_get_option(
            {"secrets.files": secrets_file_locations}
        )

        with patch("streamlit.config.get_option", new=mock_get_option):
            secrets = Secrets()

            # secrets.db_username is only defined in the first secrets.toml file, so it
            # remains unchanged.
            self.assertEqual(secrets.db_username, "Jane")

            # secrets.db_password should be overwritten because it's set to a different
            # value in our second secrets.toml file.
            self.assertEqual(secrets.db_password, "54321dvorak")

            # secrets.hi only appears in our second secrets.toml file.
            self.assertEqual(secrets.hi, "I'm new")

            # Secrets subsections are overwritten entirely rather than being merged.
            self.assertEqual(secrets.subsection, {"email2": "eng2@streamlit.io"})


class SecretsThreadingTests(unittest.TestCase):
    # The number of threads to run our tests on
    NUM_THREADS = 50

    def setUp(self) -> None:
        # st.secrets modifies os.environ, so we save it here and
        # restore in tearDown.
        self._prev_environ = dict(os.environ)
        self.secrets = Secrets()

    def tearDown(self) -> None:
        os.environ.clear()
        os.environ.update(self._prev_environ)

    @patch("streamlit.watcher.path_watcher.watch_file", MagicMock())
    @patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML)
    def test_access_secrets(self, _):
        """Accessing secrets is thread-safe."""

        def access_secrets(_: int) -> None:
            self.assertEqual(self.secrets["db_username"], "Jane")
            self.assertEqual(self.secrets["subsection"]["email"], "eng@streamlit.io")
            self.assertEqual(self.secrets["subsection"].email, "eng@streamlit.io")

        call_on_threads(access_secrets, num_threads=self.NUM_THREADS)

    @patch("streamlit.watcher.path_watcher.watch_file", MagicMock())
    @patch("builtins.open", new_callable=mock_open, read_data=MOCK_TOML)
    def test_reload_secrets(self, _):
        """Re-parsing the secrets file is thread-safe."""

        def reload_secrets(_: int) -> None:
            # Reset secrets, and then access a secret to reparse.
            self.secrets._reset()
            self.assertEqual(self.secrets["db_username"], "Jane")

        call_on_threads(reload_secrets, num_threads=self.NUM_THREADS)


class SecretsDirectoryTest(unittest.TestCase):
    def setUp(self) -> None:
        self.temp_dir = tempfile.TemporaryDirectory()
        self.temp_dir_path = self.temp_dir.name
        os.makedirs(os.path.join(self.temp_dir_path, "example_login"))
        with open(
            os.path.join(self.temp_dir_path, "example_login", "username"), "w"
        ) as f:
            f.write("example_username")
        with open(
            os.path.join(self.temp_dir_path, "example_login", "password"), "w"
        ) as f:
            f.write("example_password")
        os.makedirs(os.path.join(self.temp_dir_path, "example_token"))
        with open(os.path.join(self.temp_dir_path, "example_token", "token"), "w") as f:
            f.write("token123")

        self.secrets = Secrets()

    def tearDown(self) -> None:
        self.temp_dir.cleanup()

    @patch("streamlit.watcher.path_watcher.watch_dir")
    def test_access_secrets(self, mock_watch_dir):
        mock_get_option = testutil.build_mock_config_get_option(
            {"secrets.files": [self.temp_dir_path]}
        )

        with patch("streamlit.config.get_option", new=mock_get_option):
            self.assertEqual(
                self.secrets["example_login"]["username"], "example_username"
            )
            self.assertEqual(
                self.secrets["example_login"]["password"], "example_password"
            )
            self.assertEqual(self.secrets["example_token"], "token123")

            mock_watch_dir.assert_called_once_with(
                self.temp_dir_path,
                self.secrets._on_secrets_changed,
                watcher_type="poll",
            )

    @patch("streamlit.watcher.path_watcher.watch_dir", MagicMock())
    def test_secrets_reload(self):
        with open(
            os.path.join(self.temp_dir_path, "example_login", "password"), "w"
        ) as f:
            f.write("example_password2")

        mock_get_option = testutil.build_mock_config_get_option(
            {"secrets.files": [self.temp_dir_path]}
        )

        with patch("streamlit.config.get_option", new=mock_get_option):
            self.secrets._on_secrets_changed(self.temp_dir_path)
            self.assertEqual(
                self.secrets["example_login"]["username"], "example_username"
            )
            self.assertEqual(
                self.secrets["example_login"]["password"], "example_password2"
            )
            self.assertEqual(self.secrets["example_token"], "token123")


class AttrDictTest(unittest.TestCase):
    def test_attr_dict_is_mapping_but_not_built_in_dict(self):
        """Verify that AttrDict implements Mapping, but not built-in Dict"""
        attr_dict = AttrDict({"x": {"y": "z"}})
        self.assertIsInstance(attr_dict.x, Mapping)
        self.assertIsInstance(attr_dict.x, MappingABC)
        self.assertNotIsInstance(attr_dict.x, MutableMapping)
        self.assertNotIsInstance(attr_dict.x, MutableMappingABC)
        self.assertNotIsInstance(attr_dict.x, dict)

    def test_attr_dict_to_dict(self):
        d = {"x": {"y": "z"}}
        attr_dict = AttrDict(d)

        assert attr_dict.to_dict() == d

        # Also check that mutation on the return value of to_dict() does not
        # touch attr_dict or the original object.
        attr_dict.to_dict()["x"]["y"] = "zed"
        assert attr_dict.x.y == "z"
        assert d["x"]["y"] == "z"


================================================
File: /lib/tests/streamlit/runtime/stats_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import unittest

from streamlit.runtime.stats import (
    CacheStat,
    CacheStatsProvider,
    StatsManager,
    group_stats,
)


class MockStatsProvider(CacheStatsProvider):
    def __init__(self):
        self.stats: list[CacheStat] = []

    def get_stats(self) -> list[CacheStat]:
        return self.stats


class StatsManagerTest(unittest.TestCase):
    def test_get_stats(self):
        """StatsManager.get_stats should return all providers' stats."""
        manager = StatsManager()
        provider1 = MockStatsProvider()
        provider2 = MockStatsProvider()
        manager.register_provider(provider1)
        manager.register_provider(provider2)

        # No stats
        self.assertEqual([], manager.get_stats())

        # Some stats
        provider1.stats = [
            CacheStat("provider1", "foo", 1),
            CacheStat("provider1", "bar", 2),
        ]

        provider2.stats = [
            CacheStat("provider2", "baz", 3),
            CacheStat("provider2", "qux", 4),
        ]

        self.assertEqual(provider1.stats + provider2.stats, manager.get_stats())

    def test_group_stats(self):
        """Should return stats grouped by category_name and cache_name.
        byte_length should be summed."""

        # Similar stats sequential
        stats1 = [
            CacheStat("provider1", "foo", 1),
            CacheStat("provider1", "bar", 2),
            CacheStat("provider1", "bar", 5),
        ]

        # Similar stats not sequential
        stats2 = [
            CacheStat("provider2", "baz", 3),
            CacheStat("provider2", "qux", 4),
            CacheStat("provider2", "baz", 28),
        ]

        # All the same stats
        stats3 = [
            CacheStat("provider3", "boo", 1),
            CacheStat("provider3", "boo", 1),
            CacheStat("provider3", "boo", 1),
            CacheStat("provider3", "boo", 1),
            CacheStat("provider3", "boo", 1),
            CacheStat("provider3", "boo", 1),
            CacheStat("provider3", "boo", 1),
        ]

        self.assertEqual(
            set(group_stats(stats1)),
            {
                CacheStat("provider1", "foo", 1),
                CacheStat("provider1", "bar", 7),
            },
        )

        self.assertEqual(
            set(group_stats(stats2)),
            {
                CacheStat("provider2", "baz", 31),
                CacheStat("provider2", "qux", 4),
            },
        )

        self.assertEqual(
            set(group_stats(stats3)),
            {
                CacheStat("provider3", "boo", 7),
            },
        )


================================================
File: /lib/tests/streamlit/runtime/uploaded_file_manager_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Unit tests for UploadedFileManager"""

from __future__ import annotations

import unittest

from streamlit.runtime.memory_uploaded_file_manager import MemoryUploadedFileManager
from streamlit.runtime.stats import CacheStat
from streamlit.runtime.uploaded_file_manager import UploadedFileRec
from tests.exception_capturing_thread import call_on_threads

FILE_1 = UploadedFileRec(file_id="url1", name="file1", type="type", data=b"file1")
FILE_2 = UploadedFileRec(file_id="url2", name="file2", type="type", data=b"file222")


class UploadedFileManagerTest(unittest.TestCase):
    def setUp(self):
        self.mgr = MemoryUploadedFileManager("/mock/upload")

    def test_added_file_id(self):
        """Presigned file URL should have a unique ID."""
        info1, info2 = self.mgr.get_upload_urls("session", ["name1", "name1"])
        self.assertNotEqual(info1.file_id, info2.file_id)

    def test_retrieve_added_file(self):
        """An added file should maintain all its source properties
        except its ID."""
        self.mgr.add_file("session", FILE_1)
        self.mgr.add_file("session", FILE_2)

        file1_from_storage, *rest_files = self.mgr.get_files("session", ["url1"])
        self.assertEqual(len(rest_files), 0)
        self.assertEqual(file1_from_storage.file_id, FILE_1.file_id)
        self.assertEqual(file1_from_storage.name, FILE_1.name)
        self.assertEqual(file1_from_storage.type, FILE_1.type)
        self.assertEqual(file1_from_storage.data, FILE_1.data)

        file2_from_storage, *other_files = self.mgr.get_files("session", ["url2"])
        self.assertEqual(len(other_files), 0)
        self.assertEqual(file2_from_storage.file_id, FILE_2.file_id)
        self.assertEqual(file2_from_storage.name, FILE_2.name)
        self.assertEqual(file2_from_storage.type, FILE_2.type)
        self.assertEqual(file2_from_storage.data, FILE_2.data)

    def test_remove_file(self):
        # This should not error.
        self.mgr.remove_file("non-session", "non-file-id")

        self.mgr.add_file("session", FILE_1)
        self.mgr.remove_file("session", FILE_1.file_id)
        self.assertEqual([], self.mgr.get_files("session", [FILE_1.file_id]))

        # Remove the file again. It doesn't exist, but this isn't an error.
        self.mgr.remove_file("session", FILE_1.file_id)
        self.assertEqual([], self.mgr.get_files("session", [FILE_1.file_id]))

        self.mgr.add_file("session", FILE_1)
        self.mgr.add_file("session", FILE_2)
        self.mgr.remove_file("session", FILE_1.file_id)
        self.assertEqual(
            [FILE_2], self.mgr.get_files("session", [FILE_1.file_id, FILE_2.file_id])
        )

    def test_remove_session_files(self):
        # This should not error.
        self.mgr.remove_session_files("non-report")

        # Add two files with different session IDs, but the same widget ID.
        self.mgr.add_file("session1", FILE_1)
        self.mgr.add_file("session1", FILE_2)

        self.mgr.add_file("session2", FILE_1)

        self.mgr.remove_session_files("session1")
        self.assertEqual(
            [], self.mgr.get_files("session1", [FILE_1.file_id, FILE_2.file_id])
        )
        self.assertEqual([FILE_1], self.mgr.get_files("session2", [FILE_1.file_id]))

    def test_cache_stats_provider(self):
        """Test CacheStatsProvider implementation."""

        # Test empty manager
        self.assertEqual([], self.mgr.get_stats())

        # Test manager with files
        self.mgr.add_file("session1", FILE_1)
        self.mgr.add_file("session1", FILE_2)

        expected = [
            CacheStat(
                category_name="UploadedFileManager",
                cache_name="",
                byte_length=len(FILE_1.data) + len(FILE_2.data),
            ),
        ]
        self.assertEqual(expected, self.mgr.get_stats())


class UploadedFileManagerThreadingTest(unittest.TestCase):
    # The number of threads to run our tests on
    NUM_THREADS = 50

    def setUp(self) -> None:
        self.mgr = MemoryUploadedFileManager("/mock/upload")

    def test_add_file(self):
        """`add_file` is thread-safe."""
        # Call `add_file` from a bunch of threads
        added_files = []

        def add_file(index: int) -> None:
            file = UploadedFileRec(
                file_id=f"id_{index}",
                name=f"file_{index}",
                type="type",
                data=bytes(f"{index}", "utf-8"),
            )

            self.mgr.add_file("session", file)
            files_from_storage = self.mgr.get_files("session", [file.file_id])
            added_files.extend(files_from_storage)

        call_on_threads(add_file, num_threads=self.NUM_THREADS)

        # Ensure all our files are present
        for ii in range(self.NUM_THREADS):
            files = self.mgr.get_files("session", [f"id_{ii}"])
            self.assertEqual(1, len(files))
            self.assertEqual(bytes(f"{ii}", "utf-8"), files[0].data)

        # Ensure all files have unique IDs
        file_ids = set()
        for file_rec in self.mgr.file_storage["session"].values():
            file_ids.add(file_rec.file_id)
        self.assertEqual(self.NUM_THREADS, len(file_ids))

    def test_remove_file(self):
        """`remove_file` is thread-safe."""
        # Add a bunch of files to a single widget
        file_ids = []
        for ii in range(self.NUM_THREADS):
            file = UploadedFileRec(
                file_id=f"id_{ii}",
                name=f"file_{ii}",
                type="type",
                data=b"123",
            )
            self.mgr.add_file("session", file)
            file_ids.append(file.file_id)

        # Have each thread remove a single file
        def remove_file(index: int) -> None:
            file_id = file_ids[index]

            # Ensure our file exists
            get_files_result = self.mgr.get_files("session", [file_id])
            self.assertEqual(1, len(get_files_result))

            # Remove our file and ensure our file no longer exists
            self.mgr.remove_file("session", file_id)
            get_files_result = self.mgr.get_files("session", [file_id])
            self.assertEqual(0, len(get_files_result))

        call_on_threads(remove_file, self.NUM_THREADS)

        self.assertEqual(0, len(self.mgr.file_storage["session"]))

    def test_remove_session_files(self):
        """`remove_session_files` is thread-safe."""
        # Add a bunch of files, each to a different session
        file_ids = []
        for ii in range(self.NUM_THREADS):
            file = UploadedFileRec(
                file_id=f"id_{ii}",
                name=f"file_{ii}",
                type="type",
                data=b"123",
            )
            self.mgr.add_file(f"session_{ii}", file)
            file_ids.append(file.file_id)

        # Have each thread remove its session's file
        def remove_session_files(index: int) -> None:
            session_id = f"session_{index}"
            # Our file should exist
            session_files = self.mgr.get_files(session_id, [f"id_{index}"])
            self.assertEqual(1, len(session_files))
            self.assertEqual(file_ids[index], session_files[0].file_id)

            # Remove session files
            self.mgr.remove_session_files(session_id)

            # Our file should no longer exist
            session_files = self.mgr.get_files(session_id, [f"id_{index}"])
            self.assertEqual(0, len(session_files))

        call_on_threads(remove_session_files, num_threads=self.NUM_THREADS)


================================================
File: /lib/tests/streamlit/runtime/websocket_session_manager_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import unittest
from unittest.mock import MagicMock, patch

import pytest

from streamlit.runtime.script_data import ScriptData
from streamlit.runtime.session_manager import SessionStorage
from streamlit.runtime.websocket_session_manager import WebsocketSessionManager


class MockSessionStorage(SessionStorage):
    """A simple SessionStorage implementation used for testing.

    Essentially just a thin wrapper around a dict. This class exists so that we don't
    accidentally have our WebsocketSessionManager tests rely on a real SessionStorage
    implementation.
    """

    def __init__(self):
        self._cache = {}

    def get(self, session_id):
        return self._cache.get(session_id, None)

    def save(self, session_info):
        self._cache[session_info.session.id] = session_info

    def delete(self, session_id):
        del self._cache[session_id]

    def list(self):
        return list(self._cache.values())


@patch(
    "streamlit.runtime.app_session.asyncio.get_running_loop",
    new=MagicMock(),
)
@patch("streamlit.runtime.app_session.LocalSourcesWatcher", new=MagicMock())
@patch("streamlit.runtime.app_session.ScriptRunner", new=MagicMock())
class WebsocketSessionManagerTests(unittest.TestCase):
    def setUp(self):
        self.session_mgr = WebsocketSessionManager(
            session_storage=MockSessionStorage(),
            uploaded_file_manager=MagicMock(),
            script_cache=MagicMock(),
            message_enqueued_callback=MagicMock(),
        )

    def connect_session(self, existing_session_id=None, session_id_override=None):
        return self.session_mgr.connect_session(
            client=MagicMock(),
            script_data=ScriptData("/fake/script_path.py", is_hello=False),
            user_info={},
            existing_session_id=existing_session_id,
            session_id_override=session_id_override,
        )

    def test_connect_session(self):
        session_id = self.connect_session()
        session_info = self.session_mgr._active_session_info_by_id[session_id]

        assert session_info.session.id == session_id

    def test_connect_session_assert(self):
        with pytest.raises(AssertionError):
            self.connect_session(
                existing_session_id="existing_session_id",
                session_id_override="session_id_override",
            )

    def test_connect_session_with_session_id_override(self):
        self.connect_session(session_id_override="my_session_id")
        session_info = self.session_mgr._active_session_info_by_id["my_session_id"]

        assert session_info.session.id == "my_session_id"

    def test_connect_session_on_invalid_session_id(self):
        """Test that connect_session gives us a new session if existing_session_id is invalid."""
        session_id = self.connect_session(existing_session_id="not a valid session")
        session_info = self.session_mgr._active_session_info_by_id[session_id]

        assert session_info.session.id == session_id
        assert session_info.session.id != "not a valid session"

    @patch("streamlit.runtime.websocket_session_manager._LOGGER.warning")
    def test_connect_session_connects_new_session_if_already_connected(
        self, patched_warning
    ):
        session_id = self.connect_session()
        new_session_id = self.connect_session(existing_session_id=session_id)
        assert session_id != new_session_id

        patched_warning.assert_called_with(
            "Session with id %s is already connected! Connecting to a new session.",
            session_id,
        )

    def test_connect_session_explodes_if_ID_collission(self):
        session_id = self.connect_session()
        with pytest.raises(AssertionError):
            with patch(
                "streamlit.runtime.app_session.uuid.uuid4", return_value=session_id
            ):
                self.connect_session()

    @patch(
        "streamlit.runtime.app_session.AppSession.disconnect_file_watchers",
        new=MagicMock(),
    )
    @patch(
        "streamlit.runtime.app_session.AppSession.request_script_stop",
        new=MagicMock(),
    )
    @patch(
        "streamlit.runtime.app_session.AppSession.register_file_watchers",
        new=MagicMock(),
    )
    def test_disconnect_and_reconnect_session(self):
        session_id = self.connect_session()
        original_session_info = self.session_mgr.get_session_info(session_id)
        original_client = original_session_info.client

        # File watchers are registered on AppSession creation.
        original_session_info.session.register_file_watchers.assert_called_once()

        self.session_mgr.disconnect_session(session_id)

        assert session_id not in self.session_mgr._active_session_info_by_id
        assert session_id in self.session_mgr._session_storage._cache
        original_session_info.session.disconnect_file_watchers.assert_called_once()
        original_session_info.session.request_script_stop.assert_called_once()

        # Call disconnect_session again to verify that disconnect_session is idempotent.
        self.session_mgr.disconnect_session(session_id)

        assert session_id not in self.session_mgr._active_session_info_by_id
        assert session_id in self.session_mgr._session_storage._cache
        original_session_info.session.disconnect_file_watchers.assert_called_once()
        original_session_info.session.request_script_stop.assert_called_once()

        # Reconnect to the existing session.
        reconnected_session_id = self.connect_session(existing_session_id=session_id)
        reconnected_session_info = self.session_mgr.get_session_info(
            reconnected_session_id
        )

        assert reconnected_session_id == session_id
        assert reconnected_session_info.session == original_session_info.session
        assert reconnected_session_info != original_session_info
        assert reconnected_session_info.client != original_client
        # File watchers are registered on AppSession creation and again on AppSession
        # reconnect.
        assert reconnected_session_info.session.register_file_watchers.call_count == 2

    def test_disconnect_session_on_invalid_session_id(self):
        # Just check that no error is thrown.
        self.session_mgr.disconnect_session("nonexistent_session")

    def test_get_active_session_info(self):
        session_id = self.connect_session()

        active_session_info = self.session_mgr.get_active_session_info(session_id)
        assert active_session_info.session.id == session_id

    def test_get_active_session_info_on_invalid_session_id(self):
        assert self.session_mgr.get_active_session_info("nonexistent_session") is None

    def test_get_active_session_info_on_disconnected_session(self):
        session_id = self.connect_session()
        self.session_mgr.disconnect_session(session_id)

        assert self.session_mgr.get_active_session_info(session_id) is None

    def test_is_active_session(self):
        session_id = self.connect_session()
        assert self.session_mgr.is_active_session(session_id)

    def test_is_active_session_on_invalid_session_id(self):
        assert not self.session_mgr.is_active_session("nonexistent_session")

    def test_is_active_session_on_disconnected_session(self):
        session_id = self.connect_session()
        self.session_mgr.disconnect_session(session_id)

        assert not self.session_mgr.is_active_session(session_id)

    def test_list_active_sessions(self):
        session_ids = []
        for _ in range(3):
            session_ids.append(self.connect_session())

        assert [
            s.session.id for s in self.session_mgr.list_active_sessions()
        ] == session_ids

    @patch("streamlit.runtime.app_session.AppSession.shutdown", new=MagicMock())
    def test_close_session_on_active_session(self):
        session_id = self.connect_session()
        session_info = self.session_mgr.get_session_info(session_id)
        self.session_mgr.close_session(session_id)

        assert session_id not in self.session_mgr._active_session_info_by_id
        assert session_id not in self.session_mgr._session_storage._cache
        session_info.session.shutdown.assert_called_once()

    @patch("streamlit.runtime.app_session.AppSession.shutdown", new=MagicMock())
    def test_close_session_on_inactive_session(self):
        session_id = self.connect_session()
        session_info = self.session_mgr.get_session_info(session_id)
        self.session_mgr.disconnect_session(session_id)

        # Sanity check.
        assert not self.session_mgr.is_active_session(session_id)

        self.session_mgr.close_session(session_id)

        assert session_id not in self.session_mgr._active_session_info_by_id
        assert session_id not in self.session_mgr._session_storage._cache
        session_info.session.shutdown.assert_called_once()

    def test_close_session_on_invalid_session_id(self):
        self.session_mgr.close_session("nonexistent_session")

    def test_get_session_info_on_active_session(self):
        session_id = self.connect_session()
        session_info = self.session_mgr.get_session_info(session_id)

        assert session_info.session.id == session_id

    def test_get_session_info_on_inactive_session(self):
        session_id = self.connect_session()
        self.session_mgr.disconnect_session(session_id)

        # Sanity check.
        assert not self.session_mgr.is_active_session(session_id)

        session_info = self.session_mgr.get_session_info(session_id)
        assert session_info.session.id == session_id

    def test_get_session_info_on_invalid_session_id(self):
        assert self.session_mgr.get_session_info("nonexistent_session") is None

    def test_list_sessions(self):
        session_ids = []
        for _ in range(3):
            session_ids.append(self.connect_session())

        self.session_mgr.disconnect_session(session_ids[1])

        # Sanity check.
        assert self.session_mgr.is_active_session(session_ids[0])
        assert not self.session_mgr.is_active_session(session_ids[1])
        assert self.session_mgr.is_active_session(session_ids[2])

        assert {s.session.id for s in self.session_mgr.list_sessions()} == set(
            session_ids
        )


================================================
File: /lib/tests/streamlit/runtime/caching/__init__.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


================================================
File: /lib/tests/streamlit/runtime/caching/cache_data_api_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""st.cache_data unit tests."""

from __future__ import annotations

import logging
import os
import pickle
import re
import threading
import unittest
from typing import Any
from unittest.mock import MagicMock, Mock, mock_open, patch

from parameterized import parameterized

import streamlit as st
from streamlit import file_util
from streamlit.errors import StreamlitAPIException
from streamlit.proto.Text_pb2 import Text as TextProto
from streamlit.runtime import Runtime
from streamlit.runtime.caching import cached_message_replay
from streamlit.runtime.caching.cache_data_api import get_data_cache_stats_provider
from streamlit.runtime.caching.cache_errors import CacheError
from streamlit.runtime.caching.cached_message_replay import (
    CachedResult,
    ElementMsgData,
)
from streamlit.runtime.caching.hashing import UserHashError
from streamlit.runtime.caching.storage import (
    CacheStorage,
    CacheStorageContext,
    CacheStorageManager,
)
from streamlit.runtime.caching.storage.cache_storage_protocol import (
    InvalidCacheStorageContext,
)
from streamlit.runtime.caching.storage.dummy_cache_storage import (
    DummyCacheStorage,
    MemoryCacheStorageManager,
)
from streamlit.runtime.caching.storage.local_disk_cache_storage import (
    LocalDiskCacheStorageManager,
    get_cache_folder_path,
)
from streamlit.runtime.scriptrunner import add_script_run_ctx
from streamlit.runtime.stats import CacheStat
from tests.delta_generator_test_case import DeltaGeneratorTestCase
from tests.streamlit.element_mocks import (
    ELEMENT_PRODUCER,
    NON_WIDGET_ELEMENTS,
    WIDGET_ELEMENTS,
)
from tests.streamlit.runtime.caching.common_cache_test import (
    as_cached_result as _as_cached_result,
)
from tests.testutil import create_mock_script_run_ctx


def as_cached_result(value: Any) -> CachedResult:
    return _as_cached_result(value)


def as_replay_test_data() -> CachedResult:
    """Creates cached results for a function that returned 1
    and executed `st.text(1)`.
    """
    return CachedResult(
        1,
        [ElementMsgData("text", TextProto(body="1"), st._main.id, "")],
        st._main.id,
        st.sidebar.id,
    )


class CacheDataTest(unittest.TestCase):
    def setUp(self) -> None:
        # Caching functions rely on an active script run ctx
        add_script_run_ctx(threading.current_thread(), create_mock_script_run_ctx())
        mock_runtime = MagicMock(spec=Runtime)
        mock_runtime.cache_storage_manager = MemoryCacheStorageManager()
        Runtime._instance = mock_runtime

    def tearDown(self):
        # Some of these tests reach directly into _cache_info and twiddle it.
        # Reset default values on teardown.
        st.cache_data.clear()

    @patch.object(st, "exception")
    def test_mutate_return(self, exception):
        """Mutating a memoized return value is legal, and *won't* affect
        future accessors of the data."""

        @st.cache_data
        def f():
            return [0, 1]

        r1 = f()

        r1[0] = 1

        r2 = f()

        exception.assert_not_called()

        self.assertEqual(r1, [1, 1])
        self.assertEqual(r2, [0, 1])

    def test_cached_member_function_with_hash_func(self):
        """@st.cache_data can be applied to class member functions
        with corresponding hash_func.
        """

        class TestClass:
            @st.cache_data(
                hash_funcs={
                    "tests.streamlit.runtime.caching.cache_data_api_test.CacheDataTest.test_cached_member_function_with_hash_func.<locals>.TestClass": id
                }
            )
            def member_func(self):
                return "member func!"

            @classmethod
            @st.cache_data
            def class_method(cls):
                return "class method!"

            @staticmethod
            @st.cache_data
            def static_method():
                return "static method!"

        obj = TestClass()
        self.assertEqual("member func!", obj.member_func())
        self.assertEqual("class method!", obj.class_method())
        self.assertEqual("static method!", obj.static_method())

    def test_function_name_does_not_use_hashfuncs(self):
        """Hash funcs should only be used on arguments to a function,
        and not when computing the key for a function's unique MemCache.
        """

        str_hash_func = Mock(return_value=None)

        @st.cache_data(hash_funcs={str: str_hash_func})
        def foo(string_arg):
            return []

        # If our str hash_func is called multiple times, it's probably because
        # it's being used to compute the function's function_key (as opposed to
        # the value_key). It should only be used to compute the value_key!
        foo("ahoy")
        str_hash_func.assert_called_once_with("ahoy")

    @patch("streamlit.runtime.caching.cache_data_api.show_widget_replay_deprecation")
    def test_widget_replay_deprecation(self, show_warning_mock: Mock):
        """We show deprecation warnings when using the `experimental_allow_widgets` parameter."""

        # We show the deprecation warning at declaration time:
        @st.cache_data(experimental_allow_widgets=True)
        def foo():
            return 42

        show_warning_mock.assert_called_once()

    def test_user_hash_error(self):
        class MyObj:
            # we specify __repr__ here, to avoid `MyObj object at 0x1347a3f70`
            # in error message
            def __repr__(self):
                return "MyObj class"

        def bad_hash_func(x):
            x += 10  # Throws a TypeError since x has type MyObj.
            return x

        @st.cache_data(hash_funcs={MyObj: bad_hash_func})
        def user_hash_error_func(x):
            pass

        with self.assertRaises(UserHashError) as ctx:
            my_obj = MyObj()
            user_hash_error_func(my_obj)

        expected_message = """unsupported operand type(s) for +=: 'MyObj' and 'int'

This error is likely due to a bug in `bad_hash_func()`, which is a
user-defined hash function that was passed into the `@st.cache_data` decorator of
`user_hash_error_func()`.

`bad_hash_func()` failed when hashing an object of type
`tests.streamlit.runtime.caching.cache_data_api_test.CacheDataTest.test_user_hash_error.<locals>.MyObj`.  If you don't know where that object is coming from,
try looking at the hash chain below for an object that you do recognize, then
pass that to `hash_funcs` instead:

```
Object of type tests.streamlit.runtime.caching.cache_data_api_test.CacheDataTest.test_user_hash_error.<locals>.MyObj: MyObj class
```

If you think this is actually a Streamlit bug, please
[file a bug report here](https://github.com/streamlit/streamlit/issues/new/choose)."""
        self.assertEqual(str(ctx.exception), expected_message)

    def test_cached_st_function_clear_args(self):
        self.x = 0

        @st.cache_data()
        def foo(y):
            self.x += y
            return self.x

        assert foo(1) == 1
        foo.clear(2)
        assert foo(1) == 1
        foo.clear(1)
        assert foo(1) == 2

    def test_cached_class_method_clear_args(self):
        self.x = 0

        class ExampleClass:
            @st.cache_data
            def foo(_self, y):
                self.x += y
                return self.x

        example_instance = ExampleClass()
        # Calling method foo produces the side effect of incrementing self.x
        # and returning it as the result.

        # calling foo(1) should return 1
        assert example_instance.foo(1) == 1
        # calling foo.clear(2) should clear the cache for the argument 2,
        # and keep the cache for the argument 1, therefore calling foo(1) should return
        # cached value 1
        example_instance.foo.clear(2)
        assert example_instance.foo(1) == 1
        # calling foo.clear(1) should clear the cache for the argument 1,
        # therefore calling foo(1) should return the new value 2
        example_instance.foo.clear(1)
        assert example_instance.foo(1) == 2

        # Try the same with a keyword argument:
        example_instance.foo.clear(y=1)
        assert example_instance.foo(1) == 3

    def test_cached_class_method_clear(self):
        self.x = 0

        class ExampleClass:
            @st.cache_data
            def foo(_self, y):
                self.x += y
                return self.x

        example_instance = ExampleClass()
        # Calling method foo produces the side effect of incrementing self.x
        # and returning it as the result.

        # calling foo(1) should return 1
        assert example_instance.foo(1) == 1
        example_instance.foo.clear()
        # calling foo.clear() should clear all cached values:
        # So the call to foo() should return the new value 2
        assert example_instance.foo(1) == 2


class CacheDataPersistTest(DeltaGeneratorTestCase):
    """st.cache_data disk persistence tests"""

    def setUp(self) -> None:
        super().setUp()
        mock_runtime = MagicMock(spec=Runtime)
        mock_runtime.cache_storage_manager = LocalDiskCacheStorageManager()
        Runtime._instance = mock_runtime

    def tearDown(self) -> None:
        st.cache_data.clear()
        super().tearDown()

    @patch("streamlit.runtime.caching.storage.local_disk_cache_storage.streamlit_write")
    def test_dont_persist_by_default(self, mock_write):
        @st.cache_data
        def foo():
            return "data"

        foo()
        mock_write.assert_not_called()

    @patch("streamlit.runtime.caching.storage.local_disk_cache_storage.streamlit_write")
    def test_persist_path(self, mock_write):
        """Ensure we're writing to ~/.streamlit/cache/*.memo"""

        @st.cache_data(persist="disk")
        def foo():
            return "data"

        foo()
        mock_write.assert_called_once()

        write_path = mock_write.call_args[0][0]
        match = re.fullmatch(
            r"/mock/home/folder/.streamlit/cache/.*?\.memo", write_path
        )
        self.assertIsNotNone(match)

    @patch("streamlit.file_util.os.stat", MagicMock())
    @patch(
        "streamlit.file_util.open",
        mock_open(read_data=pickle.dumps(as_cached_result("mock_pickled_value"))),
    )
    @patch(
        "streamlit.runtime.caching.storage.local_disk_cache_storage.streamlit_read",
        wraps=file_util.streamlit_read,
    )
    def test_read_persisted_data(self, mock_read):
        """We should read persisted data from disk on cache miss."""

        @st.cache_data(persist="disk")
        def foo():
            return "actual_value"

        data = foo()
        mock_read.assert_called_once()
        self.assertEqual("mock_pickled_value", data)

    @patch("streamlit.file_util.os.stat", MagicMock())
    @patch("streamlit.file_util.open", mock_open(read_data="bad_pickled_value"))
    @patch(
        "streamlit.runtime.caching.storage.local_disk_cache_storage.streamlit_read",
        wraps=file_util.streamlit_read,
    )
    def test_read_bad_persisted_data(self, mock_read):
        """If our persisted data is bad, we raise an exception."""

        @st.cache_data(persist="disk")
        def foo():
            return "actual_value"

        with self.assertRaises(CacheError) as error:
            foo()
        mock_read.assert_called_once()
        self.assertEqual("Unable to read from cache", str(error.exception))

    @patch("streamlit.file_util.os.stat", MagicMock())
    @patch("streamlit.file_util.open", mock_open(read_data=b"bad_binary_pickled_value"))
    @patch(
        "streamlit.runtime.caching.storage.local_disk_cache_storage.streamlit_read",
        wraps=file_util.streamlit_read,
    )
    def test_read_bad_persisted_binary_data(self, mock_read):
        """If our persisted data is bad, we raise an exception."""

        @st.cache_data(persist="disk")
        def foo():
            return "actual_value"

        with self.assertRaises(CacheError) as error:
            foo()
        mock_read.assert_called_once()
        self.assertIn("Failed to unpickle", str(error.exception))

    def test_bad_persist_value(self):
        """Throw an error if an invalid value is passed to 'persist'."""
        with self.assertRaises(StreamlitAPIException) as e:

            @st.cache_data(persist="yesplz")
            def foo():
                pass

        self.assertEqual(
            "Unsupported persist option 'yesplz'. Valid values are 'disk' or None.",
            str(e.exception),
        )

    @patch("shutil.rmtree")
    def test_clear_all_disk_caches(self, mock_rmtree):
        """`clear_all` should remove the disk cache directory if it exists."""

        # If the cache dir exists, we should delete it.
        with patch("os.path.isdir", MagicMock(return_value=True)):
            st.cache_data.clear()
            mock_rmtree.assert_called_once_with(get_cache_folder_path())

        mock_rmtree.reset_mock()

        # If the cache dir does not exist, we shouldn't try to delete it.
        with patch("os.path.isdir", MagicMock(return_value=False)):
            st.cache_data.clear()
            mock_rmtree.assert_not_called()

    @patch("streamlit.file_util.os.stat", MagicMock())
    @patch(
        "streamlit.file_util.open",
        wraps=mock_open(read_data=pickle.dumps(as_cached_result("mock_pickled_value"))),
    )
    @patch("streamlit.runtime.caching.storage.local_disk_cache_storage.os.remove")
    def test_clear_one_disk_cache(self, mock_os_remove: Mock, mock_open: Mock):
        """A memoized function's clear_cache() property should just clear
        that function's cache."""

        @st.cache_data(persist="disk")
        def foo(val):
            return "actual_value"

        foo(0)
        foo(1)

        # We should've opened two files, one for each distinct "foo" call.
        self.assertEqual(2, mock_open.call_count)

        # Get the names of the two files that were created. These will look
        # something like '/mock/home/folder/.streamlit/cache/[long_hash].memo'
        created_filenames = {
            mock_open.call_args_list[0][0][0],
            mock_open.call_args_list[1][0][0],
        }

        created_files_base_names = [
            os.path.basename(filename) for filename in created_filenames
        ]

        mock_os_remove.assert_not_called()

        with patch(
            "os.listdir", MagicMock(return_value=created_files_base_names)
        ), patch("os.path.isdir", MagicMock(return_value=True)):
            # Clear foo's cache
            foo.clear()

        # os.remove should have been called once for each of our created cache files
        self.assertEqual(2, mock_os_remove.call_count)

        removed_filenames = {
            mock_os_remove.call_args_list[0][0][0],
            mock_os_remove.call_args_list[1][0][0],
        }

        # The two files we removed should be the same two files we created.
        self.assertEqual(created_filenames, removed_filenames)

    @patch("streamlit.file_util.os.stat", MagicMock())
    @patch(
        "streamlit.file_util.open",
        wraps=mock_open(read_data=pickle.dumps(as_replay_test_data())),
    )
    def test_cached_st_function_replay(self, _):
        @st.cache_data(persist="disk")
        def foo(i):
            st.text(i)
            return i

        foo(1)

        deltas = self.get_all_deltas_from_queue()
        text = [
            element.text.body
            for element in (delta.new_element for delta in deltas)
            if element.WhichOneof("type") == "text"
        ]
        assert text == ["1"]

    @patch("streamlit.file_util.os.stat", MagicMock())
    @patch(
        "streamlit.runtime.caching.storage.local_disk_cache_storage.streamlit_write",
        MagicMock(),
    )
    @patch(
        "streamlit.file_util.open",
        wraps=mock_open(read_data=pickle.dumps(1)),
    )
    def test_cached_st_function_clear_args_persist(self, _):
        self.x = 0

        @st.cache_data(persist="disk")
        def foo(y):
            self.x += y
            return self.x

        assert foo(1) == 1
        foo.clear(2)
        assert foo(1) == 1
        foo.clear(1)
        assert foo(1) == 2

    @patch("streamlit.file_util.os.stat", MagicMock())
    @patch(
        "streamlit.runtime.caching.storage.local_disk_cache_storage.streamlit_write",
        MagicMock(),
    )
    @patch(
        "streamlit.file_util.open",
        wraps=mock_open(read_data=pickle.dumps(1)),
    )
    def test_cached_format_migration(self, _):
        @st.cache_data(persist="disk")
        def foo(i):
            st.text(i)
            return i

        # Executes normally, without raising any errors
        foo(1)

    @patch("streamlit.runtime.caching.storage.local_disk_cache_storage.streamlit_write")
    def test_warning_memo_ttl_persist(self, _):
        """Using @st.cache_data with ttl and persist produces a warning."""
        with self.assertLogs(
            "streamlit.runtime.caching.storage.local_disk_cache_storage",
            level=logging.WARNING,
        ) as logs:

            @st.cache_data(ttl=60, persist="disk")
            def user_function():
                return 42

            st.write(user_function())

            output = "".join(logs.output)
            self.assertIn(
                "The cached function 'user_function' has a TTL that will be ignored.",
                output,
            )

    @parameterized.expand(
        [
            ("disk", "disk", True),
            ("True", True, True),
            ("None", None, False),
            ("False", False, False),
        ]
    )
    @patch("streamlit.runtime.caching.storage.local_disk_cache_storage.streamlit_write")
    def test_persist_param_value(
        self,
        _,
        persist_value: str | bool | None,
        should_persist: bool,
        mock_write: Mock,
    ):
        """Passing "disk" or `True` enables persistence; `None` or `False` disables it."""

        @st.cache_data(persist=persist_value)
        def foo():
            return "data"

        foo()

        if should_persist:
            mock_write.assert_called_once()
        else:
            mock_write.assert_not_called()


class CacheDataStatsProviderTest(unittest.TestCase):
    def setUp(self):
        # Caching functions rely on an active script run ctx
        add_script_run_ctx(threading.current_thread(), create_mock_script_run_ctx())
        mock_runtime = MagicMock(spec=Runtime)
        mock_runtime.cache_storage_manager = MemoryCacheStorageManager()
        Runtime._instance = mock_runtime

        # Guard against external tests not properly cache-clearing
        # in their teardowns.
        st.cache_data.clear()

    def tearDown(self):
        st.cache_data.clear()

    def test_no_stats(self):
        self.assertEqual([], get_data_cache_stats_provider().get_stats())

    def test_multiple_stats(self):
        @st.cache_data
        def foo(count):
            return [3.14] * count

        @st.cache_data
        def bar():
            return "shivermetimbers"

        foo(1)
        foo(53)
        bar()
        bar()

        foo_cache_name = f"{foo.__module__}.{foo.__qualname__}"
        bar_cache_name = f"{bar.__module__}.{bar.__qualname__}"

        expected = [
            CacheStat(
                category_name="st_cache_data",
                cache_name=foo_cache_name,
                byte_length=(
                    get_byte_length(as_cached_result([3.14] * 53))
                    + get_byte_length(as_cached_result([3.14]))
                ),
            ),
            CacheStat(
                category_name="st_cache_data",
                cache_name=bar_cache_name,
                byte_length=get_byte_length(as_cached_result("shivermetimbers")),
            ),
        ]

        # The order of these is non-deterministic, so check Set equality
        # instead of List equality
        self.assertEqual(
            set(expected), set(get_data_cache_stats_provider().get_stats())
        )


class CacheDataValidateParamsTest(DeltaGeneratorTestCase):
    """st.cache_data disk persistence tests"""

    def setUp(self) -> None:
        super().setUp()
        mock_runtime = MagicMock(spec=Runtime)
        mock_runtime.cache_storage_manager = AlwaysFailingTestCacheStorageManager()
        Runtime._instance = mock_runtime

    def test_error_logged_and_raised_on_improperly_configured_cache_data(self):
        with self.assertRaises(InvalidCacheStorageContext) as e, self.assertLogs(
            "streamlit.runtime.caching.cache_data_api", level=logging.ERROR
        ) as logs:

            @st.cache_data(persist="disk")
            def foo():
                return "data"

        self.assertEqual(str(e.exception), "This CacheStorageManager always fails")
        output = "".join(logs.output)
        self.assertIn("This CacheStorageManager always fails", output)


class CacheDataMessageReplayTest(DeltaGeneratorTestCase):
    def setUp(self):
        super().setUp()
        # Guard against external tests not properly cache-clearing
        # in their teardowns.
        st.cache_data.clear()

    def tearDown(self):
        st.cache_data.clear()

    @parameterized.expand(WIDGET_ELEMENTS)
    def test_shows_cached_widget_replay_warning(
        self, _widget_name: str, widget_producer: ELEMENT_PRODUCER
    ):
        """Test that a warning is shown when a widget is created inside a cached function."""

        if _widget_name == "experimental_audio_input":
            # The experimental_audio_input element produces also a deprecation warning
            # which makes this test irrelevant
            return

        @st.cache_data(show_spinner=False)
        def cache_widget():
            widget_producer()

        cache_widget()

        # There should be only two elements in the queue:
        assert len(self.get_all_deltas_from_queue()) == 2

        # The widget itself is still created, so we need to go back one element more:
        el = self.get_delta_from_queue(-2).new_element.exception
        assert el.type == "CachedWidgetWarning"
        assert el.is_warning is True

    @parameterized.expand(NON_WIDGET_ELEMENTS)
    def test_works_with_element_replay(
        self, element_name: str, element_producer: ELEMENT_PRODUCER
    ):
        """Test that it works with element replay if used as non-widget element."""

        if element_name == "toast":
            # The toast element is not supported in the cache_data API
            # since elements on the event dg are not supported.
            return

        @st.cache_data
        def cache_element():
            element_producer()

        with patch(
            "streamlit.runtime.caching.cache_utils.replay_cached_messages",
            wraps=cached_message_replay.replay_cached_messages,
        ) as replay_cached_messages_mock:
            # Call first time:
            cache_element()
            assert self.get_delta_from_queue().HasField("new_element") is True
            # The first time the cached function is called, the replay function is not called
            replay_cached_messages_mock.assert_not_called()

            # Call second time:
            cache_element()
            assert self.get_delta_from_queue().HasField("new_element") is True
            # The second time the cached function is called, the replay function is called
            replay_cached_messages_mock.assert_called()

            # Call third time:
            cache_element()
            assert self.get_delta_from_queue().HasField("new_element") is True
            # The third time the cached function is called, the replay function is called
            replay_cached_messages_mock.assert_called()


def get_byte_length(value):
    """Return the byte length of the pickled value."""
    return len(pickle.dumps(value))


class AlwaysFailingTestCacheStorageManager(CacheStorageManager):
    """A CacheStorageManager that always fails in check_context."""

    def create(self, context: CacheStorageContext) -> CacheStorage:
        return DummyCacheStorage()

    def clear_all(self) -> None:
        pass

    def check_context(self, context: CacheStorageContext) -> None:
        raise InvalidCacheStorageContext("This CacheStorageManager always fails")


================================================
File: /lib/tests/streamlit/runtime/caching/cache_errors_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import threading

import streamlit as st
from streamlit.elements import exception
from streamlit.proto.Exception_pb2 import Exception as ExceptionProto
from streamlit.runtime.caching.cache_errors import (
    UnhashableParamError,
    UnserializableReturnValueError,
)
from tests import testutil
from tests.delta_generator_test_case import DeltaGeneratorTestCase


class CacheErrorsTest(DeltaGeneratorTestCase):
    """Make sure user-visible error messages look correct.

    These errors are a little annoying to test, but they're important! So we
    are testing them word-for-word as much as possible. Even though this
    *feels* like an antipattern, it isn't: we're making sure the codepaths
    that pull useful debug info from the code are working.
    """

    maxDiff = None

    def test_unhashable_type(self):
        @st.cache_data
        def unhashable_type_func(lock: threading.Lock):
            return str(lock)

        with self.assertRaises(UnhashableParamError) as cm:
            unhashable_type_func(threading.Lock())

        ep = ExceptionProto()
        exception.marshall(ep, cm.exception)

        self.assertEqual(ep.type, "UnhashableParamError")

        expected_message = """
Cannot hash argument 'lock' (of type `_thread.lock`) in 'unhashable_type_func'.

To address this, you can tell Streamlit not to hash this argument by adding a
leading underscore to the argument's name in the function signature:

```
@st.cache_data
def unhashable_type_func(_lock, ...):
    ...
```
                    """

        self.assertEqual(
            testutil.normalize_md(expected_message), testutil.normalize_md(ep.message)
        )
        # Stack trace doesn't show in test :(
        # self.assertNotEqual(len(ep.stack_trace), 0)
        self.assertEqual(ep.message_is_markdown, True)
        self.assertEqual(ep.is_warning, False)

    def test_unserializable_return_value_error(self):
        @st.cache_data
        def unserializable_return_value_func():
            return threading.Lock()

        with self.assertRaises(UnserializableReturnValueError) as cm:
            unserializable_return_value_func()

        ep = ExceptionProto()
        exception.marshall(ep, cm.exception)

        self.assertEqual(ep.type, "UnserializableReturnValueError")

        self.assertIn("Cannot serialize the return value", ep.message)
        self.assertEqual(ep.message_is_markdown, True)
        self.assertEqual(ep.is_warning, False)


================================================
File: /lib/tests/streamlit/runtime/caching/cache_resource_api_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""st.cache_resource unit tests."""

from __future__ import annotations

import threading
import unittest
from typing import TYPE_CHECKING, Any
from unittest.mock import Mock, patch

from parameterized import parameterized

import streamlit as st
from streamlit.runtime.caching import (
    cache_resource_api,
    cached_message_replay,
    get_resource_cache_stats_provider,
)
from streamlit.runtime.caching.hashing import UserHashError
from streamlit.runtime.scriptrunner import add_script_run_ctx
from streamlit.runtime.stats import CacheStat
from streamlit.vendor.pympler.asizeof import asizeof
from tests.delta_generator_test_case import DeltaGeneratorTestCase
from tests.streamlit.element_mocks import (
    ELEMENT_PRODUCER,
    NON_WIDGET_ELEMENTS,
    WIDGET_ELEMENTS,
)
from tests.streamlit.runtime.caching.common_cache_test import (
    as_cached_result as _as_cached_result,
)
from tests.testutil import create_mock_script_run_ctx

if TYPE_CHECKING:
    from streamlit.runtime.caching.cached_message_replay import CachedResult


def as_cached_result(value: Any) -> CachedResult:
    return _as_cached_result(value)


class CacheResourceTest(unittest.TestCase):
    def setUp(self) -> None:
        # Caching functions rely on an active script run ctx
        add_script_run_ctx(threading.current_thread(), create_mock_script_run_ctx())

    def tearDown(self):
        st.cache_resource.clear()
        # Some of these tests reach directly into _cache_info and twiddle it.
        # Reset default values on teardown.

    @patch.object(st, "exception")
    def test_mutate_return(self, exception):
        """Mutating a cache_resource return value is legal, and *will* affect
        future accessors of the data."""

        @st.cache_resource
        def f():
            return [0, 1]

        r1 = f()

        r1[0] = 1

        r2 = f()

        exception.assert_not_called()

        self.assertEqual(r1, [1, 1])
        self.assertEqual(r2, [1, 1])

    @patch(
        "streamlit.runtime.caching.cache_resource_api.show_widget_replay_deprecation"
    )
    def test_widget_replay_deprecation(self, show_warning_mock: Mock):
        """We show deprecation warnings when using the `experimental_allow_widgets` parameter."""

        # We show the deprecation warning at declaration time:
        @st.cache_resource(experimental_allow_widgets=True)
        def foo():
            return 42

        show_warning_mock.assert_called_once()

    def test_cached_member_function_with_hash_func(self):
        """@st.cache_resource can be applied to class member functions
        with corresponding hash_func.
        """

        class TestClass:
            @st.cache_resource(
                hash_funcs={
                    "tests.streamlit.runtime.caching.cache_resource_api_test.CacheResourceTest.test_cached_member_function_with_hash_func.<locals>.TestClass": id
                }
            )
            def member_func(self):
                return "member func!"

            @classmethod
            @st.cache_resource
            def class_method(cls):
                return "class method!"

            @staticmethod
            @st.cache_resource
            def static_method():
                return "static method!"

        obj = TestClass()
        self.assertEqual("member func!", obj.member_func())
        self.assertEqual("class method!", obj.class_method())
        self.assertEqual("static method!", obj.static_method())

    def test_function_name_does_not_use_hashfuncs(self):
        """Hash funcs should only be used on arguments to a function,
        and not when computing the key for a function's unique MemCache.
        """

        str_hash_func = Mock(return_value=None)

        @st.cache_resource(hash_funcs={str: str_hash_func})
        def foo(string_arg):
            return []

        # If our str hash_func is called multiple times, it's probably because
        # it's being used to compute the function's function_key (as opposed to
        # the value_key). It should only be used to compute the value_key!
        foo("ahoy")
        str_hash_func.assert_called_once_with("ahoy")

    def test_user_hash_error(self):
        class MyObj:
            # we specify __repr__ here, to avoid `MyObj object at 0x1347a3f70`
            # in error message
            def __repr__(self):
                return "MyObj class"

        def bad_hash_func(x):
            x += 10  # Throws a TypeError since x has type MyObj.
            return x

        @st.cache_resource(hash_funcs={MyObj: bad_hash_func})
        def user_hash_error_func(x):
            pass

        with self.assertRaises(UserHashError) as ctx:
            my_obj = MyObj()
            user_hash_error_func(my_obj)

        expected_message = """unsupported operand type(s) for +=: 'MyObj' and 'int'

This error is likely due to a bug in `bad_hash_func()`, which is a
user-defined hash function that was passed into the `@st.cache_resource` decorator of
`user_hash_error_func()`.

`bad_hash_func()` failed when hashing an object of type
`tests.streamlit.runtime.caching.cache_resource_api_test.CacheResourceTest.test_user_hash_error.<locals>.MyObj`.  If you don't know where that object is coming from,
try looking at the hash chain below for an object that you do recognize, then
pass that to `hash_funcs` instead:

```
Object of type tests.streamlit.runtime.caching.cache_resource_api_test.CacheResourceTest.test_user_hash_error.<locals>.MyObj: MyObj class
```

If you think this is actually a Streamlit bug, please
[file a bug report here](https://github.com/streamlit/streamlit/issues/new/choose)."""
        self.assertEqual(str(ctx.exception), expected_message)

    def test_cached_st_function_clear_args(self):
        self.x = 0

        @st.cache_resource()
        def foo(y):
            self.x += y
            return self.x

        assert foo(1) == 1
        foo.clear(2)
        assert foo(1) == 1
        foo.clear(1)
        assert foo(1) == 2

    def test_cached_class_method_clear_args(self):
        self.x = 0

        class ExampleClass:
            @st.cache_resource()
            def foo(_self, y):
                self.x += y
                return self.x

        example_instance = ExampleClass()
        # Calling method foo produces the side effect of incrementing self.x
        # and returning it as the result.

        # calling foo(1) should return 1
        assert example_instance.foo(1) == 1
        # calling foo.clear(2) should clear the cache for the argument 2,
        # and keep the cache for the argument 1, therefore calling foo(1) should return
        # cached value 1
        example_instance.foo.clear(2)
        assert example_instance.foo(1) == 1
        # calling foo.clear(1) should clear the cache for the argument 1,
        # therefore calling foo(1) should return the new value 2
        example_instance.foo.clear(1)
        assert example_instance.foo(1) == 2

        # Try the same with a keyword argument:
        example_instance.foo.clear(y=1)
        assert example_instance.foo(1) == 3

    def test_cached_class_method_clear(self):
        self.x = 0

        class ExampleClass:
            @st.cache_resource()
            def foo(_self, y):
                self.x += y
                return self.x

        example_instance = ExampleClass()
        # Calling method foo produces the side effect of incrementing self.x
        # and returning it as the result.

        # calling foo(1) should return 1
        assert example_instance.foo(1) == 1
        example_instance.foo.clear()
        # calling foo.clear() should clear all cached values:
        # So the call to foo() should return the new value 2
        assert example_instance.foo(1) == 2


class CacheResourceValidateTest(unittest.TestCase):
    def setUp(self) -> None:
        # Caching functions rely on an active script run ctx
        add_script_run_ctx(threading.current_thread(), create_mock_script_run_ctx())

    def tearDown(self):
        st.cache_resource.clear()
        # Some of these tests reach directly into _cache_info and twiddle it.
        # Reset default values on teardown.
        cache_resource_api.CACHE_RESOURCE_MESSAGE_REPLAY_CTX._cached_func_stack = []

    def test_validate_success(self):
        """If we have a validate function and it returns True, we don't recompute our cached value."""
        validate = Mock(return_value=True)

        call_count: list[int] = [0]

        @st.cache_resource(validate=validate)
        def f() -> int:
            call_count[0] += 1
            return call_count[0]

        # First call: call_count == 1; validate not called (because we computed a new value)
        self.assertEqual(1, f())
        validate.assert_not_called()

        # Subsequent calls: call_count == 1; validate called each time
        for _ in range(3):
            self.assertEqual(1, f())
            validate.assert_called_once_with(1)
            validate.reset_mock()

    def test_validate_fail(self):
        """If we have a validate function and it returns False, we recompute our cached value."""
        validate = Mock(return_value=False)

        call_count: list[int] = [0]

        @st.cache_resource(validate=validate)
        def f() -> int:
            call_count[0] += 1
            return call_count[0]

        # First call: call_count == 1; validate not called (because we computed a new value)
        expected_call_count = 1
        self.assertEqual(expected_call_count, f())
        validate.assert_not_called()

        # Subsequent calls: call_count increases; validate called with previous value
        for _ in range(3):
            expected_call_count += 1
            self.assertEqual(expected_call_count, f())
            validate.assert_called_once_with(expected_call_count - 1)
            validate.reset_mock()


class CacheResourceStatsProviderTest(unittest.TestCase):
    def setUp(self):
        # Guard against external tests not properly cache-clearing
        # in their teardowns.
        st.cache_resource.clear()

        # Caching functions rely on an active script run ctx
        add_script_run_ctx(threading.current_thread(), create_mock_script_run_ctx())

    def tearDown(self):
        st.cache_resource.clear()

    def test_no_stats(self):
        self.assertEqual([], get_resource_cache_stats_provider().get_stats())

    def test_multiple_stats(self):
        @st.cache_resource
        def foo(count):
            return [3.14] * count

        @st.cache_resource
        def bar():
            return threading.Lock()

        foo(1)
        foo(53)
        bar()
        bar()

        foo_cache_name = f"{foo.__module__}.{foo.__qualname__}"
        bar_cache_name = f"{bar.__module__}.{bar.__qualname__}"

        expected = [
            CacheStat(
                category_name="st_cache_resource",
                cache_name=foo_cache_name,
                byte_length=(
                    get_byte_length(as_cached_result([3.14]))
                    + get_byte_length(as_cached_result([3.14] * 53))
                ),
            ),
            CacheStat(
                category_name="st_cache_resource",
                cache_name=bar_cache_name,
                byte_length=get_byte_length(as_cached_result(bar())),
            ),
        ]

        # The order of these is non-deterministic, so check Set equality
        # instead of List equality
        self.assertEqual(
            set(expected), set(get_resource_cache_stats_provider().get_stats())
        )


class CacheResourceMessageReplayTest(DeltaGeneratorTestCase):
    def setUp(self):
        super().setUp()
        # Guard against external tests not properly cache-clearing
        # in their teardowns.
        st.cache_resource.clear()

    def tearDown(self):
        st.cache_resource.clear()

    @parameterized.expand(WIDGET_ELEMENTS)
    def test_shows_cached_widget_replay_warning(
        self, _widget_name: str, widget_producer: ELEMENT_PRODUCER
    ):
        """Test that a warning is shown when a widget is created inside a cached function."""

        if _widget_name == "experimental_audio_input":
            # The experimental_audio_input element produces also a deprecation warning
            # which makes this test irrelevant
            return

        @st.cache_resource(show_spinner=False)
        def cache_widget():
            widget_producer()

        cache_widget()

        # There should be only two elements in the queue:
        assert len(self.get_all_deltas_from_queue()) == 2

        # The widget itself is still created, so we need to go back one element more:
        el = self.get_delta_from_queue(-2).new_element.exception
        assert el.type == "CachedWidgetWarning"
        assert el.is_warning is True

    @parameterized.expand(NON_WIDGET_ELEMENTS)
    def test_works_with_element_replay(
        self, element_name: str, element_producer: ELEMENT_PRODUCER
    ):
        """Test that it works with element replay if used as non-widget element."""

        if element_name == "toast":
            # The toast element is not supported in the cache_data API
            # since elements on the event dg are not supported.
            return

        @st.cache_resource
        def cache_element():
            element_producer()

        with patch(
            "streamlit.runtime.caching.cache_utils.replay_cached_messages",
            wraps=cached_message_replay.replay_cached_messages,
        ) as replay_cached_messages_mock:
            # Call first time:
            cache_element()
            assert self.get_delta_from_queue().HasField("new_element") is True
            # The first time the cached function is called, the replay function is not called
            replay_cached_messages_mock.assert_not_called()

            # Call second time:
            cache_element()
            assert self.get_delta_from_queue().HasField("new_element") is True
            # The second time the cached function is called, the replay function is called
            replay_cached_messages_mock.assert_called()

            # Call third time:
            cache_element()
            assert self.get_delta_from_queue().HasField("new_element") is True
            # The third time the cached function is called, the replay function is called
            replay_cached_messages_mock.assert_called()


def get_byte_length(value: Any) -> int:
    """Return the byte length of the pickled value."""
    return asizeof(value)


================================================
File: /lib/tests/streamlit/runtime/caching/common_cache_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Tests that are common to both st.cache_data and st.cache_resource"""

from __future__ import annotations

import threading
import time
import unittest
from datetime import timedelta
from typing import Any
from unittest.mock import MagicMock, Mock, patch

from parameterized import parameterized

import streamlit as st
from streamlit.runtime import Runtime
from streamlit.runtime.caching import cache_data, cache_resource
from streamlit.runtime.caching.cache_errors import CacheReplayClosureError
from streamlit.runtime.caching.cache_utils import CachedResult
from streamlit.runtime.caching.storage.dummy_cache_storage import (
    MemoryCacheStorageManager,
)
from streamlit.runtime.forward_msg_queue import ForwardMsgQueue
from streamlit.runtime.fragment import MemoryFragmentStorage
from streamlit.runtime.memory_uploaded_file_manager import MemoryUploadedFileManager
from streamlit.runtime.pages_manager import PagesManager
from streamlit.runtime.scriptrunner import (
    ScriptRunContext,
    add_script_run_ctx,
    get_script_run_ctx,
)
from streamlit.runtime.scriptrunner_utils import script_run_context
from streamlit.runtime.state import SafeSessionState, SessionState
from streamlit.testing.v1.app_test import AppTest
from tests.delta_generator_test_case import DeltaGeneratorTestCase
from tests.exception_capturing_thread import call_on_threads
from tests.streamlit.elements.image_test import create_image
from tests.testutil import create_mock_script_run_ctx


def get_text_or_block(delta):
    if delta.WhichOneof("type") == "new_element":
        element = delta.new_element
        if element.WhichOneof("type") == "text":
            return element.text.body
    elif delta.WhichOneof("type") == "add_block":
        return "new_block"


def as_cached_result(value: Any) -> CachedResult:
    """Creates cached results for a function that returned `value`
    and did not execute any elements.
    """
    return CachedResult(value, [], st._main.id, st.sidebar.id)


class CommonCacheTest(DeltaGeneratorTestCase):
    def tearDown(self):
        # Clear caches
        st.cache_data.clear()
        st.cache_resource.clear()

        # And some tests create widgets, and can result in DuplicateWidgetID
        # errors on subsequent runs.
        ctx = script_run_context.get_script_run_ctx()
        if ctx is not None:
            ctx.widget_ids_this_run.clear()
            ctx.widget_user_keys_this_run.clear()

        super().tearDown()

    def get_text_delta_contents(self) -> list[str]:
        deltas = self.get_all_deltas_from_queue()
        text = [
            element.text.body
            for element in (delta.new_element for delta in deltas)
            if element.WhichOneof("type") == "text"
        ]
        return text

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_simple(self, _, cache_decorator):
        @cache_decorator
        def foo():
            return 42

        self.assertEqual(foo(), 42)
        self.assertEqual(foo(), 42)

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_multiple_int_like_floats(self, _, cache_decorator):
        @cache_decorator
        def foo(x):
            return x

        self.assertEqual(foo(1.0), 1.0)
        self.assertEqual(foo(3.0), 3.0)

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_return_cached_object(self, _, cache_decorator):
        """If data has been cached, the cache function shouldn't be called."""
        with patch.object(st, "exception") as mock_exception:
            called = [False]

            @cache_decorator
            def f(x):
                called[0] = True
                return x

            self.assertFalse(called[0])
            f(0)

            self.assertTrue(called[0])

            called = [False]  # Reset called

            f(0)
            self.assertFalse(called[0])

            f(1)
            self.assertTrue(called[0])

            mock_exception.assert_not_called()

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_mutate_args(self, _, cache_decorator):
        """Mutating an argument inside a cached function doesn't throw
        an error (but it's probably not a great idea)."""
        with patch.object(st, "exception") as mock_exception:

            @cache_decorator
            def foo(d):
                d["answer"] += 1
                return d["answer"]

            d = {"answer": 0}

            self.assertEqual(foo(d), 1)
            self.assertEqual(foo(d), 2)

            mock_exception.assert_not_called()

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_ignored_args(self, _, cache_decorator):
        """Args prefixed with _ are not used as part of the cache key."""
        call_count = [0]

        @cache_decorator
        def foo(arg1, _arg2, *args, kwarg1, _kwarg2=None, **kwargs):
            call_count[0] += 1

        foo(1, 2, 3, kwarg1=4, _kwarg2=5, kwarg3=6, _kwarg4=7)
        self.assertEqual([1], call_count)

        # Call foo again, but change the values for _arg2, _kwarg2, and _kwarg4.
        # The call count shouldn't change, because these args will not be part
        # of the hash.
        foo(1, None, 3, kwarg1=4, _kwarg2=None, kwarg3=6, _kwarg4=None)
        self.assertEqual([1], call_count)

        # Changing the value of any other argument will increase the call
        # count. We test each argument type:

        # arg1 (POSITIONAL_OR_KEYWORD)
        foo(None, 2, 3, kwarg1=4, _kwarg2=5, kwarg3=6, _kwarg4=7)
        self.assertEqual([2], call_count)

        # *arg (VAR_POSITIONAL)
        foo(1, 2, None, kwarg1=4, _kwarg2=5, kwarg3=6, _kwarg4=7)
        self.assertEqual([3], call_count)

        # kwarg1 (KEYWORD_ONLY)
        foo(1, 2, 3, kwarg1=None, _kwarg2=5, kwarg3=6, _kwarg4=7)
        self.assertEqual([4], call_count)

        # **kwarg (VAR_KEYWORD)
        foo(1, 2, 3, kwarg1=4, _kwarg2=5, kwarg3=None, _kwarg4=7)
        self.assertEqual([5], call_count)

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_cached_member_function(self, _, cache_decorator):
        """Our cache decorators can be applied to class member functions."""

        class TestClass:
            @cache_decorator
            def member_func(_self):
                # We underscore-prefix `_self`, because our class is not hashable.
                return "member func!"

            @classmethod
            @cache_decorator
            def class_method(cls):
                return "class method!"

            @staticmethod
            @cache_decorator
            def static_method():
                return "static method!"

        obj = TestClass()
        self.assertEqual("member func!", obj.member_func())
        self.assertEqual("class method!", obj.class_method())
        self.assertEqual("static method!", obj.static_method())

    @parameterized.expand(
        [
            ("cache_data", cache_data),
            ("cache_resource", cache_resource),
        ]
    )
    def test_cached_st_function_warning(self, _, cache_decorator):
        """Ensure we properly warn when interactive st.foo functions are called
        inside a cached function.
        """
        forward_msg_queue = ForwardMsgQueue()
        orig_report_ctx = get_script_run_ctx()
        add_script_run_ctx(
            threading.current_thread(),
            ScriptRunContext(
                session_id="test session id",
                _enqueue=forward_msg_queue.enqueue,
                query_string="",
                session_state=SafeSessionState(SessionState(), lambda: None),
                uploaded_file_mgr=MemoryUploadedFileManager("/mock/upload"),
                main_script_path="",
                user_info={"email": "test@example.com"},
                fragment_storage=MemoryFragmentStorage(),
                pages_manager=PagesManager(""),
            ),
        )

        with patch.object(st, "exception") as warning:
            st.text("foo")
            warning.assert_not_called()

            @cache_decorator
            def cached_func():
                st.text("Inside cached func")

            cached_func()
            warning.assert_not_called()

            warning.reset_mock()

            # Make sure everything got reset properly
            st.text("foo")
            warning.assert_not_called()

            # Test nested st.cache functions
            @cache_decorator
            def outer():
                @cache_decorator
                def inner():
                    st.text("Inside nested cached func")

                return inner()

            outer()
            warning.assert_not_called()

            warning.reset_mock()

            # Test st.cache functions that raise errors
            with self.assertRaises(RuntimeError):

                @cache_decorator
                def cached_raise_error():
                    st.text("About to throw")
                    raise RuntimeError("avast!")

                cached_raise_error()

            warning.assert_not_called()
            warning.reset_mock()

            # Make sure everything got reset properly
            st.text("foo")
            warning.assert_not_called()

            # Test st.cache functions with widgets
            @cache_decorator
            def cached_widget():
                st.button("Press me!")

            cached_widget()

            warning.assert_called()
            warning.reset_mock()

            # Make sure everything got reset properly
            st.text("foo")
            warning.assert_not_called()

            add_script_run_ctx(threading.current_thread(), orig_report_ctx)

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_cached_st_function_replay(self, _, cache_decorator):
        @cache_decorator
        def foo_replay(i):
            st.text(i)
            return i

        foo_replay(1)
        st.text("---")
        foo_replay(1)

        text = self.get_text_delta_contents()

        assert text == ["1", "---", "1"]

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_cached_st_function_replay_nested(self, _, cache_decorator):
        @cache_decorator
        def inner(i):
            st.text(i)

        @cache_decorator
        def outer(i):
            inner(i)
            st.text(i + 10)

        outer(1)
        outer(1)
        st.text("---")
        inner(2)
        outer(2)
        st.text("---")
        outer(3)
        inner(3)

        text = self.get_text_delta_contents()
        assert text == [
            "1",
            "11",
            "1",
            "11",
            "---",
            "2",
            "2",
            "12",
            "---",
            "3",
            "13",
            "3",
        ]

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_cached_st_function_replay_outer_blocks(self, _, cache_decorator):
        @cache_decorator
        def foo(i):
            st.text(i)
            return i

        with st.container():
            foo(1)
            st.text("---")
            foo(1)

        text = self.get_text_delta_contents()
        assert text == ["1", "---", "1"]

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_cached_st_function_replay_sidebar(self, _, cache_decorator):
        @cache_decorator(show_spinner=False)
        def foo(i):
            st.sidebar.text(i)
            return i

        foo(1)  # [1,0]
        st.text("---")  # [0,0]
        foo(1)  # [1,1]

        text = [
            get_text_or_block(delta)
            for delta in self.get_all_deltas_from_queue()
            if get_text_or_block(delta) is not None
        ]
        assert text == ["1", "---", "1"]

        paths = [
            msg.metadata.delta_path
            for msg in self.forward_msg_queue._queue
            if msg.HasField("delta")
        ]
        assert paths == [[1, 0], [0, 0], [1, 1]]

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_cached_st_function_replay_inner_blocks(self, _, cache_decorator):
        @cache_decorator(show_spinner=False)
        def foo(i):
            with st.container():
                st.text(i)
                return i

        with st.container():  # [0,0]
            st.text(0)  # [0,0,0]
        st.text("---")  # [0,1]
        with st.container():  # [0,2]
            st.text(0)  # [0,2,0]

        foo(1)  # [0,3] and [0,3,0]
        st.text("---")  # [0,4]
        foo(1)  # [0,5] and [0,5,0]

        paths = [
            msg.metadata.delta_path
            for msg in self.forward_msg_queue._queue
            if msg.HasField("delta")
        ]
        assert paths == [
            [0, 0],
            [0, 0, 0],
            [0, 1],
            [0, 2],
            [0, 2, 0],
            [0, 3],
            [0, 3, 0],
            [0, 4],
            [0, 5],
            [0, 5, 0],
        ]

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_cached_st_function_replay_inner_direct(self, _, cache_decorator):
        @cache_decorator(show_spinner=False)
        def foo(i):
            cont = st.container()
            cont.text(i)
            return i

        foo(1)  # [0,0] and [0,0,0]
        st.text("---")  # [0,1]
        foo(1)  # [0,2] and [0,2,0]

        text = self.get_text_delta_contents()
        assert text == ["1", "---", "1"]

        paths = [
            msg.metadata.delta_path
            for msg in self.forward_msg_queue._queue
            if msg.HasField("delta")
        ]
        assert paths == [[0, 0], [0, 0, 0], [0, 1], [0, 2], [0, 2, 0]]

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_cached_st_function_replay_outer_direct(self, _, cache_decorator):
        cont = st.container()

        @cache_decorator
        def foo(i):
            cont.text(i)
            return i

        with self.assertRaises(CacheReplayClosureError):
            foo(1)
            st.text("---")
            foo(1)

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_cached_st_image_replay(self, _, cache_decorator):
        """Basic sanity check that nothing blows up. This test assumes that
        actual caching/replay functionality are covered by e2e tests that
        can more easily test them.
        """

        @cache_decorator
        def img_fn():
            st.image(create_image(10))

        img_fn()
        img_fn()

        @cache_decorator
        def img_fn_multi():
            st.image([create_image(5), create_image(15), create_image(100)])

        img_fn_multi()
        img_fn_multi()

    @parameterized.expand(
        [
            ("cache_data", cache_data, cache_data.clear),
            ("cache_resource", cache_resource, cache_resource.clear),
        ]
    )
    def test_clear_all_caches(self, _, cache_decorator, clear_cache_func):
        """Calling a cache's global `clear_all` function should remove all
        items from all caches of the appropriate type.
        """
        foo_vals = []

        @cache_decorator
        def foo(x):
            foo_vals.append(x)
            return x

        bar_vals = []

        @cache_decorator
        def bar(x):
            bar_vals.append(x)
            return x

        foo(0), foo(1), foo(2)
        bar(0), bar(1), bar(2)
        self.assertEqual([0, 1, 2], foo_vals)
        self.assertEqual([0, 1, 2], bar_vals)

        # Clear the cache and access our original values again. They
        # should be recomputed.
        clear_cache_func()

        foo(0), foo(1), foo(2)
        bar(0), bar(1), bar(2)
        self.assertEqual([0, 1, 2, 0, 1, 2], foo_vals)
        self.assertEqual([0, 1, 2, 0, 1, 2], bar_vals)

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_clear_single_cache(self, _, cache_decorator):
        foo_call_count = [0]

        @cache_decorator
        def foo():
            foo_call_count[0] += 1

        bar_call_count = [0]

        @cache_decorator
        def bar():
            bar_call_count[0] += 1

        foo(), foo(), foo()
        bar(), bar(), bar()
        self.assertEqual(1, foo_call_count[0])
        self.assertEqual(1, bar_call_count[0])

        # Clear just foo's cache, and call the functions again.
        foo.clear()

        foo(), foo(), foo()
        bar(), bar(), bar()

        # Foo will have been called a second time, and bar will still
        # have been called just once.
        self.assertEqual(2, foo_call_count[0])
        self.assertEqual(1, bar_call_count[0])

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_without_spinner(self, _, cache_decorator):
        """If the show_spinner flag is not set, the report queue should be
        empty.
        """

        @cache_decorator(show_spinner=False)
        def function_without_spinner(x: int) -> int:
            return x

        function_without_spinner(3)
        self.assertTrue(self.forward_msg_queue.is_empty())

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_with_spinner(self, _, cache_decorator):
        """If the show_spinner flag is set, there should be one element in the
        report queue.
        """

        @cache_decorator(show_spinner=True)
        def function_with_spinner(x: int) -> int:
            return x

        function_with_spinner(3)
        self.assertFalse(self.forward_msg_queue.is_empty())

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_with_custom_text_spinner(self, _, cache_decorator):
        """If the show_spinner flag is set, there should be one element in the
        report queue.
        """

        @cache_decorator(show_spinner="CUSTOM_TEXT")
        def function_with_spinner_custom_text(x: int) -> int:
            return x

        function_with_spinner_custom_text(3)
        self.assertFalse(self.forward_msg_queue.is_empty())

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_with_empty_text_spinner(self, _, cache_decorator):
        """If the show_spinner flag is set, even if it is empty text,
        there should be one element in the report queue.
        """

        @cache_decorator(show_spinner="")
        def function_with_spinner_empty_text(x: int) -> int:
            return x

        function_with_spinner_empty_text(3)
        self.assertFalse(self.forward_msg_queue.is_empty())

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_spinner_with_nested_cached_functions(self, _, cache_decorator):
        """If a cached function calls another cached function, only one spinner
        should be created.
        """

        @cache_decorator(show_spinner="")
        def inner(x: int) -> int:
            return x

        @cache_decorator(show_spinner="")
        def outer(x: int) -> int:
            return inner(x)

        outer(3)
        self.assertFalse(self.forward_msg_queue.is_empty())

        # The spinner uses an empty element and shows the spinner only
        # after a timeout. Instead of mocking the time and waiting for the
        # timeout, we check for the empty element in the queue as the spinner's
        # surrogate.
        empty_elements_count = 0
        for msg in self.forward_msg_queue._queue:
            if (
                msg.HasField("delta")
                and msg.delta.HasField("new_element")
                and msg.delta.new_element.HasField("empty")
            ):
                empty_elements_count += 1
        # Since we automatically prevent spinners for nested cached functions,
        # there should only be a single empty element.
        self.assertEqual(empty_elements_count, 1)


class CommonCacheTTLTest(unittest.TestCase):
    def setUp(self) -> None:
        # Caching functions rely on an active script run ctx
        add_script_run_ctx(threading.current_thread(), create_mock_script_run_ctx())
        mock_runtime = MagicMock(spec=Runtime)
        mock_runtime.cache_storage_manager = MemoryCacheStorageManager()
        Runtime._instance = mock_runtime

    def tearDown(self):
        cache_data.clear()
        cache_resource.clear()

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    @patch("streamlit.runtime.caching.cache_utils.TTLCACHE_TIMER")
    def test_ttl(self, _, cache_decorator, timer_patch: Mock):
        """Entries should expire after the given ttl."""
        one_day = 60 * 60 * 24

        # Create 2 cached functions to test that they don't interfere
        # with each other.
        foo_vals = []

        @cache_decorator(ttl=one_day)
        def foo(x):
            foo_vals.append(x)
            return x

        bar_vals = []

        @cache_decorator(ttl=one_day * 2)
        def bar(x):
            bar_vals.append(x)
            return x

        # Store a value at time 0
        timer_patch.return_value = 0
        foo(0)
        bar(0)
        self.assertEqual([0], foo_vals)
        self.assertEqual([0], bar_vals)

        # Advance our timer, but not enough to expire our value.
        timer_patch.return_value = one_day * 0.5
        foo(0)
        bar(0)
        self.assertEqual([0], foo_vals)
        self.assertEqual([0], bar_vals)

        # Advance our timer enough to expire foo, but not bar.
        timer_patch.return_value = one_day * 1.5
        foo(0)
        bar(0)
        self.assertEqual([0, 0], foo_vals)
        self.assertEqual([0], bar_vals)

        # Expire bar. Foo's second value was inserted at time=1.5 days,
        # so it won't expire until time=2.5 days
        timer_patch.return_value = (one_day * 2) + 1
        foo(0)
        bar(0)
        self.assertEqual([0, 0], foo_vals)
        self.assertEqual([0, 0], bar_vals)

        # Expire foo for a second time.
        timer_patch.return_value = (one_day * 2.5) + 1
        foo(0)
        bar(0)
        self.assertEqual([0, 0, 0], foo_vals)
        self.assertEqual([0, 0], bar_vals)

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    @patch("streamlit.runtime.caching.cache_utils.TTLCACHE_TIMER")
    def test_ttl_timedelta(self, _, cache_decorator, timer_patch: Mock):
        """Entries should expire after the given ttl."""
        one_day_seconds = 60 * 60 * 24
        one_day_timedelta = timedelta(days=1)
        two_days_timedelta = timedelta(days=2)

        # Create 2 cached functions to test that they don't interfere
        # with each other.
        foo_vals = []

        @cache_decorator(ttl=one_day_timedelta)
        def foo(x):
            foo_vals.append(x)
            return x

        bar_vals = []

        @cache_decorator(ttl=two_days_timedelta)
        def bar(x):
            bar_vals.append(x)
            return x

        # Store a value at time 0
        timer_patch.return_value = 0
        foo(0)
        bar(0)
        self.assertEqual([0], foo_vals)
        self.assertEqual([0], bar_vals)

        # Advance our timer, but not enough to expire our value.
        timer_patch.return_value = one_day_seconds * 0.5
        foo(0)
        bar(0)
        self.assertEqual([0], foo_vals)
        self.assertEqual([0], bar_vals)

        # Advance our timer enough to expire foo, but not bar.
        timer_patch.return_value = one_day_seconds * 1.5
        foo(0)
        bar(0)
        self.assertEqual([0, 0], foo_vals)
        self.assertEqual([0], bar_vals)

        # Expire bar. Foo's second value was inserted at time=1.5 days,
        # so it won't expire until time=2.5 days
        timer_patch.return_value = (one_day_seconds * 2) + 1
        foo(0)
        bar(0)
        self.assertEqual([0, 0], foo_vals)
        self.assertEqual([0, 0], bar_vals)

        # Expire foo for a second time.
        timer_patch.return_value = (one_day_seconds * 2.5) + 1
        foo(0)
        bar(0)
        self.assertEqual([0, 0, 0], foo_vals)
        self.assertEqual([0, 0], bar_vals)


class CommonCacheThreadingTest(unittest.TestCase):
    # The number of threads to run our tests on
    NUM_THREADS = 50

    def setUp(self):
        mock_runtime = MagicMock(spec=Runtime)
        mock_runtime.cache_storage_manager = MemoryCacheStorageManager()
        Runtime._instance = mock_runtime

    def tearDown(self):
        # Some of these tests reach directly into CALL_STACK data and twiddle it.
        # Reset default values on teardown.

        # Clear caches
        st.cache_data.clear()
        st.cache_resource.clear()

        # And some tests create widgets, and can result in DuplicateWidgetID
        # errors on subsequent runs.
        ctx = script_run_context.get_script_run_ctx()
        if ctx is not None:
            ctx.widget_ids_this_run.clear()
            ctx.widget_user_keys_this_run.clear()

        super().tearDown()

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_get_cache(self, _, cache_decorator):
        """Accessing a cached value is safe from multiple threads."""

        cached_func_call_count = [0]

        @cache_decorator
        def foo():
            cached_func_call_count[0] += 1
            return 42

        def call_foo(_: int) -> None:
            self.assertEqual(42, foo())

        # Call foo from multiple threads and assert no errors.
        call_on_threads(call_foo, self.NUM_THREADS)

        # The cached function should only be called once (see `test_compute_value_only_once`).
        self.assertEqual(1, cached_func_call_count[0])

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_compute_value_only_once(self, _, cache_decorator):
        """Cached values should be computed only once, even if multiple sessions read from an
        unwarmed cache simultaneously.
        """
        cached_func_call_count = [0]

        @cache_decorator
        def foo():
            self.assertEqual(
                0,
                cached_func_call_count[0],
                "A cached value was computed multiple times!",
            )
            cached_func_call_count[0] += 1

            # Sleep to "guarantee" that our other threads try to access the
            # cached data while it's being computed. (The other threads should
            # block on cache computation, so this function should only
            # be called a single time.)
            time.sleep(0.25)
            return 42

        def call_foo(_: int) -> None:
            self.assertEqual(42, foo())

        call_on_threads(call_foo, num_threads=self.NUM_THREADS, timeout=0.5)

    @parameterized.expand(
        [
            ("cache_data", cache_data, cache_data.clear),
            ("cache_resource", cache_resource, cache_resource.clear),
        ]
    )
    def test_clear_all_caches(self, _, cache_decorator, clear_cache_func):
        """Clearing all caches is safe to call from multiple threads."""

        @cache_decorator
        def foo():
            return 42

        # Populate the cache
        foo()

        def clear_caches(_: int) -> None:
            clear_cache_func()

        # Clear the cache from a bunch of threads and assert no errors.
        call_on_threads(clear_caches, self.NUM_THREADS)

        # Sanity check: ensure we can still call our cached function.
        self.assertEqual(42, foo())

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_clear_single_cache(self, _, cache_decorator):
        """It's safe to clear a single function cache from multiple threads."""

        @cache_decorator
        def foo():
            return 42

        # Populate the cache
        foo()

        def clear_foo(_: int) -> None:
            foo.clear()

        # Clear it from a bunch of threads and assert no errors.
        call_on_threads(clear_foo, self.NUM_THREADS)

        # Sanity check: ensure we can still call our cached function.
        self.assertEqual(42, foo())


def test_arrow_replay():
    """Regression test for https://github.com/streamlit/streamlit/issues/6103"""
    at = AppTest.from_file("test_data/arrow_replay.py").run()

    assert not at.exception


================================================
File: /lib/tests/streamlit/runtime/caching/hashing_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""st.memo/singleton hashing tests."""

from __future__ import annotations

import datetime
import functools
import hashlib
import os
import re
import tempfile
import time
import types
import unittest
import uuid
from dataclasses import dataclass
from enum import Enum, auto
from io import BytesIO, StringIO
from unittest.mock import MagicMock, Mock

import numpy as np
import pandas as pd
from parameterized import parameterized
from PIL import Image

from streamlit.proto.Common_pb2 import FileURLs
from streamlit.runtime.caching import cache_data, cache_resource
from streamlit.runtime.caching.cache_errors import UnhashableTypeError
from streamlit.runtime.caching.cache_type import CacheType
from streamlit.runtime.caching.hashing import (
    _NP_SIZE_LARGE,
    _PANDAS_ROWS_LARGE,
    UserHashError,
    update_hash,
)
from streamlit.runtime.uploaded_file_manager import UploadedFile, UploadedFileRec
from streamlit.type_util import is_type
from streamlit.util import HASHLIB_KWARGS

get_main_script_director = MagicMock(return_value=os.getcwd())


def get_hash(value, hash_funcs=None, cache_type=None):
    hasher = hashlib.new("md5", **HASHLIB_KWARGS)
    update_hash(
        value, hasher, cache_type=cache_type or MagicMock(), hash_funcs=hash_funcs
    )
    return hasher.digest()


class HashTest(unittest.TestCase):
    def test_string(self):
        self.assertEqual(get_hash("hello"), get_hash("hello"))
        self.assertNotEqual(get_hash("hello"), get_hash("hell"))

    def test_int(self):
        self.assertEqual(get_hash(145757624235), get_hash(145757624235))
        self.assertNotEqual(get_hash(10), get_hash(11))
        self.assertNotEqual(get_hash(-1), get_hash(1))
        self.assertNotEqual(get_hash(2**7), get_hash(2**7 - 1))
        self.assertNotEqual(get_hash(2**7), get_hash(2**7 + 1))

    def test_uuid(self):
        uuid1 = uuid.uuid4()
        uuid1_copy = uuid.UUID(uuid1.hex)
        uuid2 = uuid.uuid4()

        # Our hashing functionality should work with UUIDs
        # regardless of UUID factory function.

        uuid3 = uuid.uuid5(uuid.NAMESPACE_DNS, "streamlit.io")
        uuid3_copy = uuid.UUID(uuid3.hex)
        uuid4 = uuid.uuid5(uuid.NAMESPACE_DNS, "snowflake.com")

        self.assertEqual(get_hash(uuid1), get_hash(uuid1_copy))
        self.assertNotEqual(id(uuid1), id(uuid1_copy))
        self.assertNotEqual(get_hash(uuid1), get_hash(uuid2))

        self.assertEqual(get_hash(uuid3), get_hash(uuid3_copy))
        self.assertNotEqual(id(uuid3), id(uuid3_copy))
        self.assertNotEqual(get_hash(uuid3), get_hash(uuid4))

    def test_datetime_naive(self):
        naive_datetime1 = datetime.datetime(2007, 12, 23, 15, 45, 55)
        naive_datetime1_copy = datetime.datetime(2007, 12, 23, 15, 45, 55)
        naive_datetime3 = datetime.datetime(2011, 12, 21, 15, 45, 55)

        self.assertEqual(get_hash(naive_datetime1), get_hash(naive_datetime1_copy))
        self.assertNotEqual(id(naive_datetime1), id(naive_datetime1_copy))
        self.assertNotEqual(get_hash(naive_datetime1), get_hash(naive_datetime3))

    def test_datetime_aware(self):
        tz_info = datetime.timezone.utc
        aware_datetime1 = datetime.datetime(2007, 12, 23, 15, 45, 55, tzinfo=tz_info)
        aware_datetime1_copy = datetime.datetime(
            2007, 12, 23, 15, 45, 55, tzinfo=tz_info
        )
        aware_datetime2 = datetime.datetime(2011, 12, 21, 15, 45, 55, tzinfo=tz_info)

        # naive datetime1 is the same datetime that aware_datetime,
        # but without timezone info. They should have different hashes.
        naive_datetime1 = datetime.datetime(2007, 12, 23, 15, 45, 55)

        self.assertEqual(get_hash(aware_datetime1), get_hash(aware_datetime1_copy))
        self.assertNotEqual(id(aware_datetime1), id(aware_datetime1_copy))
        self.assertNotEqual(get_hash(aware_datetime1), get_hash(aware_datetime2))
        self.assertNotEqual(get_hash(aware_datetime1), get_hash(naive_datetime1))

    @parameterized.expand(
        [
            "US/Pacific",
            "America/Los_Angeles",
            "Europe/Berlin",
            "UTC",
            None,  # check for naive too
        ]
    )
    def test_pandas_timestamp(self, tz_info):
        timestamp1 = pd.Timestamp("2017-01-01T12", tz=tz_info)
        timestamp1_copy = pd.Timestamp("2017-01-01T12", tz=tz_info)
        timestamp2 = pd.Timestamp("2019-01-01T12", tz=tz_info)

        self.assertEqual(get_hash(timestamp1), get_hash(timestamp1_copy))
        self.assertNotEqual(id(timestamp1), id(timestamp1_copy))
        self.assertNotEqual(get_hash(timestamp1), get_hash(timestamp2))

    def test_mocks_do_not_result_in_infinite_recursion(self):
        try:
            get_hash(Mock())
            get_hash(MagicMock())
        except BaseException:
            self.fail("get_hash raised an exception")

    def test_list(self):
        self.assertEqual(get_hash([1, 2]), get_hash([1, 2]))
        self.assertNotEqual(get_hash([1, 2]), get_hash([2, 2]))
        self.assertNotEqual(get_hash([1]), get_hash(1))

        # test that we can hash self-referencing lists
        a = [1, 2, 3]
        a.append(a)
        b = [1, 2, 3]
        b.append(b)
        self.assertEqual(get_hash(a), get_hash(b))

    @parameterized.expand(
        [("cache_data", cache_data), ("cache_resource", cache_resource)]
    )
    def test_recursive_hash_func(self, _, cache_decorator):
        """Test that if user defined hash_func returns the value of the same type
        that hash_funcs tries to cache, we break the recursive cycle with predefined
        placeholder"""

        def hash_int(x):
            return x

        @cache_decorator(hash_funcs={int: hash_int})
        def foo(x):
            return x

        self.assertEqual(foo(1), foo(1))
        # Note: We're able to break the recursive cycle caused by the identity
        # hash func but it causes all cycles to hash to the same thing.
        # https://github.com/streamlit/streamlit/issues/1659
        # self.assertNotEqual(foo(2), foo(1))

    def test_tuple(self):
        self.assertEqual(get_hash((1, 2)), get_hash((1, 2)))
        self.assertNotEqual(get_hash((1, 2)), get_hash((2, 2)))
        self.assertNotEqual(get_hash((1,)), get_hash(1))
        self.assertNotEqual(get_hash((1,)), get_hash([1]))

    def test_mappingproxy(self):
        a = types.MappingProxyType({"a": 1})
        b = types.MappingProxyType({"a": 1})
        c = types.MappingProxyType({"c": 1})

        self.assertEqual(get_hash(a), get_hash(b))
        self.assertNotEqual(get_hash(a), get_hash(c))

    def test_dict_items(self):
        a = types.MappingProxyType({"a": 1}).items()
        b = types.MappingProxyType({"a": 1}).items()
        c = types.MappingProxyType({"c": 1}).items()

        assert is_type(a, "builtins.dict_items")
        self.assertEqual(get_hash(a), get_hash(b))
        self.assertNotEqual(get_hash(a), get_hash(c))

    def test_getset_descriptor(self):
        class A:
            x = 1

        class B:
            x = 1

        a = A.__dict__["__dict__"]
        b = B.__dict__["__dict__"]
        assert is_type(a, "builtins.getset_descriptor")

        self.assertEqual(get_hash(a), get_hash(a))
        self.assertNotEqual(get_hash(a), get_hash(b))

    def test_dict(self):
        self.assertEqual(get_hash({1: 1}), get_hash({1: 1}))
        self.assertNotEqual(get_hash({1: 1}), get_hash({1: 2}))
        self.assertNotEqual(get_hash({1: 1}), get_hash([(1, 1)]))

        dict_gen = {1: (x for x in range(1))}
        with self.assertRaises(UnhashableTypeError):
            get_hash(dict_gen)

    def test_self_reference_dict(self):
        d1 = {"cat": "hat"}
        d2 = {"things": [1, 2]}

        self.assertEqual(get_hash(d1), get_hash(d1))
        self.assertNotEqual(get_hash(d1), get_hash(d2))

        # test that we can hash self-referencing dictionaries
        d2 = {"book": d1}
        self.assertNotEqual(get_hash(d2), get_hash(d1))

    def test_float(self):
        self.assertEqual(get_hash(0.1), get_hash(0.1))
        self.assertNotEqual(get_hash(23.5234), get_hash(23.5235))

    def test_bool(self):
        self.assertEqual(get_hash(True), get_hash(True))
        self.assertNotEqual(get_hash(True), get_hash(False))

    def test_none(self):
        self.assertEqual(get_hash(None), get_hash(None))
        self.assertNotEqual(get_hash(None), get_hash(False))

    def test_builtins(self):
        self.assertEqual(get_hash(abs), get_hash(abs))
        self.assertNotEqual(get_hash(abs), get_hash(type))

    def test_regex(self):
        p2 = re.compile(".*")
        p1 = re.compile(".*")
        p3 = re.compile(".*", re.I)
        self.assertEqual(get_hash(p1), get_hash(p2))
        self.assertNotEqual(get_hash(p1), get_hash(p3))

    def test_pandas_large_dataframe(self):
        df1 = pd.DataFrame(np.zeros((_PANDAS_ROWS_LARGE, 4)), columns=list("ABCD"))
        df2 = pd.DataFrame(np.ones((_PANDAS_ROWS_LARGE, 4)), columns=list("ABCD"))
        df3 = pd.DataFrame(np.zeros((_PANDAS_ROWS_LARGE, 4)), columns=list("ABCD"))

        self.assertEqual(get_hash(df1), get_hash(df3))
        self.assertNotEqual(get_hash(df1), get_hash(df2))

    @parameterized.expand(
        [
            (pd.DataFrame({"foo": [12]}), pd.DataFrame({"foo": [12]}), True),
            (pd.DataFrame({"foo": [12]}), pd.DataFrame({"foo": [42]}), False),
            (
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}),
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}),
                True,
            ),
            # Extra column
            (
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}),
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4], "C": [1, 2, 3]}),
                False,
            ),
            # Different values
            (
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}),
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 5]}),
                False,
            ),
            # Different order
            (
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}),
                pd.DataFrame(data={"B": [1, 2, 3], "A": [2, 3, 4]}),
                False,
            ),
            # Different index
            (
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}, index=[1, 2, 3]),
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}, index=[1, 2, 4]),
                False,
            ),
            # Missing column
            (
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}),
                pd.DataFrame(data={"A": [1, 2, 3]}),
                False,
            ),
            # Different sort
            (
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}).sort_values(
                    by=["A"]
                ),
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}).sort_values(
                    by=["B"], ascending=False
                ),
                False,
            ),
            # Different headers
            (
                pd.DataFrame(data={"A": [1, 2, 3], "C": [2, 3, 4]}),
                pd.DataFrame(data={"A": [1, 2, 3], "B": [2, 3, 4]}),
                False,
            ),
            # Reordered columns
            (
                pd.DataFrame(data={"A": [1, 2, 3], "C": [2, 3, 4]}),
                pd.DataFrame(data={"C": [2, 3, 4], "A": [1, 2, 3]}),
                False,
            ),
            # Slightly different dtypes
            (
                pd.DataFrame(
                    data={"A": [1, 2, 3], "C": pd.array([1, 2, 3], dtype="UInt64")}
                ),
                pd.DataFrame(
                    data={"A": [1, 2, 3], "C": pd.array([1, 2, 3], dtype="Int64")}
                ),
                False,
            ),
        ]
    )
    def test_pandas_dataframe(self, df1, df2, expected):
        result = get_hash(df1) == get_hash(df2)
        self.assertEqual(result, expected)

    def test_pandas_series(self):
        series1 = pd.Series([1, 2])
        series2 = pd.Series([1, 3])
        series3 = pd.Series([1, 2])

        self.assertEqual(get_hash(series1), get_hash(series3))
        self.assertNotEqual(get_hash(series1), get_hash(series2))

        series4 = pd.Series(range(_PANDAS_ROWS_LARGE))
        series5 = pd.Series(range(_PANDAS_ROWS_LARGE))

        self.assertEqual(get_hash(series4), get_hash(series5))

    def test_pandas_series_similar_dtypes(self):
        series1 = pd.Series([1, 2], dtype="UInt64")
        series2 = pd.Series([1, 2], dtype="Int64")

        self.assertNotEqual(get_hash(series1), get_hash(series2))

    def test_numpy(self):
        np1 = np.zeros(10)
        np2 = np.zeros(11)
        np3 = np.zeros(10)

        self.assertEqual(get_hash(np1), get_hash(np3))
        self.assertNotEqual(get_hash(np1), get_hash(np2))

        np4 = np.zeros(_NP_SIZE_LARGE)
        np5 = np.zeros(_NP_SIZE_LARGE)

        self.assertEqual(get_hash(np4), get_hash(np5))

    def test_numpy_similar_dtypes(self):
        np1 = np.ones(10, dtype="u8")
        np2 = np.ones(10, dtype="i8")

        np3 = np.ones(10, dtype=[("a", "u8"), ("b", "i8")])
        np4 = np.ones(10, dtype=[("a", "i8"), ("b", "u8")])

        self.assertNotEqual(get_hash(np1), get_hash(np2))
        self.assertNotEqual(get_hash(np3), get_hash(np4))

    def test_PIL_image(self):
        im1 = Image.new("RGB", (50, 50), (220, 20, 60))
        im2 = Image.new("RGB", (50, 50), (30, 144, 255))
        im3 = Image.new("RGB", (50, 50), (220, 20, 60))

        self.assertEqual(get_hash(im1), get_hash(im3))
        self.assertNotEqual(get_hash(im1), get_hash(im2))

        # Check for big PIL images, they converted to numpy array with size
        # bigger than _NP_SIZE_LARGE
        # 1000 * 1000 * 3 = 3_000_000 > _NP_SIZE_LARGE = 1_000_000
        im4 = Image.new("RGB", (1000, 1000), (100, 20, 60))
        im5 = Image.new("RGB", (1000, 1000), (100, 20, 60))
        im6 = Image.new("RGB", (1000, 1000), (101, 21, 61))

        im4_np_array = np.frombuffer(im4.tobytes(), dtype="uint8")
        self.assertGreater(im4_np_array.size, _NP_SIZE_LARGE)

        self.assertEqual(get_hash(im4), get_hash(im5))
        self.assertNotEqual(get_hash(im5), get_hash(im6))

    @parameterized.expand(
        [
            (BytesIO, b"123", b"456", b"123"),
            (StringIO, "123", "456", "123"),
        ]
    )
    def test_io(self, io_type, io_data1, io_data2, io_data3):
        io1 = io_type(io_data1)
        io2 = io_type(io_data2)
        io3 = io_type(io_data3)

        self.assertEqual(get_hash(io1), get_hash(io3))
        self.assertNotEqual(get_hash(io1), get_hash(io2))

        # Changing the stream position should change the hash
        io1.seek(1)
        io3.seek(0)
        self.assertNotEqual(get_hash(io1), get_hash(io3))

    def test_uploaded_file_io(self):
        rec1 = UploadedFileRec("file1", "name", "type", b"123")
        rec2 = UploadedFileRec("file1", "name", "type", b"456")
        rec3 = UploadedFileRec("file1", "name", "type", b"123")
        io1 = UploadedFile(
            rec1, FileURLs(file_id=rec1.file_id, upload_url="u1", delete_url="d1")
        )
        io2 = UploadedFile(
            rec2, FileURLs(file_id=rec2.file_id, upload_url="u2", delete_url="d2")
        )
        io3 = UploadedFile(
            rec3, FileURLs(file_id=rec3.file_id, upload_url="u3", delete_url="u3")
        )

        self.assertEqual(get_hash(io1), get_hash(io3))
        self.assertNotEqual(get_hash(io1), get_hash(io2))

        # Changing the stream position should change the hash
        io1.seek(1)
        io3.seek(0)
        self.assertNotEqual(get_hash(io1), get_hash(io3))

    def test_partial(self):
        p1 = functools.partial(int, base=2)
        p2 = functools.partial(int, base=3)
        p3 = functools.partial(int, base=2)

        self.assertEqual(get_hash(p1), get_hash(p3))
        self.assertNotEqual(get_hash(p1), get_hash(p2))

    def test_files(self):
        temp1 = tempfile.NamedTemporaryFile()
        temp2 = tempfile.NamedTemporaryFile()

        with open(__file__) as f:
            with open(__file__) as g:
                self.assertEqual(get_hash(f), get_hash(g))

            self.assertNotEqual(get_hash(f), get_hash(temp1))

        self.assertEqual(get_hash(temp1), get_hash(temp1))
        self.assertNotEqual(get_hash(temp1), get_hash(temp2))

    def test_file_position(self):
        with open(__file__) as f:
            h1 = get_hash(f)
            self.assertEqual(h1, get_hash(f))
            f.readline()
            self.assertNotEqual(h1, get_hash(f))
            f.seek(0)
            self.assertEqual(h1, get_hash(f))

    def test_magic_mock(self):
        """MagicMocks never hash to the same thing."""
        # (This also tests that MagicMock can hash at all, without blowing the
        # stack due to an infinite recursion.)
        self.assertNotEqual(get_hash(MagicMock()), get_hash(MagicMock()))

    def test_dataclass(self):
        @dataclass(frozen=True, eq=True)
        class Data:
            foo: str

        bar = Data("bar")

        assert get_hash(bar)

    def test_enum(self):
        """The hashing function returns the same result when called with the same
        Enum members."""

        class EnumClass(Enum):
            ENUM_1 = auto()
            ENUM_2 = auto()

        # Hash values should be stable
        self.assertEqual(get_hash(EnumClass.ENUM_1), get_hash(EnumClass.ENUM_1))

        # Different enum values should produce different hashes
        self.assertNotEqual(get_hash(EnumClass.ENUM_1), get_hash(EnumClass.ENUM_2))

    def test_different_enums(self):
        """Different enum classes should have different hashes, even when the enum
        values are the same."""

        class EnumClassA(Enum):
            ENUM_1 = "hello"

        class EnumClassB(Enum):
            ENUM_1 = "hello"

        enum_a = EnumClassA.ENUM_1
        enum_b = EnumClassB.ENUM_1

        self.assertNotEqual(get_hash(enum_a), get_hash(enum_b))


class NotHashableTest(unittest.TestCase):
    """Tests for various unhashable types."""

    def test_lambdas_not_hashable(self):
        with self.assertRaises(UnhashableTypeError):
            get_hash(lambda x: x.lower())

    def test_generator_not_hashable(self):
        with self.assertRaises(UnhashableTypeError):
            get_hash(x for x in range(1))

    def test_hash_funcs_acceptable_keys(self):
        """Test that hashes are equivalent when hash_func key is supplied both as a
        type literal, and as a type name string.
        """
        test_generator = (x for x in range(1))

        with self.assertRaises(UnhashableTypeError):
            get_hash(test_generator)

        self.assertEqual(
            get_hash(test_generator, hash_funcs={types.GeneratorType: id}),
            get_hash(test_generator, hash_funcs={"builtins.generator": id}),
        )

    def test_hash_funcs_error(self):
        with self.assertRaises(UserHashError) as ctx:
            get_hash(1, cache_type=CacheType.DATA, hash_funcs={int: lambda x: "a" + x})

        expected_message = """can only concatenate str (not "int") to str

This error is likely due to a bug in `<lambda>()`, which is a
user-defined hash function that was passed into the `@st.cache_data` decorator of
something.

`<lambda>()` failed when hashing an object of type
`builtins.int`.  If you don't know where that object is coming from,
try looking at the hash chain below for an object that you do recognize, then
pass that to `hash_funcs` instead:

```
Object of type builtins.int: 1
```

If you think this is actually a Streamlit bug, please
[file a bug report here](https://github.com/streamlit/streamlit/issues/new/choose)."""
        self.assertEqual(str(ctx.exception), expected_message)

    def test_non_hashable(self):
        """Test user provided hash functions."""

        g = (x for x in range(1))

        # Unhashable object raises an error
        with self.assertRaises(UnhashableTypeError):
            get_hash(g)

        id_hash_func = {types.GeneratorType: id}

        self.assertEqual(
            get_hash(g, hash_funcs=id_hash_func),
            get_hash(g, hash_funcs=id_hash_func),
        )

        unique_hash_func = {types.GeneratorType: lambda x: time.time()}

        self.assertNotEqual(
            get_hash(g, hash_funcs=unique_hash_func),
            get_hash(g, hash_funcs=unique_hash_func),
        )

    def test_override_streamlit_hash_func(self):
        """Test that a user provided hash function has priority over a streamlit one."""

        hash_funcs = {int: lambda x: "hello"}
        self.assertNotEqual(get_hash(1), get_hash(1, hash_funcs=hash_funcs))

    def test_function_not_hashable(self):
        def foo():
            pass

        with self.assertRaises(UnhashableTypeError):
            get_hash(foo)

    def test_reduce_not_hashable(self):
        class A:
            def __init__(self):
                self.x = [1, 2, 3]

        with self.assertRaises(UnhashableTypeError):
            get_hash(A().__reduce__())


================================================
File: /lib/tests/streamlit/runtime/caching/legacy_cache_api_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""st.cache (legacy) unit tests. These tests just test that `st.cache` correctly routes
to `st.cache_data` and `st.cache_resource` as expected. It does not test the actual
caching functionality, which is tested in the cache_data & cache_resource tests."""

from __future__ import annotations

import threading
import unittest
from unittest.mock import MagicMock, Mock, patch

import streamlit as st
from streamlit.runtime import Runtime
from streamlit.runtime.caching.storage.dummy_cache_storage import (
    MemoryCacheStorageManager,
)
from streamlit.runtime.scriptrunner import add_script_run_ctx
from tests.testutil import create_mock_script_run_ctx


class LegacyCacheTest(unittest.TestCase):
    def setUp(self) -> None:
        # Caching functions rely on an active script run ctx
        add_script_run_ctx(threading.current_thread(), create_mock_script_run_ctx())
        mock_runtime = MagicMock(spec=Runtime)
        mock_runtime.cache_storage_manager = MemoryCacheStorageManager()
        Runtime._instance = mock_runtime

    def tearDown(self):
        # Some of these tests reach directly into _cache_info and twiddle it.
        # Reset default values on teardown.
        st.cache_data.clear()
        st.cache_resource.clear()

    @patch("streamlit.deprecation_util.show_deprecation_warning")
    def test_deprecation_warnings(self, show_warning_mock: Mock):
        """We show deprecation warnings when using `@st.cache`."""

        # We show the deprecation warning at declaration time:
        @st.cache
        def foo():
            return 42

        show_warning_mock.assert_called_once()

    @patch("streamlit.cache_data")
    def test_cache_data_usage(self, cache_data_mock: Mock):
        """`@st.cache` should call `st.cache_data` when no parameter are specified."""

        @st.cache
        def foo():
            return 42

        cache_data_mock.assert_called_once()

    @patch("streamlit.cache_data")
    def test_cache_data_usage_with_kwargs(self, cache_data_mock: Mock):
        """`@st.cache` should call `st.cache_data` with kwargs when `allow_output_mutation` is False."""

        @st.cache(
            allow_output_mutation=False,
            persist=True,
            show_spinner=True,
            hash_funcs={},
            max_entries=10,
            ttl=1,
        )
        def foo():
            return 42

        cache_data_mock.assert_called_once_with(
            None,
            persist=True,
            show_spinner=True,
            hash_funcs={},
            max_entries=10,
            ttl=1,
        )

    @patch("streamlit.cache_resource")
    def test_cache_resource_usage_with_kwargs(self, cache_resource_mock: Mock):
        """`@st.cache` should call `st.cache_resource` with kwargs when `allow_output_mutation` is True."""

        @st.cache(
            allow_output_mutation=True,
            persist=True,  # persist is not forwarded to cache_resource
            show_spinner=True,
            hash_funcs={},
            max_entries=10,
            ttl=1,
        )
        def foo():
            return 42

        cache_resource_mock.assert_called_once_with(
            None,
            show_spinner=True,
            hash_funcs={},
            max_entries=10,
            ttl=1,
        )


================================================
File: /lib/tests/streamlit/runtime/caching/storage/__init__.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


================================================
File: /lib/tests/streamlit/runtime/caching/storage/dummy_cache_storage_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Unit tests for DummyCacheStorage and MemoryCacheStorageManager"""

from __future__ import annotations

import unittest

from streamlit.runtime.caching.storage import (
    CacheStorageContext,
    CacheStorageKeyNotFoundError,
)
from streamlit.runtime.caching.storage.dummy_cache_storage import (
    DummyCacheStorage,
    MemoryCacheStorageManager,
)


class DummyCacheStorageManagerTest(unittest.TestCase):
    def setUp(self) -> None:
        super().setUp()
        self.context = CacheStorageContext(
            function_key="func-key",
            function_display_name="func-display-name",
            persist="disk",
        )
        self.dummy_cache_storage = DummyCacheStorage()
        self.storage_manager = MemoryCacheStorageManager()
        self.storage = self.storage_manager.create(self.context)

    def test_in_memory_wrapped_dummy_cache_storage_get_not_found(self):
        """
        Test that storage.get() returns CacheStorageKeyNotFoundError when key is not
        present.
        """
        with self.assertRaises(CacheStorageKeyNotFoundError):
            self.storage.get("some-key")

    def test_in_memory_wrapped_dummy_cache_storage_get_found(self):
        """
        Test that storage.get() returns the value when key is present.
        """
        self.storage.set("some-key", b"some-value")
        self.assertEqual(self.storage.get("some-key"), b"some-value")

    def test_in_memory_wrapped_dummy_cache_storage_storage_set(self):
        """
        Test that storage.set() sets the value correctly.
        """
        self.storage.set("new-key", b"new-value")
        self.assertEqual(self.storage.get("new-key"), b"new-value")

    def test_in_memory_wrapped_dummy_cache_storage_storage_set_override(self):
        """
        Test that storage.set() overrides the value.
        """
        self.storage.set("another_key", b"another_value")
        self.storage.set("another_key", b"new_value")
        self.assertEqual(self.storage.get("another_key"), b"new_value")

    def test_in_memory_wrapped_dummy_cache_storage_storage_delete(self):
        """
        Test that storage.delete() deletes the value correctly.
        """
        self.storage.set("new-key", b"new-value")
        self.storage.delete("new-key")
        with self.assertRaises(CacheStorageKeyNotFoundError):
            self.storage.get("new-key")


class DummyCacheStorageTest(unittest.TestCase):
    def setUp(self):
        super().setUp()
        self.storage = DummyCacheStorage()

    def test_dummy_storage_get_always_not_found(self):
        """Test that storage.get() always returns CacheStorageKeyNotFoundError."""

        with self.assertRaises(CacheStorageKeyNotFoundError):
            self.storage.get("some-key")

        self.storage.set("some-key", b"some-value")

        with self.assertRaises(CacheStorageKeyNotFoundError):
            self.storage.get("some-key")

    def test_storage_set(self):
        """
        Test that storage.set() works correctly, at always do nothing without
        raising exception."""
        self.storage.set("new-key", b"new-value")
        with self.assertRaises(CacheStorageKeyNotFoundError):
            self.storage.get("new-key")

    def test_storage_delete(self):
        """
        Test that storage.delete() works correctly, at always do nothing without
        raising exception.
        """
        self.storage.delete("another-key")
        self.storage.delete("another-key")
        self.storage.delete("another-key")

    def test_storage_clear(self):
        """
        Test that storage.clear() works correctly, at always do nothing without
        raising exception.
        """
        self.storage.clear()

    def test_storage_close(self):
        """
        Test that storage.close() works correctly, at always do nothing without
        raising exception.
        """
        self.storage.close()


================================================
File: /lib/tests/streamlit/runtime/caching/storage/in_memory_cache_storage_wrapper_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Unit tests for InMemoryCacheStorageWrapper"""

from __future__ import annotations

import unittest
from unittest.mock import patch

from testfixtures import TempDirectory

from streamlit.runtime.caching.storage import (
    CacheStorageContext,
    CacheStorageKeyNotFoundError,
)
from streamlit.runtime.caching.storage.dummy_cache_storage import DummyCacheStorage
from streamlit.runtime.caching.storage.in_memory_cache_storage_wrapper import (
    InMemoryCacheStorageWrapper,
)
from streamlit.runtime.caching.storage.local_disk_cache_storage import (
    LocalDiskCacheStorage,
)


class InMemoryCacheStorageWrapperTest(unittest.TestCase):
    """Unit tests for InMemoryCacheStorageWrapper"""

    def setUp(self) -> None:
        self.tempdir = TempDirectory(create=True)
        self.patch_get_cache_folder_path = patch(
            "streamlit.runtime.caching.storage.local_disk_cache_storage.get_cache_folder_path",
            return_value=self.tempdir.path,
        )
        self.patch_get_cache_folder_path.start()

    def tearDown(self):
        super().tearDown()
        self.patch_get_cache_folder_path.stop()
        self.tempdir.cleanup()

    def get_storage_context(self):
        return CacheStorageContext(
            function_key="func-key",
            function_display_name="func-display-name",
            persist="disk",
        )

    def test_in_memory_cache_storage_wrapper_works_with_local_disk_storage(self):
        """
        InMemoryCacheStorageWrapper should work with local disk storage without raising
        an exception
        """
        context = self.get_storage_context()

        InMemoryCacheStorageWrapper(
            persist_storage=LocalDiskCacheStorage(context),
            context=context,
        )

    def test_in_memory_cache_storage_wrapper_works_with_dummy_storage(self):
        """
        InMemoryCacheStorageWrapper should work with dummy storage without raising
        an exception
        """
        context = self.get_storage_context()

        InMemoryCacheStorageWrapper(
            persist_storage=DummyCacheStorage(),
            context=context,
        )

    def test_in_memory_cache_storage_wrapper_get_key_in_persist_storage(self):
        """
        Test that storage.get() returns the value from persist storage
        if value doesn't exist in memory.
        """

        context = self.get_storage_context()
        persist_storage = LocalDiskCacheStorage(context)
        wrapped_storage = InMemoryCacheStorageWrapper(
            persist_storage=persist_storage, context=context
        )

        persist_storage.set("some-key", b"some-value")
        with patch.object(
            persist_storage, "get", wraps=persist_storage.get
        ) as mock_persist_get:
            self.assertEqual(wrapped_storage.get("some-key"), b"some-value")
            mock_persist_get.assert_called_once_with("some-key")

            # Call get again to make that underlying storage is not called again
            self.assertEqual(wrapped_storage.get("some-key"), b"some-value")
            mock_persist_get.assert_called_once()

    def test_in_memory_cache_storage_wrapper_get_key_in_memory_storage(self):
        """
        Test that storage.get() returns the value from in_memory storage
        if value exists in memory.
        """
        context = self.get_storage_context()
        persist_storage = LocalDiskCacheStorage(context)
        wrapped_storage = InMemoryCacheStorageWrapper(
            persist_storage=persist_storage, context=context
        )

        wrapped_storage.set("some-key", b"some-value")

        with patch.object(
            persist_storage, "get", wraps=persist_storage.get
        ) as mock_persist_get:
            self.assertEqual(wrapped_storage.get("some-key"), b"some-value")
            mock_persist_get.assert_not_called()

    def test_in_memory_cache_storage_wrapper_set(self):
        """
        Test that storage.set() sets value both in in-memory cache and
        in persist storage
        """
        context = self.get_storage_context()
        persist_storage = LocalDiskCacheStorage(context)
        wrapped_storage = InMemoryCacheStorageWrapper(
            persist_storage=persist_storage, context=context
        )

        persist_storage.set("some-key", b"some-value")
        with patch.object(
            persist_storage, "set", wraps=persist_storage.set
        ) as mock_persist_set:
            wrapped_storage.set("some-key", b"some-value")
            mock_persist_set.assert_called_once_with("some-key", b"some-value")

        self.assertEqual(wrapped_storage.get("some-key"), b"some-value")

    def test_in_memory_cache_storage_wrapper_delete(self):
        """
        Test that storage.delete() deletes value both in in-memory cache
        and in persist storage
        """
        context = self.get_storage_context()
        persist_storage = LocalDiskCacheStorage(context)
        wrapped_storage = InMemoryCacheStorageWrapper(
            persist_storage=persist_storage, context=context
        )

        wrapped_storage.set("some-key", b"some-value")
        with patch.object(
            persist_storage, "delete", wraps=persist_storage.delete
        ) as mock_persist_delete:
            wrapped_storage.delete("some-key")
            mock_persist_delete.assert_called_once_with("some-key")

        with self.assertRaises(CacheStorageKeyNotFoundError):
            wrapped_storage.get("some-key")

    def test_in_memory_cache_storage_wrapper_close(self):
        """
        Test that storage.close() closes the underlying persist storage
        """
        context = self.get_storage_context()
        persist_storage = LocalDiskCacheStorage(context)
        wrapped_storage = InMemoryCacheStorageWrapper(
            persist_storage=persist_storage, context=context
        )

        with patch.object(
            persist_storage, "close", wraps=persist_storage.close
        ) as mock_persist_close:
            wrapped_storage.close()
            mock_persist_close.assert_called_once()


================================================
File: /lib/tests/streamlit/runtime/caching/storage/local_disk_cache_storage_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Unit tests for LocalDiskCacheStorage and LocalDiskCacheStorageManager"""

from __future__ import annotations

import logging
import math
import os.path
import shutil
import unittest
from unittest.mock import MagicMock, patch

from testfixtures import TempDirectory

from streamlit import errors
from streamlit.logger import get_logger
from streamlit.runtime.caching.storage import (
    CacheStorageContext,
    CacheStorageError,
    CacheStorageKeyNotFoundError,
)
from streamlit.runtime.caching.storage.in_memory_cache_storage_wrapper import (
    InMemoryCacheStorageWrapper,
)
from streamlit.runtime.caching.storage.local_disk_cache_storage import (
    LocalDiskCacheStorage,
    LocalDiskCacheStorageManager,
)


class LocalDiskCacheStorageManagerTest(unittest.TestCase):
    def setUp(self) -> None:
        super().setUp()
        self.tempdir = TempDirectory(create=True)
        self.patch_get_cache_folder_path = patch(
            "streamlit.runtime.caching.storage.local_disk_cache_storage.get_cache_folder_path",
            return_value=self.tempdir.path,
        )
        self.patch_get_cache_folder_path.start()

    def tearDown(self) -> None:
        super().tearDown()
        self.patch_get_cache_folder_path.stop()
        self.tempdir.cleanup()

    def test_create_persist_context(self):
        """Tests that LocalDiskCacheStorageManager.create()
        returns a LocalDiskCacheStorage with correct parameters from context, if
        persist="disk"
        """
        context = CacheStorageContext(
            function_key="func-key",
            function_display_name="func-display-name",
            persist="disk",
            ttl_seconds=60,
            max_entries=100,
        )
        manager = LocalDiskCacheStorageManager()
        storage = manager.create(context)
        self.assertIsInstance(storage, InMemoryCacheStorageWrapper)
        self.assertEqual(storage.ttl_seconds, 60)
        self.assertEqual(storage.max_entries, 100)

    def test_create_not_persist_context(self):
        """Tests that LocalDiskCacheStorageManager.create()
        returns a LocalDiskCacheStorage with correct parameters from context, if
        persist is None
        """
        context = CacheStorageContext(
            function_key="func-key",
            function_display_name="func-display-name",
            persist=None,
            ttl_seconds=None,
            max_entries=None,
        )
        manager = LocalDiskCacheStorageManager()
        storage = manager.create(context)
        self.assertIsInstance(storage, InMemoryCacheStorageWrapper)
        self.assertEqual(storage.ttl_seconds, math.inf)
        self.assertEqual(storage.max_entries, math.inf)

    def test_check_context_with_persist_and_ttl(self):
        """Tests that LocalDiskCacheStorageManager.check_context() writes a warning
        in logs when persist="disk" and ttl_seconds is not None
        """
        context = CacheStorageContext(
            function_key="func-key",
            function_display_name="func-display-name",
            persist="disk",
            ttl_seconds=60,
            max_entries=100,
        )

        with self.assertLogs(
            "streamlit.runtime.caching.storage.local_disk_cache_storage",
            level=logging.WARNING,
        ) as logs:
            manager = LocalDiskCacheStorageManager()
            manager.check_context(context)

            output = "".join(logs.output)
            self.assertIn(
                "The cached function 'func-display-name' has a TTL that will be "
                "ignored. Persistent cached functions currently don't support TTL.",
                output,
            )

    def test_check_context_without_persist(self):
        """Tests that LocalDiskCacheStorageManager.check_context() does not
        write a warning in logs when persist is None and ttl_seconds is NOT None.
        """
        context = CacheStorageContext(
            function_key="func-key",
            function_display_name="func-display-name",
            persist=None,
            ttl_seconds=60,
            max_entries=100,
        )

        with self.assertLogs(
            "streamlit.runtime.caching.storage.local_disk_cache_storage",
            level=logging.WARNING,
        ) as logs:
            manager = LocalDiskCacheStorageManager()
            manager.check_context(context)

            # assertLogs is being used as a context manager, but it also checks
            # that some log output was captured, so we have to let it capture something
            get_logger(
                "streamlit.runtime.caching.storage.local_disk_cache_storage"
            ).warning("irrelevant warning so assertLogs passes")

            output = "".join(logs.output)
            self.assertNotIn(
                "The cached function 'func-display-name' has a TTL that will be "
                "ignored. Persistent cached functions currently don't support TTL.",
                output,
            )

    @patch("shutil.rmtree", wraps=shutil.rmtree)
    def test_clear_all(self, mock_rmtree):
        """Tests that LocalDiskCacheStorageManager.clear_all() calls shutil.rmtree
        to remove the cache folder
        """
        manager = LocalDiskCacheStorageManager()
        manager.clear_all()
        mock_rmtree.assert_called_once()


class LocalDiskPersistCacheStorageTest(unittest.TestCase):
    def setUp(self):
        super().setUp()
        self.context = CacheStorageContext(
            function_key="func-key",
            function_display_name="func-display-name",
            persist="disk",
        )
        self.storage = LocalDiskCacheStorage(self.context)
        self.tempdir = TempDirectory(create=True)
        self.patch_get_cache_folder_path = patch(
            "streamlit.runtime.caching.storage.local_disk_cache_storage.get_cache_folder_path",
            return_value=self.tempdir.path,
        )
        self.patch_get_cache_folder_path.start()

    def tearDown(self):
        super().tearDown()
        self.storage.clear()
        self.patch_get_cache_folder_path.stop()
        self.tempdir.cleanup()

    def test_storage_get_not_found(self):
        """Test that storage.get() returns the correct value."""

        with self.assertRaises(CacheStorageKeyNotFoundError):
            self.storage.get("some-key")

    def test_storage_get_found(self):
        """Test that storage.get() returns the correct value."""
        self.storage.set("some-key", b"some-value")
        self.assertEqual(self.storage.get("some-key"), b"some-value")

    def test_storage_set(self):
        """Test that storage.set() writes the correct value to disk."""
        self.storage.set("new-key", b"new-value")
        self.assertTrue(os.path.exists(self.tempdir.path + "/func-key-new-key.memo"))

        with open(self.tempdir.path + "/func-key-new-key.memo", "rb") as f:
            self.assertEqual(f.read(), b"new-value")

    @patch(
        "streamlit.runtime.caching.storage.local_disk_cache_storage.streamlit_write",
        MagicMock(side_effect=errors.Error("mock exception")),
    )
    def test_storage_set_error(self):
        """Test that storage.set() raises an exception when it fails to write to disk."""
        with self.assertRaises(CacheStorageError) as e:
            self.storage.set("uniqueKey", b"new-value")
        self.assertEqual(str(e.exception), "Unable to write to cache")

    def test_storage_set_override(self):
        """Test that storage.set() overrides the value of an existing key."""
        self.storage.set("another_key", b"another_value")
        self.storage.set("another_key", b"new_value")
        self.assertEqual(self.storage.get("another_key"), b"new_value")

    def test_storage_delete(self):
        """Test that storage.delete() removes the correct file from disk."""
        self.storage.set("new-key", b"new-value")
        self.assertTrue(os.path.exists(self.tempdir.path + "/func-key-new-key.memo"))
        self.storage.delete("new-key")
        self.assertFalse(os.path.exists(self.tempdir.path + "/func-key-new-key.memo"))

        with self.assertRaises(CacheStorageKeyNotFoundError):
            self.storage.get("new-key")

    def test_storage_clear(self):
        """Test that storage.clear() removes all storage files from disk."""
        self.storage.set("some-key", b"some-value")
        self.storage.set("another-key", b"another-value")
        self.assertTrue(os.path.exists(self.tempdir.path + "/func-key-some-key.memo"))
        self.assertTrue(
            os.path.exists(self.tempdir.path + "/func-key-another-key.memo")
        )

        self.storage.clear()

        self.assertFalse(os.path.exists(self.tempdir.path + "/func-key-some-key.memo"))
        self.assertFalse(
            os.path.exists(self.tempdir.path + "/func-key-another-key.memo")
        )

        with self.assertRaises(CacheStorageKeyNotFoundError):
            self.storage.get("some-key")

        with self.assertRaises(CacheStorageKeyNotFoundError):
            self.storage.get("another-key")

        # test that cache folder is empty
        self.assertEqual(os.listdir(self.tempdir.path), [])

    def test_storage_clear_not_existing_cache_directory(self):
        """Test that clear() is not crashing if the cache directory does not exist."""
        self.tempdir.cleanup()
        self.storage.clear()

    def test_storage_clear_call_listdir_existing_cache_directory(self):
        """Test that clear() call os.listdir if cache folder does not exist."""
        with patch("os.listdir") as mock_listdir:
            self.storage.clear()
        mock_listdir.assert_called_once()

    def test_storage_clear_not_call_listdir_not_existing_cache_directory(self):
        """Test that clear() doesn't call os.listdir if cache folder does not exist."""
        self.tempdir.cleanup()

        with patch("os.listdir") as mock_listdir:
            self.storage.clear()

        mock_listdir.assert_not_called()

    def test_storage_close(self):
        """Test that storage.close() does not raise any exception."""
        self.storage.close()


================================================
File: /lib/tests/streamlit/runtime/caching/test_data/arrow_replay.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""A small script that reproduces the bug from #6103"""

import pandas as pd

import streamlit as st

df = pd.DataFrame(
    {"col1": [1, 2, 3, 4], "col2": ["pino", "gino", "lucullo", "augusto"]}
)

st.write(df)


@st.cache_resource(show_spinner=False)
def stampa_df(df_param: pd.DataFrame) -> None:
    st.write(df_param)


stampa_df(df)


================================================
File: /lib/tests/streamlit/runtime/scriptrunner/__init__.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


================================================
File: /lib/tests/streamlit/runtime/scriptrunner/code_exec_test.py
================================================
# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import unittest

from parameterized import parameterized

from streamlit.runtime.forward_msg_queue import ForwardMsgQueue
from streamlit.runtime.fragment import MemoryFragmentStorage
from streamlit.runtime.memory_uploaded_file_manager import MemoryUploadedFileManager
from streamlit.runtime.pages_manager import PagesManager
from streamlit.runtime.scriptrunner.exec_code import exec_func_with_error_handling
from streamlit.runtime.scriptrunner_utils.exceptions import (
    RerunException,
    StopException,
)
from streamlit.runtime.scriptrunner_utils.script_requests import RerunData
from streamlit.runtime.scriptrunner_utils.script_run_context import ScriptRunContext
from streamlit.runtime.state import SafeSessionState, SessionState


class TestWrapInTryAndExec(unittest.TestCase):
    def setUp(self) -> None:
